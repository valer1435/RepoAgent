{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RepoAgent","text":""},{"location":"#overview","title":"Overview","text":"<p>RepoAgent is a tool designed for automated documentation generation and management within Git repositories, with a particular focus on Python projects. It analyzes the repository's structure, detects changes in code, and leverages language models to create and update documentation.</p>"},{"location":"#purpose","title":"Purpose","text":"<p>The primary purpose of RepoAgent is to streamline the process of keeping project documentation synchronized with the codebase. It automates tasks such as summarizing modules, identifying areas requiring documentation updates, and generating documentation content using AI-powered chat engines. The tool also provides functionalities for managing settings, handling task dependencies, and cleaning up temporary files created during the documentation process.</p>"},{"location":"repo_agent/","title":"Repo Agent","text":""},{"location":"repo_agent/#overview","title":"Overview","text":"<p>The <code>RepoAgent</code> module serves as the core engine for automated documentation generation and management within Git repositories, specifically tailored for Python projects. It encompasses functionalities for detecting code changes, interacting with language models, managing project metadata, orchestrating tasks, handling settings, and facilitating file system operations. The module provides a comprehensive framework for analyzing repository structure, identifying documentation needs, and generating updated documentation content.</p>"},{"location":"repo_agent/#purpose","title":"Purpose","text":"<p>This module is designed to automate the process of keeping project documentation synchronized with its codebase. It achieves this by: </p> <ul> <li>Change Detection: Monitoring Python files within a Git repository for modifications.</li> <li>Documentation Interaction: Utilizing language models via a chat engine to generate and refine documentation content.</li> <li>Metadata Management: Representing and managing essential project metadata, including documentation item types, statuses, and relationships between code elements.</li> <li>File Handling: Analyzing and manipulating files within the repository, including identifying references and managing temporary files.</li> <li>Task Orchestration: Managing a queue of tasks with dependencies for efficient documentation generation and updates.</li> <li>Settings Management: Providing a centralized system for configuring application behavior and project-specific parameters. </li> <li>Project Analysis: Analyzing the structure and dependencies within a software project to inform documentation efforts.</li> <li>Documentation Runner: Orchestrating the complete documentation generation and update process.</li> <li>Logging: Configuring and managing logging levels for monitoring and debugging purposes.</li> <li>Module Summarization: Generating summaries of repository modules and their contents.</li> <li>Command Line Interface: Providing a command-line interface to trigger documentation processes, handle errors, and clean up temporary files.</li> </ul>"},{"location":"repo_agent/change_detector/","title":"Change Detector","text":""},{"location":"repo_agent/change_detector/#repo_agent.change_detector.ChangeDetector","title":"<code>ChangeDetector</code>","text":"<p>Detects changes in a Git repository, focusing on Python files.</p> <p>This class analyzes staged and unstaged files to identify modifications, additions, and removals of code structures. It helps determine which files should be staged for commit based on project-specific rules.</p> Source code in <code>repo_agent/change_detector.py</code> <pre><code>class ChangeDetector:\n    \"\"\"\n    Detects changes in a Git repository, focusing on Python files.\n\n    This class analyzes staged and unstaged files to identify modifications,\n    additions, and removals of code structures. It helps determine which\n    files should be staged for commit based on project-specific rules.\n    \"\"\"\n\n    def __init__(self, repo_path):\n        \"\"\"\n        Initializes a change detector with the specified repository path and associated Git repository.\n\n\n\n        Args:\n            repo_path: The path to the Git repository.\n\n        Returns:\n            None\n\n        \"\"\"\n\n        self.repo_path = repo_path\n        self.repo = git.Repo(repo_path)\n\n    def get_staged_pys(self):\n        \"\"\"\n        Returns a dictionary of staged Python files, mapping file paths to a boolean indicating if the file is newly added.\n\n        The keys are the paths to the staged Python files, and the values are booleans\n        indicating whether the file is new (added) or modified.\n\n        Args:\n            None\n\n        Returns:\n            dict: A dictionary where keys are staged .py file paths and values\n                  are boolean indicating if it's a newly added file.\n\n\n        \"\"\"\n\n        repo = self.repo\n        staged_files = {}\n        diffs = repo.index.diff(\"HEAD\", R=True)\n        for diff in diffs:\n            if diff.change_type in [\"A\", \"M\"] and diff.a_path.endswith(\".py\"):\n                is_new_file = diff.change_type == \"A\"\n                staged_files[diff.a_path] = is_new_file\n        return staged_files\n\n    def get_file_diff(self, file_path, is_new_file):\n        \"\"\"\n        Calculates and returns the differences for a given file, preparing new files for comparison by staging them first.\n\n        Args:\n            file_path: The path to the file.\n            is_new_file: Whether the file is new or not.\n\n        Returns:\n            list[str]: A list of strings representing the diff lines.\n\n\n        \"\"\"\n\n        repo = self.repo\n        if is_new_file:\n            add_command = f\"git -C {repo.working_dir} add {file_path}\"\n            subprocess.run(add_command, shell=True, check=True)\n            diffs = repo.git.diff(\"--staged\", file_path).splitlines()\n        else:\n            diffs = repo.git.diff(\"HEAD\", file_path).splitlines()\n        return diffs\n\n    def parse_diffs(self, diffs):\n        \"\"\"\n        Identifies lines added or removed within a diff, preserving accurate line numbers even with contextual changes. It parses diff output to extract modified content and its original location.\n\n        Args:\n            diffs: A list of strings representing the diff output.\n\n        Returns:\n            dict: A dictionary with 'added' and 'removed' keys, each containing a list of tuples.\n                  Each tuple represents a changed line and contains the line number and the line content.\n\n\n        \"\"\"\n\n        changed_lines = {\"added\": [], \"removed\": []}\n        line_number_current = 0\n        line_number_change = 0\n        for line in diffs:\n            line_number_info = re.match(\"@@ \\\\-(\\\\d+),\\\\d+ \\\\+(\\\\d+),\\\\d+ @@\", line)\n            if line_number_info:\n                line_number_current = int(line_number_info.group(1))\n                line_number_change = int(line_number_info.group(2))\n                continue\n            if line.startswith(\"+\") and (not line.startswith(\"+++\")):\n                changed_lines[\"added\"].append((line_number_change, line[1:]))\n                line_number_change += 1\n            elif line.startswith(\"-\") and (not line.startswith(\"---\")):\n                changed_lines[\"removed\"].append((line_number_current, line[1:]))\n                line_number_current += 1\n            else:\n                line_number_current += 1\n                line_number_change += 1\n        return changed_lines\n\n    def identify_changes_in_structure(self, changed_lines, structures):\n        \"\"\"\n        Determines the code structures impacted by modifications to specific lines, classifying them as added or removed.\n\n        Args:\n            changed_lines: A dictionary where keys are change types ('added', 'removed') and values are lists of tuples representing changed line numbers.\n            structures: A list of tuples containing information about each structure\n                (type, name, start line, end line, parent structure).\n\n        Returns:\n            dict: A dictionary with 'added' and 'removed' keys, each mapping to a set of\n                tuples representing the names and parent structures that were added or removed.\n\n\n        \"\"\"\n\n        changes_in_structures = {\"added\": set(), \"removed\": set()}\n        for change_type, lines in changed_lines.items():\n            for line_number, _ in lines:\n                for (\n                    structure_type,\n                    name,\n                    start_line,\n                    end_line,\n                    parent_structure,\n                ) in structures:\n                    if start_line &lt;= line_number &lt;= end_line:\n                        changes_in_structures[change_type].add((name, parent_structure))\n        return changes_in_structures\n\n    def get_to_be_staged_files(self):\n        \"\"\"\n        Determines files to be staged, including documentation and project hierarchy files. It considers untracked files within designated documentation directories, as well as unstaged files linked to already-staged Python code or matching project settings.\n\n        This method identifies files to be staged based on project settings,\n        tracked/untracked status, and file extensions. It considers markdown\n        documentation files and their corresponding Python files.\n\n        Args:\n            None\n\n        Returns:\n            list: A list of strings representing the paths to the files that should be staged.\n\n\n        \"\"\"\n\n        to_be_staged_files = []\n        staged_files = [item.a_path for item in self.repo.index.diff(\"HEAD\")]\n        print(\n            f\"{Fore.LIGHTYELLOW_EX}target_repo_path{Style.RESET_ALL}: {self.repo_path}\"\n        )\n        print(\n            f\"{Fore.LIGHTMAGENTA_EX}already_staged_files{Style.RESET_ALL}:{staged_files}\"\n        )\n        setting = SettingsManager.get_setting()\n        project_hierarchy = setting.project.hierarchy_name\n        diffs = self.repo.index.diff(None)\n        untracked_files = self.repo.untracked_files\n        print(f\"{Fore.LIGHTCYAN_EX}untracked_files{Style.RESET_ALL}: {untracked_files}\")\n        for untracked_file in untracked_files:\n            if untracked_file.startswith(setting.project.markdown_docs_name):\n                to_be_staged_files.append(untracked_file)\n            continue\n            print(f\"rel_untracked_file:{rel_untracked_file}\")\n            if rel_untracked_file.endswith(\".md\"):\n                rel_untracked_file = os.path.relpath(\n                    rel_untracked_file, setting.project.markdown_docs_name\n                )\n                corresponding_py_file = os.path.splitext(rel_untracked_file)[0] + \".py\"\n                print(\n                    f\"corresponding_py_file in untracked_files:{corresponding_py_file}\"\n                )\n                if corresponding_py_file in staged_files:\n                    to_be_staged_files.append(\n                        os.path.join(\n                            self.repo_path.lstrip(\"/\"),\n                            setting.project.markdown_docs_name,\n                            rel_untracked_file,\n                        )\n                    )\n            elif rel_untracked_file == project_hierarchy:\n                to_be_staged_files.append(rel_untracked_file)\n        unstaged_files = [diff.b_path for diff in diffs]\n        print(f\"{Fore.LIGHTCYAN_EX}unstaged_files{Style.RESET_ALL}: {unstaged_files}\")\n        for unstaged_file in unstaged_files:\n            if unstaged_file.startswith(\n                setting.project.markdown_docs_name\n            ) or unstaged_file.startswith(setting.project.hierarchy_name):\n                to_be_staged_files.append(unstaged_file)\n            elif unstaged_file == project_hierarchy:\n                to_be_staged_files.append(unstaged_file)\n            continue\n            abs_unstaged_file = os.path.join(self.repo_path, unstaged_file)\n            rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)\n            print(f\"rel_unstaged_file:{rel_unstaged_file}\")\n            if unstaged_file.endswith(\".md\"):\n                rel_unstaged_file = os.path.relpath(\n                    rel_unstaged_file, setting.project.markdown_docs_name\n                )\n                corresponding_py_file = os.path.splitext(rel_unstaged_file)[0] + \".py\"\n                print(f\"corresponding_py_file:{corresponding_py_file}\")\n                if corresponding_py_file in staged_files:\n                    to_be_staged_files.append(\n                        os.path.join(\n                            self.repo_path.lstrip(\"/\"),\n                            setting.project.markdown_docs_name,\n                            rel_unstaged_file,\n                        )\n                    )\n            elif unstaged_file == project_hierarchy:\n                to_be_staged_files.append(unstaged_file)\n        print(\n            f\"{Fore.LIGHTRED_EX}newly_staged_files{Style.RESET_ALL}: {to_be_staged_files}\"\n        )\n        return to_be_staged_files\n\n    def add_unstaged_files(self):\n        \"\"\"\n        Stages files identified as needing staging and returns the list of staged file paths.\n\n        Args:\n            None\n\n        Returns:\n            list: A list of file paths that were added to the staging area.\n\n\n        \"\"\"\n\n        unstaged_files_meeting_conditions = self.get_to_be_staged_files()\n        for file_path in unstaged_files_meeting_conditions:\n            add_command = f\"git -C {self.repo.working_dir} add {file_path}\"\n            subprocess.run(add_command, shell=True, check=True)\n        return unstaged_files_meeting_conditions\n</code></pre>"},{"location":"repo_agent/change_detector/#repo_agent.change_detector.ChangeDetector.__init__","title":"<code>__init__(repo_path)</code>","text":"<p>Initializes a change detector with the specified repository path and associated Git repository.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <p>The path to the Git repository.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/change_detector.py</code> <pre><code>def __init__(self, repo_path):\n    \"\"\"\n    Initializes a change detector with the specified repository path and associated Git repository.\n\n\n\n    Args:\n        repo_path: The path to the Git repository.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    self.repo_path = repo_path\n    self.repo = git.Repo(repo_path)\n</code></pre>"},{"location":"repo_agent/change_detector/#repo_agent.change_detector.ChangeDetector.add_unstaged_files","title":"<code>add_unstaged_files()</code>","text":"<p>Stages files identified as needing staging and returns the list of staged file paths.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of file paths that were added to the staging area.</p> Source code in <code>repo_agent/change_detector.py</code> <pre><code>def add_unstaged_files(self):\n    \"\"\"\n    Stages files identified as needing staging and returns the list of staged file paths.\n\n    Args:\n        None\n\n    Returns:\n        list: A list of file paths that were added to the staging area.\n\n\n    \"\"\"\n\n    unstaged_files_meeting_conditions = self.get_to_be_staged_files()\n    for file_path in unstaged_files_meeting_conditions:\n        add_command = f\"git -C {self.repo.working_dir} add {file_path}\"\n        subprocess.run(add_command, shell=True, check=True)\n    return unstaged_files_meeting_conditions\n</code></pre>"},{"location":"repo_agent/change_detector/#repo_agent.change_detector.ChangeDetector.get_file_diff","title":"<code>get_file_diff(file_path, is_new_file)</code>","text":"<p>Calculates and returns the differences for a given file, preparing new files for comparison by staging them first.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <p>The path to the file.</p> required <code>is_new_file</code> <p>Whether the file is new or not.</p> required <p>Returns:</p> Type Description <p>list[str]: A list of strings representing the diff lines.</p> Source code in <code>repo_agent/change_detector.py</code> <pre><code>def get_file_diff(self, file_path, is_new_file):\n    \"\"\"\n    Calculates and returns the differences for a given file, preparing new files for comparison by staging them first.\n\n    Args:\n        file_path: The path to the file.\n        is_new_file: Whether the file is new or not.\n\n    Returns:\n        list[str]: A list of strings representing the diff lines.\n\n\n    \"\"\"\n\n    repo = self.repo\n    if is_new_file:\n        add_command = f\"git -C {repo.working_dir} add {file_path}\"\n        subprocess.run(add_command, shell=True, check=True)\n        diffs = repo.git.diff(\"--staged\", file_path).splitlines()\n    else:\n        diffs = repo.git.diff(\"HEAD\", file_path).splitlines()\n    return diffs\n</code></pre>"},{"location":"repo_agent/change_detector/#repo_agent.change_detector.ChangeDetector.get_staged_pys","title":"<code>get_staged_pys()</code>","text":"<p>Returns a dictionary of staged Python files, mapping file paths to a boolean indicating if the file is newly added.</p> <p>The keys are the paths to the staged Python files, and the values are booleans indicating whether the file is new (added) or modified.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where keys are staged .py file paths and values   are boolean indicating if it's a newly added file.</p> Source code in <code>repo_agent/change_detector.py</code> <pre><code>def get_staged_pys(self):\n    \"\"\"\n    Returns a dictionary of staged Python files, mapping file paths to a boolean indicating if the file is newly added.\n\n    The keys are the paths to the staged Python files, and the values are booleans\n    indicating whether the file is new (added) or modified.\n\n    Args:\n        None\n\n    Returns:\n        dict: A dictionary where keys are staged .py file paths and values\n              are boolean indicating if it's a newly added file.\n\n\n    \"\"\"\n\n    repo = self.repo\n    staged_files = {}\n    diffs = repo.index.diff(\"HEAD\", R=True)\n    for diff in diffs:\n        if diff.change_type in [\"A\", \"M\"] and diff.a_path.endswith(\".py\"):\n            is_new_file = diff.change_type == \"A\"\n            staged_files[diff.a_path] = is_new_file\n    return staged_files\n</code></pre>"},{"location":"repo_agent/change_detector/#repo_agent.change_detector.ChangeDetector.get_to_be_staged_files","title":"<code>get_to_be_staged_files()</code>","text":"<p>Determines files to be staged, including documentation and project hierarchy files. It considers untracked files within designated documentation directories, as well as unstaged files linked to already-staged Python code or matching project settings.</p> <p>This method identifies files to be staged based on project settings, tracked/untracked status, and file extensions. It considers markdown documentation files and their corresponding Python files.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of strings representing the paths to the files that should be staged.</p> Source code in <code>repo_agent/change_detector.py</code> <pre><code>def get_to_be_staged_files(self):\n    \"\"\"\n    Determines files to be staged, including documentation and project hierarchy files. It considers untracked files within designated documentation directories, as well as unstaged files linked to already-staged Python code or matching project settings.\n\n    This method identifies files to be staged based on project settings,\n    tracked/untracked status, and file extensions. It considers markdown\n    documentation files and their corresponding Python files.\n\n    Args:\n        None\n\n    Returns:\n        list: A list of strings representing the paths to the files that should be staged.\n\n\n    \"\"\"\n\n    to_be_staged_files = []\n    staged_files = [item.a_path for item in self.repo.index.diff(\"HEAD\")]\n    print(\n        f\"{Fore.LIGHTYELLOW_EX}target_repo_path{Style.RESET_ALL}: {self.repo_path}\"\n    )\n    print(\n        f\"{Fore.LIGHTMAGENTA_EX}already_staged_files{Style.RESET_ALL}:{staged_files}\"\n    )\n    setting = SettingsManager.get_setting()\n    project_hierarchy = setting.project.hierarchy_name\n    diffs = self.repo.index.diff(None)\n    untracked_files = self.repo.untracked_files\n    print(f\"{Fore.LIGHTCYAN_EX}untracked_files{Style.RESET_ALL}: {untracked_files}\")\n    for untracked_file in untracked_files:\n        if untracked_file.startswith(setting.project.markdown_docs_name):\n            to_be_staged_files.append(untracked_file)\n        continue\n        print(f\"rel_untracked_file:{rel_untracked_file}\")\n        if rel_untracked_file.endswith(\".md\"):\n            rel_untracked_file = os.path.relpath(\n                rel_untracked_file, setting.project.markdown_docs_name\n            )\n            corresponding_py_file = os.path.splitext(rel_untracked_file)[0] + \".py\"\n            print(\n                f\"corresponding_py_file in untracked_files:{corresponding_py_file}\"\n            )\n            if corresponding_py_file in staged_files:\n                to_be_staged_files.append(\n                    os.path.join(\n                        self.repo_path.lstrip(\"/\"),\n                        setting.project.markdown_docs_name,\n                        rel_untracked_file,\n                    )\n                )\n        elif rel_untracked_file == project_hierarchy:\n            to_be_staged_files.append(rel_untracked_file)\n    unstaged_files = [diff.b_path for diff in diffs]\n    print(f\"{Fore.LIGHTCYAN_EX}unstaged_files{Style.RESET_ALL}: {unstaged_files}\")\n    for unstaged_file in unstaged_files:\n        if unstaged_file.startswith(\n            setting.project.markdown_docs_name\n        ) or unstaged_file.startswith(setting.project.hierarchy_name):\n            to_be_staged_files.append(unstaged_file)\n        elif unstaged_file == project_hierarchy:\n            to_be_staged_files.append(unstaged_file)\n        continue\n        abs_unstaged_file = os.path.join(self.repo_path, unstaged_file)\n        rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)\n        print(f\"rel_unstaged_file:{rel_unstaged_file}\")\n        if unstaged_file.endswith(\".md\"):\n            rel_unstaged_file = os.path.relpath(\n                rel_unstaged_file, setting.project.markdown_docs_name\n            )\n            corresponding_py_file = os.path.splitext(rel_unstaged_file)[0] + \".py\"\n            print(f\"corresponding_py_file:{corresponding_py_file}\")\n            if corresponding_py_file in staged_files:\n                to_be_staged_files.append(\n                    os.path.join(\n                        self.repo_path.lstrip(\"/\"),\n                        setting.project.markdown_docs_name,\n                        rel_unstaged_file,\n                    )\n                )\n        elif unstaged_file == project_hierarchy:\n            to_be_staged_files.append(unstaged_file)\n    print(\n        f\"{Fore.LIGHTRED_EX}newly_staged_files{Style.RESET_ALL}: {to_be_staged_files}\"\n    )\n    return to_be_staged_files\n</code></pre>"},{"location":"repo_agent/change_detector/#repo_agent.change_detector.ChangeDetector.identify_changes_in_structure","title":"<code>identify_changes_in_structure(changed_lines, structures)</code>","text":"<p>Determines the code structures impacted by modifications to specific lines, classifying them as added or removed.</p> <p>Parameters:</p> Name Type Description Default <code>changed_lines</code> <p>A dictionary where keys are change types ('added', 'removed') and values are lists of tuples representing changed line numbers.</p> required <code>structures</code> <p>A list of tuples containing information about each structure (type, name, start line, end line, parent structure).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with 'added' and 'removed' keys, each mapping to a set of tuples representing the names and parent structures that were added or removed.</p> Source code in <code>repo_agent/change_detector.py</code> <pre><code>def identify_changes_in_structure(self, changed_lines, structures):\n    \"\"\"\n    Determines the code structures impacted by modifications to specific lines, classifying them as added or removed.\n\n    Args:\n        changed_lines: A dictionary where keys are change types ('added', 'removed') and values are lists of tuples representing changed line numbers.\n        structures: A list of tuples containing information about each structure\n            (type, name, start line, end line, parent structure).\n\n    Returns:\n        dict: A dictionary with 'added' and 'removed' keys, each mapping to a set of\n            tuples representing the names and parent structures that were added or removed.\n\n\n    \"\"\"\n\n    changes_in_structures = {\"added\": set(), \"removed\": set()}\n    for change_type, lines in changed_lines.items():\n        for line_number, _ in lines:\n            for (\n                structure_type,\n                name,\n                start_line,\n                end_line,\n                parent_structure,\n            ) in structures:\n                if start_line &lt;= line_number &lt;= end_line:\n                    changes_in_structures[change_type].add((name, parent_structure))\n    return changes_in_structures\n</code></pre>"},{"location":"repo_agent/change_detector/#repo_agent.change_detector.ChangeDetector.parse_diffs","title":"<code>parse_diffs(diffs)</code>","text":"<p>Identifies lines added or removed within a diff, preserving accurate line numbers even with contextual changes. It parses diff output to extract modified content and its original location.</p> <p>Parameters:</p> Name Type Description Default <code>diffs</code> <p>A list of strings representing the diff output.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with 'added' and 'removed' keys, each containing a list of tuples.   Each tuple represents a changed line and contains the line number and the line content.</p> Source code in <code>repo_agent/change_detector.py</code> <pre><code>def parse_diffs(self, diffs):\n    \"\"\"\n    Identifies lines added or removed within a diff, preserving accurate line numbers even with contextual changes. It parses diff output to extract modified content and its original location.\n\n    Args:\n        diffs: A list of strings representing the diff output.\n\n    Returns:\n        dict: A dictionary with 'added' and 'removed' keys, each containing a list of tuples.\n              Each tuple represents a changed line and contains the line number and the line content.\n\n\n    \"\"\"\n\n    changed_lines = {\"added\": [], \"removed\": []}\n    line_number_current = 0\n    line_number_change = 0\n    for line in diffs:\n        line_number_info = re.match(\"@@ \\\\-(\\\\d+),\\\\d+ \\\\+(\\\\d+),\\\\d+ @@\", line)\n        if line_number_info:\n            line_number_current = int(line_number_info.group(1))\n            line_number_change = int(line_number_info.group(2))\n            continue\n        if line.startswith(\"+\") and (not line.startswith(\"+++\")):\n            changed_lines[\"added\"].append((line_number_change, line[1:]))\n            line_number_change += 1\n        elif line.startswith(\"-\") and (not line.startswith(\"---\")):\n            changed_lines[\"removed\"].append((line_number_current, line[1:]))\n            line_number_current += 1\n        else:\n            line_number_current += 1\n            line_number_change += 1\n    return changed_lines\n</code></pre>"},{"location":"repo_agent/chat_engine/","title":"Chat Engine","text":""},{"location":"repo_agent/chat_engine/#repo_agent.chat_engine.ChatEngine","title":"<code>ChatEngine</code>","text":"<p>A class for interacting with a language model to generate documentation and ideas.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <p>The language model instance used for generating text.</p> <code>settings</code> <p>Settings object containing configuration parameters.</p> <code>project_manager</code> <p>The project manager instance (currently unused).</p> Source code in <code>repo_agent/chat_engine.py</code> <pre><code>class ChatEngine:\n    \"\"\"\n    A class for interacting with a language model to generate documentation and ideas.\n\n    Attributes:\n        llm: The language model instance used for generating text.\n        settings: Settings object containing configuration parameters.\n        project_manager: The project manager instance (currently unused).\n    \"\"\"\n\n    def __init__(self, project_manager):\n        \"\"\"\n        Configures the chat engine with settings for the language model, including API key, base URL, timeout, model name, and temperature.\n\n        Args:\n            project_manager: The project manager instance. This is not directly used in initialization but kept for potential future use.\n\n        Returns:\n            None\n\n        \"\"\"\n\n        setting = SettingsManager.get_setting()\n        self.llm = OpenAILike(\n            context_window=20000,\n            api_key=setting.chat_completion.openai_api_key.get_secret_value(),\n            api_base=setting.chat_completion.openai_base_url,\n            timeout=setting.chat_completion.request_timeout,\n            model=setting.chat_completion.model,\n            temperature=setting.chat_completion.temperature,\n            max_retries=1,\n            is_chat_model=True,\n        )\n\n    def build_prompt(self, doc_item: DocItem, main_idea=\"\", context_length=20000):\n        \"\"\"\n        Creates a prompt to improve code documentation by gathering information about the code\u2019s type, name, content, docstring, and file path. It also includes details of objects that call or are called by the current code, along with their documentation and code, to provide context. A user-defined main idea can be included to focus the enhancement process.\n\n        Args:\n            doc_item: A DocItem object containing code information and references.\n            main_idea: An optional main idea to incorporate into the prompt.\n            context_length: The maximum length of the context window (default is 20000).\n\n        Returns:\n            str: A formatted string representing the prompt for documentation enhancement.\n\n        \"\"\"\n\n        setting = SettingsManager.get_setting()\n        code_info = doc_item.content\n        referenced = len(doc_item.who_reference_me) &gt; 0 and len(code_info) &lt; 16000\n        code_type = code_info[\"type\"]\n        code_name = code_info[\"name\"]\n        code_content = code_info[\"code_content\"]\n        have_return = code_info[\"have_return\"]\n        docstring = (\n            code_info[\"md_content\"][-1]\n            if code_info[\"md_content\"]\n            else \"Empty docstring\"\n        )\n        file_path = doc_item.get_full_name()\n\n        def get_referenced_prompt(doc_item: DocItem) -&gt; str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\n                \"As you can see, the code calls the following objects, their code and docs are as following:\"\n            ]\n            for reference_item in doc_item.reference_who:\n                instance_prompt = (\n                    f\"obj: {reference_item.get_full_name()}\\nDocument: \\n{(reference_item.md_content[-1] if len(reference_item.md_content) &gt; 0 else 'None')}\\nRaw code:```\\n{(reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else '')}\\n```\"\n                    + \"=\" * 10\n                )\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n        def get_referencer_prompt(doc_item: DocItem) -&gt; str:\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\n                \"Also, the code has been called by the following objects, their code and docs are as following:\"\n            ]\n            for referencer_item in doc_item.who_reference_me:\n                instance_prompt = (\n                    f\"obj: {referencer_item.get_full_name()}\\nDocument: \\n{(referencer_item.md_content[-1] if len(referencer_item.md_content) &gt; 0 else 'None')}\\nRaw code:```\\n{(referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None')}\\n```\"\n                    + \"=\" * 10\n                )\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n        def get_relationship_description(referencer_content, reference_letter):\n            if referencer_content and reference_letter:\n                return \"And please include the reference relationship with its callers and callees in the project from a functional perspective\"\n            elif referencer_content:\n                return \"And please include the relationship with its callers in the project from a functional perspective.\"\n            elif reference_letter:\n                return \"And please include the relationship with its callees in the project from a functional perspective.\"\n            else:\n                return \"\"\n\n        code_type_tell = \"Class\" if code_type == \"ClassDef\" else \"Function\"\n        if referenced:\n            combine_ref_situation = (\n                \"and combine it with its calling situation in the project,\"\n            )\n            referencer_content = get_referencer_prompt(doc_item)\n            reference_letter = get_referenced_prompt(doc_item)\n            has_relationship = get_relationship_description(\n                referencer_content, reference_letter\n            )\n        else:\n            combine_ref_situation = \"\"\n            referencer_content = \"\"\n            reference_letter = \"\"\n            has_relationship = \"\"\n        if main_idea:\n            return docstring_update_chat_templates.format_messages(\n                combine_ref_situation=combine_ref_situation,\n                file_path=file_path,\n                code_type_tell=code_type_tell,\n                code_name=code_name,\n                main_idea=(\n                    main_idea\n                    if not main_idea\n                    else f\"You can use user-defined main idea of the project to enhance exist docstring\\n{main_idea}\"\n                ),\n                docstring=docstring,\n                has_relationship=has_relationship,\n                reference_letter=reference_letter,\n                referencer_content=referencer_content,\n                language=setting.project.language,\n            )\n        else:\n            return chat_template.format_messages(\n                combine_ref_situation=combine_ref_situation,\n                file_path=file_path,\n                code_type_tell=code_type_tell,\n                code_name=code_name,\n                code_content=code_content,\n                main_idea=(\n                    main_idea\n                    if not main_idea\n                    else f\"You can use user-defined main idea of the project to enhance exist docstring\\n{main_idea}\"\n                ),\n                docstring=docstring,\n                has_relationship=has_relationship,\n                reference_letter=reference_letter,\n                referencer_content=referencer_content,\n                language=setting.project.language,\n            )\n\n    def generate_doc(self, doc_item: DocItem):\n        \"\"\"\n        Constructs a prompt from the document item and queries a language model to generate documentation. Post-processes the response by removing code block markers.\n\n        Args:\n            doc_item: The DocItem for which to generate documentation.\n        Returns:\n            str: The generated documentation string, with code block markers removed.\n        Raises:\n            Exception: If there is an error during the LLM chat call.\n\n\n        \"\"\"\n\n        settings = SettingsManager.get_setting()\n        if settings.project.main_idea:\n            messages = self.build_prompt(doc_item, main_idea=settings.project.main_idea)\n        else:\n            messages = self.build_prompt(doc_item)\n        try:\n            response = self.llm.chat(messages)\n            answer = response.message.content\n            return answer.replace(\"```python\\n\", \"\").replace(\"```\", \"\")\n        except Exception as e:\n            logger.error(f\"Error in llamaindex chat call: {e}\")\n            raise\n\n    def generate_idea(self, list_items: str):\n        \"\"\"\n        Creates a novel idea based on the given components, adapting to configured language preferences. It communicates with a language model and records token consumption for tracking. Any issues during communication with the language model are logged and then propagated.\n\n        Args:\n            list_items: A string containing the list of items to base the idea on.\n\n        Returns:\n            str: The generated idea as a string.  If an error occurs during the LLM chat call,\n                 the exception is re-raised after logging.\n\n\n        \"\"\"\n\n        settings = SettingsManager.get_setting()\n        messages = idea_chat_template.format_messages(\n            components=list_items, language=settings.project.language\n        )\n        try:\n            response = self.llm.chat(messages)\n            logger.debug(f\"LLM Prompt Tokens: {response.raw.usage.prompt_tokens}\")\n            logger.debug(\n                f\"LLM Completion Tokens: {response.raw.usage.completion_tokens}\"\n            )\n            logger.debug(f\"Total LLM Token Count: {response.raw.usage.total_tokens}\")\n            return response.message.content\n        except Exception as e:\n            logger.error(f\"Error in llamaindex chat call: {e}\")\n            raise\n\n    def summarize_module(self, module_desc: str):\n        \"\"\"\n        Creates a succinct summary of a module\u2019s purpose, informed by project configuration and language preferences. This is achieved through interaction with a large language model, with token usage tracked for monitoring.\n\n        Args:\n            module_desc: The description of the module to be summarized.\n\n        Returns:\n            str: The summary generated by the LLM.  Raises any exceptions encountered during the LLM call.\n\n\n        \"\"\"\n\n        settings = SettingsManager.get_setting()\n        messages = module_summary_template.format_messages(\n            components=module_desc,\n            main_idea=settings.project.main_idea,\n            language=settings.project.language,\n        )\n        try:\n            response = self.llm.chat(messages)\n            logger.debug(f\"LLM Prompt Tokens: {response.raw.usage.prompt_tokens}\")\n            logger.debug(\n                f\"LLM Completion Tokens: {response.raw.usage.completion_tokens}\"\n            )\n            logger.debug(f\"Total LLM Token Count: {response.raw.usage.total_tokens}\")\n            return response.message.content\n        except Exception as e:\n            logger.error(f\"Error in llamaindex chat call: {e}\")\n            raise\n</code></pre>"},{"location":"repo_agent/chat_engine/#repo_agent.chat_engine.ChatEngine.__init__","title":"<code>__init__(project_manager)</code>","text":"<p>Configures the chat engine with settings for the language model, including API key, base URL, timeout, model name, and temperature.</p> <p>Parameters:</p> Name Type Description Default <code>project_manager</code> <p>The project manager instance. This is not directly used in initialization but kept for potential future use.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/chat_engine.py</code> <pre><code>def __init__(self, project_manager):\n    \"\"\"\n    Configures the chat engine with settings for the language model, including API key, base URL, timeout, model name, and temperature.\n\n    Args:\n        project_manager: The project manager instance. This is not directly used in initialization but kept for potential future use.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    setting = SettingsManager.get_setting()\n    self.llm = OpenAILike(\n        context_window=20000,\n        api_key=setting.chat_completion.openai_api_key.get_secret_value(),\n        api_base=setting.chat_completion.openai_base_url,\n        timeout=setting.chat_completion.request_timeout,\n        model=setting.chat_completion.model,\n        temperature=setting.chat_completion.temperature,\n        max_retries=1,\n        is_chat_model=True,\n    )\n</code></pre>"},{"location":"repo_agent/chat_engine/#repo_agent.chat_engine.ChatEngine.build_prompt","title":"<code>build_prompt(doc_item, main_idea='', context_length=20000)</code>","text":"<p>Creates a prompt to improve code documentation by gathering information about the code\u2019s type, name, content, docstring, and file path. It also includes details of objects that call or are called by the current code, along with their documentation and code, to provide context. A user-defined main idea can be included to focus the enhancement process.</p> <p>Parameters:</p> Name Type Description Default <code>doc_item</code> <code>DocItem</code> <p>A DocItem object containing code information and references.</p> required <code>main_idea</code> <p>An optional main idea to incorporate into the prompt.</p> <code>''</code> <code>context_length</code> <p>The maximum length of the context window (default is 20000).</p> <code>20000</code> <p>Returns:</p> Name Type Description <code>str</code> <p>A formatted string representing the prompt for documentation enhancement.</p> Source code in <code>repo_agent/chat_engine.py</code> <pre><code>def build_prompt(self, doc_item: DocItem, main_idea=\"\", context_length=20000):\n    \"\"\"\n    Creates a prompt to improve code documentation by gathering information about the code\u2019s type, name, content, docstring, and file path. It also includes details of objects that call or are called by the current code, along with their documentation and code, to provide context. A user-defined main idea can be included to focus the enhancement process.\n\n    Args:\n        doc_item: A DocItem object containing code information and references.\n        main_idea: An optional main idea to incorporate into the prompt.\n        context_length: The maximum length of the context window (default is 20000).\n\n    Returns:\n        str: A formatted string representing the prompt for documentation enhancement.\n\n    \"\"\"\n\n    setting = SettingsManager.get_setting()\n    code_info = doc_item.content\n    referenced = len(doc_item.who_reference_me) &gt; 0 and len(code_info) &lt; 16000\n    code_type = code_info[\"type\"]\n    code_name = code_info[\"name\"]\n    code_content = code_info[\"code_content\"]\n    have_return = code_info[\"have_return\"]\n    docstring = (\n        code_info[\"md_content\"][-1]\n        if code_info[\"md_content\"]\n        else \"Empty docstring\"\n    )\n    file_path = doc_item.get_full_name()\n\n    def get_referenced_prompt(doc_item: DocItem) -&gt; str:\n        if len(doc_item.reference_who) == 0:\n            return \"\"\n        prompt = [\n            \"As you can see, the code calls the following objects, their code and docs are as following:\"\n        ]\n        for reference_item in doc_item.reference_who:\n            instance_prompt = (\n                f\"obj: {reference_item.get_full_name()}\\nDocument: \\n{(reference_item.md_content[-1] if len(reference_item.md_content) &gt; 0 else 'None')}\\nRaw code:```\\n{(reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else '')}\\n```\"\n                + \"=\" * 10\n            )\n            prompt.append(instance_prompt)\n        return \"\\n\".join(prompt)\n\n    def get_referencer_prompt(doc_item: DocItem) -&gt; str:\n        if len(doc_item.who_reference_me) == 0:\n            return \"\"\n        prompt = [\n            \"Also, the code has been called by the following objects, their code and docs are as following:\"\n        ]\n        for referencer_item in doc_item.who_reference_me:\n            instance_prompt = (\n                f\"obj: {referencer_item.get_full_name()}\\nDocument: \\n{(referencer_item.md_content[-1] if len(referencer_item.md_content) &gt; 0 else 'None')}\\nRaw code:```\\n{(referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None')}\\n```\"\n                + \"=\" * 10\n            )\n            prompt.append(instance_prompt)\n        return \"\\n\".join(prompt)\n\n    def get_relationship_description(referencer_content, reference_letter):\n        if referencer_content and reference_letter:\n            return \"And please include the reference relationship with its callers and callees in the project from a functional perspective\"\n        elif referencer_content:\n            return \"And please include the relationship with its callers in the project from a functional perspective.\"\n        elif reference_letter:\n            return \"And please include the relationship with its callees in the project from a functional perspective.\"\n        else:\n            return \"\"\n\n    code_type_tell = \"Class\" if code_type == \"ClassDef\" else \"Function\"\n    if referenced:\n        combine_ref_situation = (\n            \"and combine it with its calling situation in the project,\"\n        )\n        referencer_content = get_referencer_prompt(doc_item)\n        reference_letter = get_referenced_prompt(doc_item)\n        has_relationship = get_relationship_description(\n            referencer_content, reference_letter\n        )\n    else:\n        combine_ref_situation = \"\"\n        referencer_content = \"\"\n        reference_letter = \"\"\n        has_relationship = \"\"\n    if main_idea:\n        return docstring_update_chat_templates.format_messages(\n            combine_ref_situation=combine_ref_situation,\n            file_path=file_path,\n            code_type_tell=code_type_tell,\n            code_name=code_name,\n            main_idea=(\n                main_idea\n                if not main_idea\n                else f\"You can use user-defined main idea of the project to enhance exist docstring\\n{main_idea}\"\n            ),\n            docstring=docstring,\n            has_relationship=has_relationship,\n            reference_letter=reference_letter,\n            referencer_content=referencer_content,\n            language=setting.project.language,\n        )\n    else:\n        return chat_template.format_messages(\n            combine_ref_situation=combine_ref_situation,\n            file_path=file_path,\n            code_type_tell=code_type_tell,\n            code_name=code_name,\n            code_content=code_content,\n            main_idea=(\n                main_idea\n                if not main_idea\n                else f\"You can use user-defined main idea of the project to enhance exist docstring\\n{main_idea}\"\n            ),\n            docstring=docstring,\n            has_relationship=has_relationship,\n            reference_letter=reference_letter,\n            referencer_content=referencer_content,\n            language=setting.project.language,\n        )\n</code></pre>"},{"location":"repo_agent/chat_engine/#repo_agent.chat_engine.ChatEngine.generate_doc","title":"<code>generate_doc(doc_item)</code>","text":"<p>Constructs a prompt from the document item and queries a language model to generate documentation. Post-processes the response by removing code block markers.</p> <p>Parameters:</p> Name Type Description Default <code>doc_item</code> <code>DocItem</code> <p>The DocItem for which to generate documentation.</p> required Source code in <code>repo_agent/chat_engine.py</code> <pre><code>def generate_doc(self, doc_item: DocItem):\n    \"\"\"\n    Constructs a prompt from the document item and queries a language model to generate documentation. Post-processes the response by removing code block markers.\n\n    Args:\n        doc_item: The DocItem for which to generate documentation.\n    Returns:\n        str: The generated documentation string, with code block markers removed.\n    Raises:\n        Exception: If there is an error during the LLM chat call.\n\n\n    \"\"\"\n\n    settings = SettingsManager.get_setting()\n    if settings.project.main_idea:\n        messages = self.build_prompt(doc_item, main_idea=settings.project.main_idea)\n    else:\n        messages = self.build_prompt(doc_item)\n    try:\n        response = self.llm.chat(messages)\n        answer = response.message.content\n        return answer.replace(\"```python\\n\", \"\").replace(\"```\", \"\")\n    except Exception as e:\n        logger.error(f\"Error in llamaindex chat call: {e}\")\n        raise\n</code></pre>"},{"location":"repo_agent/chat_engine/#repo_agent.chat_engine.ChatEngine.generate_idea","title":"<code>generate_idea(list_items)</code>","text":"<p>Creates a novel idea based on the given components, adapting to configured language preferences. It communicates with a language model and records token consumption for tracking. Any issues during communication with the language model are logged and then propagated.</p> <p>Parameters:</p> Name Type Description Default <code>list_items</code> <code>str</code> <p>A string containing the list of items to base the idea on.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The generated idea as a string.  If an error occurs during the LLM chat call,  the exception is re-raised after logging.</p> Source code in <code>repo_agent/chat_engine.py</code> <pre><code>def generate_idea(self, list_items: str):\n    \"\"\"\n    Creates a novel idea based on the given components, adapting to configured language preferences. It communicates with a language model and records token consumption for tracking. Any issues during communication with the language model are logged and then propagated.\n\n    Args:\n        list_items: A string containing the list of items to base the idea on.\n\n    Returns:\n        str: The generated idea as a string.  If an error occurs during the LLM chat call,\n             the exception is re-raised after logging.\n\n\n    \"\"\"\n\n    settings = SettingsManager.get_setting()\n    messages = idea_chat_template.format_messages(\n        components=list_items, language=settings.project.language\n    )\n    try:\n        response = self.llm.chat(messages)\n        logger.debug(f\"LLM Prompt Tokens: {response.raw.usage.prompt_tokens}\")\n        logger.debug(\n            f\"LLM Completion Tokens: {response.raw.usage.completion_tokens}\"\n        )\n        logger.debug(f\"Total LLM Token Count: {response.raw.usage.total_tokens}\")\n        return response.message.content\n    except Exception as e:\n        logger.error(f\"Error in llamaindex chat call: {e}\")\n        raise\n</code></pre>"},{"location":"repo_agent/chat_engine/#repo_agent.chat_engine.ChatEngine.summarize_module","title":"<code>summarize_module(module_desc)</code>","text":"<p>Creates a succinct summary of a module\u2019s purpose, informed by project configuration and language preferences. This is achieved through interaction with a large language model, with token usage tracked for monitoring.</p> <p>Parameters:</p> Name Type Description Default <code>module_desc</code> <code>str</code> <p>The description of the module to be summarized.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The summary generated by the LLM.  Raises any exceptions encountered during the LLM call.</p> Source code in <code>repo_agent/chat_engine.py</code> <pre><code>def summarize_module(self, module_desc: str):\n    \"\"\"\n    Creates a succinct summary of a module\u2019s purpose, informed by project configuration and language preferences. This is achieved through interaction with a large language model, with token usage tracked for monitoring.\n\n    Args:\n        module_desc: The description of the module to be summarized.\n\n    Returns:\n        str: The summary generated by the LLM.  Raises any exceptions encountered during the LLM call.\n\n\n    \"\"\"\n\n    settings = SettingsManager.get_setting()\n    messages = module_summary_template.format_messages(\n        components=module_desc,\n        main_idea=settings.project.main_idea,\n        language=settings.project.language,\n    )\n    try:\n        response = self.llm.chat(messages)\n        logger.debug(f\"LLM Prompt Tokens: {response.raw.usage.prompt_tokens}\")\n        logger.debug(\n            f\"LLM Completion Tokens: {response.raw.usage.completion_tokens}\"\n        )\n        logger.debug(f\"Total LLM Token Count: {response.raw.usage.total_tokens}\")\n        return response.message.content\n    except Exception as e:\n        logger.error(f\"Error in llamaindex chat call: {e}\")\n        raise\n</code></pre>"},{"location":"repo_agent/doc_meta_info/","title":"Doc Meta Info","text":"<p>\u5b58\u50a8doc\u5bf9\u5e94\u7684\u4fe1\u606f\uff0c\u540c\u65f6\u5904\u7406\u5f15\u7528\u7684\u5173\u7cfb</p>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.DocItem","title":"<code>DocItem</code>  <code>dataclass</code>","text":"<p>Represents a documentation item within a code repository.</p> <p>This class stores information about various elements in the codebase, such as functions, classes, and modules, along with their relationships and associated metadata for documentation generation.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>@dataclass\nclass DocItem:\n    \"\"\"\n    Represents a documentation item within a code repository.\n\n    This class stores information about various elements in the codebase,\n    such as functions, classes, and modules, along with their relationships\n    and associated metadata for documentation generation.\n    \"\"\"\n\n    item_type: DocItemType = DocItemType._class_function\n    item_status: DocItemStatus = DocItemStatus.doc_has_not_been_generated\n    obj_name: str = \"\"\n    code_start_line: int = -1\n    code_end_line: int = -1\n    source_node: Optional[ast.__ast.stmt] = None\n    md_content: List[str] = field(default_factory=list)\n    content: Dict[Any, Any] = field(default_factory=dict)\n    children: Dict[str, DocItem] = field(default_factory=dict)\n    father: Any[DocItem] = None\n    depth: int = 0\n    tree_path: List[DocItem] = field(default_factory=list)\n    max_reference_ansce: Any[DocItem] = None\n    reference_who: List[DocItem] = field(default_factory=list)\n    who_reference_me: List[DocItem] = field(default_factory=list)\n    special_reference_type: List[bool] = field(default_factory=list)\n    reference_who_name_list: List[str] = field(default_factory=list)\n    who_reference_me_name_list: List[str] = field(default_factory=list)\n    has_task: bool = False\n    multithread_task_id: int = -1\n\n    @staticmethod\n    def has_ans_relation(now_a: DocItem, now_b: DocItem):\n        \"\"\"\n        Determines if a direct hierarchical relationship exists between two DocItems.\n\n\n\n        Args:\n            now_a: The first DocItem.\n            now_b: The second DocItem.\n\n        Returns:\n            The DocItem that is the ancestor or successor, or None if no such relation exists.\n\n        \"\"\"\n\n        if now_b in now_a.tree_path:\n            return now_b\n        if now_a in now_b.tree_path:\n            return now_a\n        return None\n\n    def get_travel_list(self):\n        \"\"\"\n        Collects this node and all its descendants into a single list, traversing the tree structure.\n\n        Args:\n            None\n\n        Returns:\n            list: A list containing the current node and all its descendants.\n\n        \"\"\"\n\n        now_list = [self]\n        for _, child in self.children.items():\n            now_list = now_list + child.get_travel_list()\n        return now_list\n\n    def check_depth(self):\n        \"\"\"\n        Determines the depth of the subtree rooted at this node.\n\n        The depth represents the longest path from this node to a leaf node, measured in edges.\n\n        The depth is defined as the number of edges on the longest path from this node to a leaf.\n\n        Args:\n            None\n\n        Returns:\n            int: The depth of the tree.\n\n        \"\"\"\n\n        if len(self.children) == 0:\n            self.depth = 0\n            return self.depth\n        max_child_depth = 0\n        for _, child in self.children.items():\n            child_depth = child.check_depth()\n            max_child_depth = max(child_depth, max_child_depth)\n        self.depth = max_child_depth + 1\n        return self.depth\n\n    def parse_tree_path(self, now_path):\n        \"\"\"\n        Recursively constructs the path from the root to this node within a documentation tree.\n\n        Args:\n            now_path: The current path being built.\n\n        Returns:\n            None\n\n        \"\"\"\n\n        self.tree_path = now_path + [self]\n        for key, child in self.children.items():\n            child.parse_tree_path(self.tree_path)\n\n    def get_file_name(self):\n        \"\"\"\n        Returns the base name of the documented item, including the .py extension.\n\n        Args:\n            stri: Not used. Included for compatibility with parent class method signature.\n\n        Returns:\n            str: The file name of the module (without path) including the .py extension.\n\n        \"\"\"\n\n        full_name = self.get_full_name()\n        return full_name.split(\".py\")[0] + \".py\"\n\n    def get_full_name(self, strict=False):\n        \"\"\"\n        Constructs the complete, hierarchical name of the object by ascending through its parent relationships. Ensures uniqueness within each level of the hierarchy when requested, appending a version indicator to duplicated names.\n\n        Args:\n            strict: If True, ensures uniqueness of names in the path by appending\n                '(name_duplicate_version)' if a duplicate is found.\n\n        Returns:\n            str: The full name of the object as a string, with components joined by '/'.\n\n        \"\"\"\n\n        if self.father == None:\n            return self.obj_name\n        name_list = []\n        now = self\n        while now != None:\n            self_name = now.obj_name\n            if strict:\n                for name, item in self.father.children.items():\n                    if item == now:\n                        self_name = name\n                        break\n                if self_name != now.obj_name:\n                    self_name = self_name + \"(name_duplicate_version)\"\n            name_list = [self_name] + name_list\n            now = now.father\n        name_list = name_list[1:]\n        return \"/\".join(name_list)\n\n    def find(self, recursive_file_path: list) -&gt; Optional[DocItem]:\n        \"\"\"\n        Locates a DocItem by traversing the repository structure using a list representing the path to the item. Returns the found DocItem or None if the path is invalid.\n\n        Args:\n            recursive_file_path: A list representing the path to the desired DocItem\n                within the repository structure. Each element in the list is a key\n                representing a directory or item name.\n\n        Returns:\n            DocItem: The DocItem found at the specified recursive file path, or None if\n                the path does not exist.\n\n\n        \"\"\"\n\n        assert self.item_type == DocItemType._repo\n        pos = 0\n        now = self\n        while pos &lt; len(recursive_file_path):\n            if not recursive_file_path[pos] in now.children.keys():\n                return None\n            now = now.children[recursive_file_path[pos]]\n            pos += 1\n        return now\n\n    @staticmethod\n    def check_has_task(now_item: DocItem, ignore_list: List[str] = []):\n        \"\"\"\n        Recursively checks if the current DocItem or any of its descendants require processing, updating the `has_task` attribute accordingly.\n\n        Args:\n            now_item: The DocItem to check.\n            ignore_list: A list of strings representing items to ignore during the check.\n\n        Returns:\n            None\n\n        \"\"\"\n\n        if need_to_generate(now_item, ignore_list=ignore_list):\n            now_item.has_task = True\n        for _, child in now_item.children.items():\n            DocItem.check_has_task(child, ignore_list)\n            now_item.has_task = child.has_task or now_item.has_task\n\n    def print_recursive(\n        self,\n        indent=0,\n        print_content=False,\n        diff_status=False,\n        ignore_list: List[str] = [],\n    ):\n        \"\"\"\n        Prints the object and its children in a tree-like structure, indicating status changes where applicable. The output is indented to represent the hierarchy of objects.\n\n        Args:\n            indent: The level of indentation for printing.\n            print_content: Whether to print the content of the object. Not used in this method, but passed down recursively.\n            diff_status: Whether to include diff status in the output.\n            ignore_list: A list of strings to ignore when determining if a diff is needed.\n\n        Returns:\n            None\n\n\n        \"\"\"\n\n        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \" * indent + \"|-\"\n\n        print_obj_name = self.obj_name\n        setting = SettingsManager.get_setting()\n        if self.item_type == DocItemType._repo:\n            print_obj_name = setting.project.target_repo\n        if diff_status and need_to_generate(self, ignore_list=ignore_list):\n            print(\n                print_indent(indent)\n                + f\"{self.item_type.print_self()}: {print_obj_name} : {self.item_status.name}\"\n            )\n        else:\n            print(\n                print_indent(indent)\n                + f\"{self.item_type.print_self()}: {print_obj_name}\"\n            )\n        for child_name, child in self.children.items():\n            if diff_status and child.has_task == False:\n                continue\n            child.print_recursive(\n                indent=indent + 1,\n                print_content=print_content,\n                diff_status=diff_status,\n                ignore_list=ignore_list,\n            )\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.DocItem.check_depth","title":"<code>check_depth()</code>","text":"<p>Determines the depth of the subtree rooted at this node.</p> <p>The depth represents the longest path from this node to a leaf node, measured in edges.</p> <p>The depth is defined as the number of edges on the longest path from this node to a leaf.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The depth of the tree.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def check_depth(self):\n    \"\"\"\n    Determines the depth of the subtree rooted at this node.\n\n    The depth represents the longest path from this node to a leaf node, measured in edges.\n\n    The depth is defined as the number of edges on the longest path from this node to a leaf.\n\n    Args:\n        None\n\n    Returns:\n        int: The depth of the tree.\n\n    \"\"\"\n\n    if len(self.children) == 0:\n        self.depth = 0\n        return self.depth\n    max_child_depth = 0\n    for _, child in self.children.items():\n        child_depth = child.check_depth()\n        max_child_depth = max(child_depth, max_child_depth)\n    self.depth = max_child_depth + 1\n    return self.depth\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.DocItem.check_has_task","title":"<code>check_has_task(now_item, ignore_list=[])</code>  <code>staticmethod</code>","text":"<p>Recursively checks if the current DocItem or any of its descendants require processing, updating the <code>has_task</code> attribute accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>now_item</code> <code>DocItem</code> <p>The DocItem to check.</p> required <code>ignore_list</code> <code>List[str]</code> <p>A list of strings representing items to ignore during the check.</p> <code>[]</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>@staticmethod\ndef check_has_task(now_item: DocItem, ignore_list: List[str] = []):\n    \"\"\"\n    Recursively checks if the current DocItem or any of its descendants require processing, updating the `has_task` attribute accordingly.\n\n    Args:\n        now_item: The DocItem to check.\n        ignore_list: A list of strings representing items to ignore during the check.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    if need_to_generate(now_item, ignore_list=ignore_list):\n        now_item.has_task = True\n    for _, child in now_item.children.items():\n        DocItem.check_has_task(child, ignore_list)\n        now_item.has_task = child.has_task or now_item.has_task\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.DocItem.find","title":"<code>find(recursive_file_path)</code>","text":"<p>Locates a DocItem by traversing the repository structure using a list representing the path to the item. Returns the found DocItem or None if the path is invalid.</p> <p>Parameters:</p> Name Type Description Default <code>recursive_file_path</code> <code>list</code> <p>A list representing the path to the desired DocItem within the repository structure. Each element in the list is a key representing a directory or item name.</p> required <p>Returns:</p> Name Type Description <code>DocItem</code> <code>Optional[DocItem]</code> <p>The DocItem found at the specified recursive file path, or None if the path does not exist.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def find(self, recursive_file_path: list) -&gt; Optional[DocItem]:\n    \"\"\"\n    Locates a DocItem by traversing the repository structure using a list representing the path to the item. Returns the found DocItem or None if the path is invalid.\n\n    Args:\n        recursive_file_path: A list representing the path to the desired DocItem\n            within the repository structure. Each element in the list is a key\n            representing a directory or item name.\n\n    Returns:\n        DocItem: The DocItem found at the specified recursive file path, or None if\n            the path does not exist.\n\n\n    \"\"\"\n\n    assert self.item_type == DocItemType._repo\n    pos = 0\n    now = self\n    while pos &lt; len(recursive_file_path):\n        if not recursive_file_path[pos] in now.children.keys():\n            return None\n        now = now.children[recursive_file_path[pos]]\n        pos += 1\n    return now\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.DocItem.get_file_name","title":"<code>get_file_name()</code>","text":"<p>Returns the base name of the documented item, including the .py extension.</p> <p>Parameters:</p> Name Type Description Default <code>stri</code> <p>Not used. Included for compatibility with parent class method signature.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The file name of the module (without path) including the .py extension.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def get_file_name(self):\n    \"\"\"\n    Returns the base name of the documented item, including the .py extension.\n\n    Args:\n        stri: Not used. Included for compatibility with parent class method signature.\n\n    Returns:\n        str: The file name of the module (without path) including the .py extension.\n\n    \"\"\"\n\n    full_name = self.get_full_name()\n    return full_name.split(\".py\")[0] + \".py\"\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.DocItem.get_full_name","title":"<code>get_full_name(strict=False)</code>","text":"<p>Constructs the complete, hierarchical name of the object by ascending through its parent relationships. Ensures uniqueness within each level of the hierarchy when requested, appending a version indicator to duplicated names.</p> <p>Parameters:</p> Name Type Description Default <code>strict</code> <p>If True, ensures uniqueness of names in the path by appending '(name_duplicate_version)' if a duplicate is found.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The full name of the object as a string, with components joined by '/'.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def get_full_name(self, strict=False):\n    \"\"\"\n    Constructs the complete, hierarchical name of the object by ascending through its parent relationships. Ensures uniqueness within each level of the hierarchy when requested, appending a version indicator to duplicated names.\n\n    Args:\n        strict: If True, ensures uniqueness of names in the path by appending\n            '(name_duplicate_version)' if a duplicate is found.\n\n    Returns:\n        str: The full name of the object as a string, with components joined by '/'.\n\n    \"\"\"\n\n    if self.father == None:\n        return self.obj_name\n    name_list = []\n    now = self\n    while now != None:\n        self_name = now.obj_name\n        if strict:\n            for name, item in self.father.children.items():\n                if item == now:\n                    self_name = name\n                    break\n            if self_name != now.obj_name:\n                self_name = self_name + \"(name_duplicate_version)\"\n        name_list = [self_name] + name_list\n        now = now.father\n    name_list = name_list[1:]\n    return \"/\".join(name_list)\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.DocItem.get_travel_list","title":"<code>get_travel_list()</code>","text":"<p>Collects this node and all its descendants into a single list, traversing the tree structure.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list containing the current node and all its descendants.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def get_travel_list(self):\n    \"\"\"\n    Collects this node and all its descendants into a single list, traversing the tree structure.\n\n    Args:\n        None\n\n    Returns:\n        list: A list containing the current node and all its descendants.\n\n    \"\"\"\n\n    now_list = [self]\n    for _, child in self.children.items():\n        now_list = now_list + child.get_travel_list()\n    return now_list\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.DocItem.has_ans_relation","title":"<code>has_ans_relation(now_a, now_b)</code>  <code>staticmethod</code>","text":"<p>Determines if a direct hierarchical relationship exists between two DocItems.</p> <p>Parameters:</p> Name Type Description Default <code>now_a</code> <code>DocItem</code> <p>The first DocItem.</p> required <code>now_b</code> <code>DocItem</code> <p>The second DocItem.</p> required <p>Returns:</p> Type Description <p>The DocItem that is the ancestor or successor, or None if no such relation exists.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>@staticmethod\ndef has_ans_relation(now_a: DocItem, now_b: DocItem):\n    \"\"\"\n    Determines if a direct hierarchical relationship exists between two DocItems.\n\n\n\n    Args:\n        now_a: The first DocItem.\n        now_b: The second DocItem.\n\n    Returns:\n        The DocItem that is the ancestor or successor, or None if no such relation exists.\n\n    \"\"\"\n\n    if now_b in now_a.tree_path:\n        return now_b\n    if now_a in now_b.tree_path:\n        return now_a\n    return None\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.DocItem.parse_tree_path","title":"<code>parse_tree_path(now_path)</code>","text":"<p>Recursively constructs the path from the root to this node within a documentation tree.</p> <p>Parameters:</p> Name Type Description Default <code>now_path</code> <p>The current path being built.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def parse_tree_path(self, now_path):\n    \"\"\"\n    Recursively constructs the path from the root to this node within a documentation tree.\n\n    Args:\n        now_path: The current path being built.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    self.tree_path = now_path + [self]\n    for key, child in self.children.items():\n        child.parse_tree_path(self.tree_path)\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.DocItem.print_recursive","title":"<code>print_recursive(indent=0, print_content=False, diff_status=False, ignore_list=[])</code>","text":"<p>Prints the object and its children in a tree-like structure, indicating status changes where applicable. The output is indented to represent the hierarchy of objects.</p> <p>Parameters:</p> Name Type Description Default <code>indent</code> <p>The level of indentation for printing.</p> <code>0</code> <code>print_content</code> <p>Whether to print the content of the object. Not used in this method, but passed down recursively.</p> <code>False</code> <code>diff_status</code> <p>Whether to include diff status in the output.</p> <code>False</code> <code>ignore_list</code> <code>List[str]</code> <p>A list of strings to ignore when determining if a diff is needed.</p> <code>[]</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def print_recursive(\n    self,\n    indent=0,\n    print_content=False,\n    diff_status=False,\n    ignore_list: List[str] = [],\n):\n    \"\"\"\n    Prints the object and its children in a tree-like structure, indicating status changes where applicable. The output is indented to represent the hierarchy of objects.\n\n    Args:\n        indent: The level of indentation for printing.\n        print_content: Whether to print the content of the object. Not used in this method, but passed down recursively.\n        diff_status: Whether to include diff status in the output.\n        ignore_list: A list of strings to ignore when determining if a diff is needed.\n\n    Returns:\n        None\n\n\n    \"\"\"\n\n    def print_indent(indent=0):\n        if indent == 0:\n            return \"\"\n        return \"  \" * indent + \"|-\"\n\n    print_obj_name = self.obj_name\n    setting = SettingsManager.get_setting()\n    if self.item_type == DocItemType._repo:\n        print_obj_name = setting.project.target_repo\n    if diff_status and need_to_generate(self, ignore_list=ignore_list):\n        print(\n            print_indent(indent)\n            + f\"{self.item_type.print_self()}: {print_obj_name} : {self.item_status.name}\"\n        )\n    else:\n        print(\n            print_indent(indent)\n            + f\"{self.item_type.print_self()}: {print_obj_name}\"\n        )\n    for child_name, child in self.children.items():\n        if diff_status and child.has_task == False:\n            continue\n        child.print_recursive(\n            indent=indent + 1,\n            print_content=print_content,\n            diff_status=diff_status,\n            ignore_list=ignore_list,\n        )\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.DocItemStatus","title":"<code>DocItemStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Represents the status of documentation items in relation to their code.</p> <p>This class is used to track whether documentation for a given item (e.g., a function, class, or module) is up-to-date, has been generated, and if the underlying code has changed since the last documentation generation. It also manages flags related to referencers.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>@unique\nclass DocItemStatus(Enum):\n    \"\"\"\n    Represents the status of documentation items in relation to their code.\n\n    This class is used to track whether documentation for a given item (e.g., a function,\n    class, or module) is up-to-date, has been generated, and if the underlying code\n    has changed since the last documentation generation. It also manages flags related\n    to referencers.\n    \"\"\"\n\n    doc_up_to_date = auto()\n    doc_has_not_been_generated = auto()\n    code_changed = auto()\n    add_new_referencer = auto()\n    referencer_not_exist = auto()\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.DocItemType","title":"<code>DocItemType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Represents the type of a documentation item found in a repository.</p> <p>This class provides an enum-like structure to categorize different elements within code documentation, such as classes, functions, directories, and files. It also includes methods for converting the item type to a string and printing it with color coding.</p> <p>Class Attributes: - _repo - _dir - _file - _class - _class_function - _function - _sub_function - _global_var</p> <p>Class Methods: - to_str:</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>@unique\nclass DocItemType(Enum):\n    \"\"\"\n    Represents the type of a documentation item found in a repository.\n\n    This class provides an enum-like structure to categorize different elements\n    within code documentation, such as classes, functions, directories, and files.\n    It also includes methods for converting the item type to a string\n    and printing it with color coding.\n\n    Class Attributes:\n    - _repo\n    - _dir\n    - _file\n    - _class\n    - _class_function\n    - _function\n    - _sub_function\n    - _global_var\n\n    Class Methods:\n    - to_str:\n    \"\"\"\n\n    _repo = auto()\n    _dir = auto()\n    _file = auto()\n    _class = auto()\n    _class_function = auto()\n    _function = auto()\n    _sub_function = auto()\n    _global_var = auto()\n\n    def to_str(self):\n        \"\"\"\n        Returns a string representation of the DocItemType, such as 'ClassDef', 'FunctionDef', or 'Dir'. If the type is not explicitly mapped, it returns the item's name.\n\n        Args:\n            self: The DocItemType enum instance.\n\n        Returns:\n            str: A string representing the DocItemType, such as 'ClassDef',\n                 'FunctionDef', or 'Dir'.  If the type is not recognized,\n                 returns the name of the item.\n\n        \"\"\"\n\n        if self == DocItemType._class:\n            return \"ClassDef\"\n        elif self == DocItemType._function:\n            return \"FunctionDef\"\n        elif self == DocItemType._class_function:\n            return \"FunctionDef\"\n        elif self == DocItemType._sub_function:\n            return \"FunctionDef\"\n        elif self == DocItemType._dir:\n            return \"Dir\"\n        return self.name\n\n    def print_self(self):\n        \"\"\"\n        Formats the DocItem name with color-coding to visually distinguish its type.\n\n        Args:\n            self: The DocItem object whose name should be printed.\n\n        Returns:\n            str: A colored string representing the name of the DocItem,\n                 with Style.RESET_ALL appended to reset the color.\n\n        \"\"\"\n\n        color = Fore.WHITE\n        if self == DocItemType._dir:\n            color = Fore.GREEN\n        elif self == DocItemType._file:\n            color = Fore.YELLOW\n        elif self == DocItemType._class:\n            color = Fore.RED\n        elif self in [\n            DocItemType._function,\n            DocItemType._sub_function,\n            DocItemType._class_function,\n        ]:\n            color = Fore.BLUE\n        return color + self.name + Style.RESET_ALL\n\n    def get_edge_type(self, from_item_type: DocItemType, to_item_type: DocItemType):\n        \"\"\"\n        Determines the relationship between two documentation items.\n\n        Args:\n            type: The source documentation item type.\n            to_item_type: The destination documentation item type.\n\n        Returns:\n            A string representing the connection between the two items.\n\n\n                Args:\n                    type: The source DocItemType.\n                    to_item_type: The destination DocItemType.\n\n                Returns:\n                    str: The edge type string representation.\n\n        \"\"\"\n\n        pass\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.DocItemType.get_edge_type","title":"<code>get_edge_type(from_item_type, to_item_type)</code>","text":"<p>Determines the relationship between two documentation items.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <p>The source documentation item type.</p> required <code>to_item_type</code> <code>DocItemType</code> <p>The destination documentation item type.</p> required <p>Returns:</p> Type Description <p>A string representing the connection between the two items.</p> <p>Args:     type: The source DocItemType.     to_item_type: The destination DocItemType.</p> <p>Returns:     str: The edge type string representation.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def get_edge_type(self, from_item_type: DocItemType, to_item_type: DocItemType):\n    \"\"\"\n    Determines the relationship between two documentation items.\n\n    Args:\n        type: The source documentation item type.\n        to_item_type: The destination documentation item type.\n\n    Returns:\n        A string representing the connection between the two items.\n\n\n            Args:\n                type: The source DocItemType.\n                to_item_type: The destination DocItemType.\n\n            Returns:\n                str: The edge type string representation.\n\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.DocItemType.print_self","title":"<code>print_self()</code>","text":"<p>Formats the DocItem name with color-coding to visually distinguish its type.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>The DocItem object whose name should be printed.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A colored string representing the name of the DocItem,  with Style.RESET_ALL appended to reset the color.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def print_self(self):\n    \"\"\"\n    Formats the DocItem name with color-coding to visually distinguish its type.\n\n    Args:\n        self: The DocItem object whose name should be printed.\n\n    Returns:\n        str: A colored string representing the name of the DocItem,\n             with Style.RESET_ALL appended to reset the color.\n\n    \"\"\"\n\n    color = Fore.WHITE\n    if self == DocItemType._dir:\n        color = Fore.GREEN\n    elif self == DocItemType._file:\n        color = Fore.YELLOW\n    elif self == DocItemType._class:\n        color = Fore.RED\n    elif self in [\n        DocItemType._function,\n        DocItemType._sub_function,\n        DocItemType._class_function,\n    ]:\n        color = Fore.BLUE\n    return color + self.name + Style.RESET_ALL\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.DocItemType.to_str","title":"<code>to_str()</code>","text":"<p>Returns a string representation of the DocItemType, such as 'ClassDef', 'FunctionDef', or 'Dir'. If the type is not explicitly mapped, it returns the item's name.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>The DocItemType enum instance.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A string representing the DocItemType, such as 'ClassDef',  'FunctionDef', or 'Dir'.  If the type is not recognized,  returns the name of the item.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def to_str(self):\n    \"\"\"\n    Returns a string representation of the DocItemType, such as 'ClassDef', 'FunctionDef', or 'Dir'. If the type is not explicitly mapped, it returns the item's name.\n\n    Args:\n        self: The DocItemType enum instance.\n\n    Returns:\n        str: A string representing the DocItemType, such as 'ClassDef',\n             'FunctionDef', or 'Dir'.  If the type is not recognized,\n             returns the name of the item.\n\n    \"\"\"\n\n    if self == DocItemType._class:\n        return \"ClassDef\"\n    elif self == DocItemType._function:\n        return \"FunctionDef\"\n    elif self == DocItemType._class_function:\n        return \"FunctionDef\"\n    elif self == DocItemType._sub_function:\n        return \"FunctionDef\"\n    elif self == DocItemType._dir:\n        return \"Dir\"\n    return self.name\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.EdgeType","title":"<code>EdgeType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Represents the type of edge in a graph, categorizing relationships between nodes.</p> <p>This class serves as an enumeration for different edge types used to represent relationships such as references, subfile inclusions, and file-item associations.</p> <p>Class Attributes: - reference_edge - subfile_edge - file_item_edge</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>@unique\nclass EdgeType(Enum):\n    \"\"\"\n    Represents the type of edge in a graph, categorizing relationships between nodes.\n\n    This class serves as an enumeration for different edge types used to represent\n    relationships such as references, subfile inclusions, and file-item associations.\n\n    Class Attributes:\n    - reference_edge\n    - subfile_edge\n    - file_item_edge\n    \"\"\"\n\n    reference_edge = auto()\n    subfile_edge = auto()\n    file_item_edge = auto()\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.MetaInfo","title":"<code>MetaInfo</code>  <code>dataclass</code>","text":"<p>MetaInfo class for managing and representing project metadata.</p> <p>This class stores information about a software repository, including its structure, files, references, and task dependencies. It provides methods for loading, saving, and manipulating this metadata to support documentation generation and analysis.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>@dataclass\nclass MetaInfo:\n    \"\"\"\n    MetaInfo class for managing and representing project metadata.\n\n    This class stores information about a software repository, including its structure,\n    files, references, and task dependencies. It provides methods for loading, saving,\n    and manipulating this metadata to support documentation generation and analysis.\n    \"\"\"\n\n    repo_path: Path = \"\"\n    document_version: str = \"\"\n    main_idea: str = \"\"\n    repo_structure: Dict[str, Any] = field(default_factory=dict)\n    target_repo_hierarchical_tree: \"DocItem\" = field(default_factory=lambda: DocItem())\n    white_list: Any[List] = None\n    fake_file_reflection: Dict[str, str] = field(default_factory=dict)\n    jump_files: List[str] = field(default_factory=list)\n    deleted_items_from_older_meta: List[List] = field(default_factory=list)\n    in_generation_process: bool = False\n    checkpoint_lock: threading.Lock = threading.Lock()\n\n    @staticmethod\n    def init_meta_info(file_path_reflections, jump_files) -&gt; MetaInfo:\n        \"\"\"\n        Creates a MetaInfo object representing the project\u2019s structure by analyzing files and directories.\n\n        Args:\n            file_path_reflections: The file path reflections to use.\n            jump_files: The jump files to use.\n\n        Returns:\n            MetaInfo: A MetaInfo object representing the project's metadata.\n\n\n        \"\"\"\n\n        setting = SettingsManager.get_setting()\n        project_abs_path = setting.project.target_repo\n        print(\n            f\"{Fore.LIGHTRED_EX}Initializing MetaInfo: {Style.RESET_ALL}from {project_abs_path}\"\n        )\n        file_handler = FileHandler(project_abs_path, None)\n        repo_structure = file_handler.generate_overall_structure(\n            file_path_reflections, jump_files\n        )\n        metainfo = MetaInfo.from_project_hierarchy_json(repo_structure)\n        metainfo.repo_path = project_abs_path\n        metainfo.fake_file_reflection = file_path_reflections\n        metainfo.jump_files = jump_files\n        return metainfo\n\n    @staticmethod\n    def from_checkpoint_path(\n        checkpoint_dir_path: Path, repo_structure: Optional[Dict[str, Any]] = None\n    ) -&gt; MetaInfo:\n        \"\"\"\n        Loads project metadata from a checkpoint directory to restore a previous state.\n\n        Args:\n            checkpoint_dir_path: The path to the checkpoint directory.\n            t_dir_path:  The path to the temporary directory.\n            repo_structure: An optional dictionary representing the repository structure.\n\n        Returns:\n            MetaInfo: A MetaInfo object loaded from the checkpoint data.\n\n\n        \"\"\"\n\n        setting = SettingsManager.get_setting()\n        project_hierarchy_json_path = checkpoint_dir_path / \"project_hierarchy.json\"\n        with open(project_hierarchy_json_path, \"r\", encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        metainfo = MetaInfo.from_project_hierarchy_json(\n            project_hierarchy_json, repo_structure\n        )\n        with open(\n            checkpoint_dir_path / \"meta-info.json\", \"r\", encoding=\"utf-8\"\n        ) as reader:\n            meta_data = json.load(reader)\n            metainfo.repo_path = setting.project.target_repo\n            metainfo.main_idea = meta_data[\"main_idea\"]\n            metainfo.document_version = meta_data[\"doc_version\"]\n            metainfo.fake_file_reflection = meta_data[\"fake_file_reflection\"]\n            metainfo.jump_files = meta_data[\"jump_files\"]\n            metainfo.in_generation_process = meta_data[\"in_generation_process\"]\n            metainfo.deleted_items_from_older_meta = meta_data[\n                \"deleted_items_from_older_meta\"\n            ]\n        print(f\"{Fore.CYAN}Loading MetaInfo:{Style.RESET_ALL} {checkpoint_dir_path}\")\n        return metainfo\n\n    def checkpoint(self, target_dir_path: str | Path, flash_reference_relation=False):\n        \"\"\"\n        Persists the project\u2019s metadata and hierarchy to a specified directory, ensuring data is saved for later use or recovery. Includes options to control the level of detail in the saved hierarchy representation.\n\n        Args:\n            target_dir_path: The path to the directory where the checkpoint should be saved.\n            flash_reference_relation: A boolean indicating whether to include flash reference relations in the hierarchy JSON.\n\n        Returns:\n            None\n\n\n        \"\"\"\n\n        with self.checkpoint_lock:\n            target_dir = Path(target_dir_path)\n            logger.debug(f\"Checkpointing MetaInfo to directory: {target_dir}\")\n            print(f\"{Fore.GREEN}MetaInfo is Refreshed and Saved{Style.RESET_ALL}\")\n            if not target_dir.exists():\n                target_dir.mkdir(parents=True, exist_ok=True)\n                logger.debug(f\"Created directory: {target_dir}\")\n            now_hierarchy_json = self.to_hierarchy_json(\n                flash_reference_relation=flash_reference_relation\n            )\n            hierarchy_file = target_dir / \"project_hierarchy.json\"\n            try:\n                with hierarchy_file.open(\"w\", encoding=\"utf-8\") as writer:\n                    json.dump(now_hierarchy_json, writer, indent=2, ensure_ascii=False)\n                logger.debug(f\"Saved hierarchy JSON to {hierarchy_file}\")\n            except IOError as e:\n                logger.error(f\"Failed to save hierarchy JSON to {hierarchy_file}: {e}\")\n            meta_info_file = target_dir / \"meta-info.json\"\n            meta = {\n                \"main_idea\": SettingsManager().get_setting().project.main_idea,\n                \"doc_version\": self.document_version,\n                \"in_generation_process\": self.in_generation_process,\n                \"fake_file_reflection\": self.fake_file_reflection,\n                \"jump_files\": self.jump_files,\n                \"deleted_items_from_older_meta\": self.deleted_items_from_older_meta,\n            }\n            try:\n                with meta_info_file.open(\"w\", encoding=\"utf-8\") as writer:\n                    json.dump(meta, writer, indent=2, ensure_ascii=False)\n                logger.debug(f\"Saved meta-info JSON to {meta_info_file}\")\n            except IOError as e:\n                logger.error(f\"Failed to save meta-info JSON to {meta_info_file}: {e}\")\n\n    def print_task_list(self, task_dict: Dict[Task]):\n        \"\"\"\n        Displays a table summarizing task details, including ID, reason for documentation generation, file path, and dependencies. Dependency lists are truncated for brevity if they exceed a certain length.\n\n        Args:\n            task_dict: A dictionary where keys are task IDs and values are Task objects.\n\n        Returns:\n            None\n\n\n        \"\"\"\n\n        task_table = PrettyTable(\n            [\"task_id\", \"Doc Generation Reason\", \"Path\", \"dependency\"]\n        )\n        for task_id, task_info in task_dict.items():\n            remain_str = \"None\"\n            if task_info.dependencies != []:\n                remain_str = \",\".join(\n                    [str(d_task.task_id) for d_task in task_info.dependencies]\n                )\n                if len(remain_str) &gt; 20:\n                    remain_str = remain_str[:8] + \"...\" + remain_str[-8:]\n            task_table.add_row(\n                [\n                    task_id,\n                    task_info.extra_info.item_status.name,\n                    task_info.extra_info.get_full_name(strict=True),\n                    remain_str,\n                ]\n            )\n        print(task_table)\n\n    def get_all_files(self, count_repo=False) -&gt; List[DocItem]:\n        \"\"\"\n        Returns a list of all items \u2013 files and directories \u2013 within the project's structure. Optionally includes the root repository item in the results.\n\n        Args:\n            count_repo: Whether to include the root repo item in the results.\n\n        Returns:\n            A list of DocItem objects representing all files and directories.\n\n\n        \"\"\"\n\n        files = []\n\n        def walk_tree(now_node):\n            if now_node.item_type in [DocItemType._file, DocItemType._dir]:\n                files.append(now_node)\n            if count_repo and now_node.item_type == DocItemType._repo:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n\n        walk_tree(self.target_repo_hierarchical_tree)\n        return files\n\n    def find_obj_with_lineno(self, file_node: DocItem, start_line_num) -&gt; DocItem:\n        \"\"\"\n        No valid docstring found.\n\n        \"\"\"\n\n        now_node = file_node\n        assert now_node != None\n        while len(now_node.children) &gt; 0:\n            find_qualify_child = False\n            for _, child in now_node.children.items():\n                assert child.content != None\n                if (\n                    child.content[\"code_start_line\"] &lt;= start_line_num\n                    and child.content[\"code_end_line\"] &gt;= start_line_num\n                ):\n                    now_node = child\n                    find_qualify_child = True\n                    break\n            if not find_qualify_child:\n                return now_node\n        return now_node\n\n    def parse_reference(self):\n        \"\"\"\n        No valid docstring found.\n\n        \"\"\"\n\n        file_nodes = self.get_all_files()\n        white_list_file_names, white_list_obj_names = ([], [])\n        if self.white_list != None:\n            white_list_file_names = [cont[\"file_path\"] for cont in self.white_list]\n            white_list_obj_names = [cont[\"id_text\"] for cont in self.white_list]\n        for file_node in tqdm(file_nodes, desc=\"parsing bidirectional reference\"):\n            \"\u68c0\u6d4b\u4e00\u4e2a\u6587\u4ef6\u5185\u7684\u6240\u6709\u5f15\u7528\u4fe1\u606f\uff0c\u53ea\u80fd\u68c0\u6d4b\u5f15\u7528\u8be5\u6587\u4ef6\u5185\u67d0\u4e2aobj\u7684\u5176\u4ed6\u5185\u5bb9\u3002\\n            1. \u5982\u679c\u67d0\u4e2a\u6587\u4ef6\u662fjump-files\uff0c\u5c31\u4e0d\u5e94\u8be5\u51fa\u73b0\u5728\u8fd9\u4e2a\u5faa\u73af\u91cc\\n            2. \u5982\u679c\u68c0\u6d4b\u5230\u7684\u5f15\u7528\u4fe1\u606f\u6765\u6e90\u4e8e\u4e00\u4e2ajump-files, \u5ffd\u7565\u5b83\\n            3. \u5982\u679c\u68c0\u6d4b\u5230\u4e00\u4e2a\u5f15\u7528\u6765\u6e90\u4e8efake-file,\u5219\u8ba4\u4e3a\u4ed6\u7684\u6bcd\u6587\u4ef6\u662f\u539f\u6765\u7684\u6587\u4ef6\\n\"\n            assert not file_node.get_full_name().endswith(latest_verison_substring)\n            ref_count = 0\n            rel_file_path = file_node.get_full_name()\n            assert rel_file_path not in self.jump_files\n            if (\n                white_list_file_names != []\n                and file_node.get_file_name() not in white_list_file_names\n            ):\n                continue\n\n            def walk_file(now_obj: DocItem):\n                \"\"\"\u5728\u6587\u4ef6\u5185\u904d\u5386\u6240\u6709\u53d8\u91cf\"\"\"\n                nonlocal ref_count, white_list_file_names\n                in_file_only = False\n                if (\n                    white_list_obj_names != []\n                    and now_obj.obj_name not in white_list_obj_names\n                ):\n                    in_file_only = True\n                if SettingsManager().get_setting().project.parse_references:\n                    reference_list = find_all_referencer(\n                        repo_path=self.repo_path,\n                        variable_name=now_obj.obj_name,\n                        file_path=rel_file_path,\n                        line_number=now_obj.content[\"code_start_line\"],\n                        column_number=now_obj.content[\"name_column\"],\n                        in_file_only=in_file_only,\n                    )\n                else:\n                    reference_list = []\n                for referencer_pos in reference_list:\n                    referencer_file_ral_path = referencer_pos[0]\n                    if referencer_file_ral_path in self.fake_file_reflection.values():\n                        \"\u68c0\u6d4b\u5230\u7684\u5f15\u7528\u8005\u6765\u81ea\u4e8eunstaged files\uff0c\u8df3\u8fc7\u8be5\u5f15\u7528\"\n                        print(\n                            f\"{Fore.LIGHTBLUE_EX}[Reference From Unstaged Version, skip]{Style.RESET_ALL} {referencer_file_ral_path} -&gt; {now_obj.get_full_name()}\"\n                        )\n                        continue\n                    elif referencer_file_ral_path in self.jump_files:\n                        \"\u68c0\u6d4b\u5230\u7684\u5f15\u7528\u8005\u6765\u81ea\u4e8euntracked files\uff0c\u8df3\u8fc7\u8be5\u5f15\u7528\"\n                        print(\n                            f\"{Fore.LIGHTBLUE_EX}[Reference From Unstracked Version, skip]{Style.RESET_ALL} {referencer_file_ral_path} -&gt; {now_obj.get_full_name()}\"\n                        )\n                        continue\n                    target_file_hiera = referencer_file_ral_path.split(\"/\")\n                    referencer_file_item = self.target_repo_hierarchical_tree.find(\n                        target_file_hiera\n                    )\n                    if referencer_file_item == None:\n                        print(\n                            f'{Fore.LIGHTRED_EX}Error: Find \"{referencer_file_ral_path}\"(not in target repo){Style.RESET_ALL} referenced {now_obj.get_full_name()}'\n                        )\n                        continue\n                    referencer_node = self.find_obj_with_lineno(\n                        referencer_file_item, referencer_pos[1]\n                    )\n                    if referencer_node.obj_name == now_obj.obj_name:\n                        logger.info(\n                            f\"Jedi find {now_obj.get_full_name()} with name_duplicate_reference, skipped\"\n                        )\n                        continue\n                    if DocItem.has_ans_relation(now_obj, referencer_node) == None:\n                        if now_obj not in referencer_node.reference_who:\n                            special_reference_type = (\n                                referencer_node.item_type\n                                in [\n                                    DocItemType._function,\n                                    DocItemType._sub_function,\n                                    DocItemType._class_function,\n                                ]\n                                and referencer_node.code_start_line == referencer_pos[1]\n                            )\n                            referencer_node.special_reference_type.append(\n                                special_reference_type\n                            )\n                            referencer_node.reference_who.append(now_obj)\n                            now_obj.who_reference_me.append(referencer_node)\n                            ref_count += 1\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _, child in file_node.children.items():\n                walk_file(child)\n\n    def get_task_manager(self, now_node: DocItem, task_available_func) -&gt; TaskManager:\n        \"\"\"\n        Creates a task manager from documentation items, establishing dependencies based on references and the hierarchical structure of the code. Items can be filtered by a whitelist or a provided availability function. The process prioritizes tasks with fewer dependencies to resolve potential circular references.\n\n        Args:\n            target_repo_hierarchical_tree: The root node of the hierarchical tree representing the repository's structure.\n            task_available_func: A function that determines whether a given documentation item should be included as a task.  Can be None.\n\n        Returns:\n            TaskManager: A TaskManager object containing tasks generated from the documentation items,\n                with dependencies based on references and children within the tree.\n\n\n        \"\"\"\n\n        doc_items = now_node.get_travel_list()\n        if self.white_list != None:\n\n            def in_white_list(item: DocItem):\n                for cont in self.white_list:\n                    if (\n                        item.get_file_name() == cont[\"file_path\"]\n                        and item.obj_name == cont[\"id_text\"]\n                    ):\n                        return True\n                return False\n\n            doc_items = list(filter(in_white_list, doc_items))\n        doc_items = list(filter(task_available_func, doc_items))\n        doc_items = sorted(doc_items, key=lambda x: x.depth)\n        deal_items = []\n        task_manager = TaskManager()\n        bar = tqdm(total=len(doc_items), desc=\"parsing topology task-list\")\n        while doc_items:\n            min_break_level = 10000000.0\n            target_item = None\n            for item in doc_items:\n                \"\u4e00\u4e2a\u4efb\u52a1\u4f9d\u8d56\u4e8e\u6240\u6709\u5f15\u7528\u8005\u548c\u4ed6\u7684\u5b50\u8282\u70b9,\u6211\u4eec\u4e0d\u80fd\u4fdd\u8bc1\u5f15\u7528\u4e0d\u6210\u73af(\u4e5f\u8bb8\u6709\u4e9b\u4ed3\u5e93\u7684\u5e9f\u4ee3\u7801\u4f1a\u51fa\u73b0\u6210\u73af)\u3002\\n                \u8fd9\u65f6\u5c31\u53ea\u80fd\u9009\u62e9\u4e00\u4e2a\u76f8\u5bf9\u6765\u8bf4\u9075\u5b88\u7a0b\u5ea6\u6700\u597d\u7684\u4e86\\n                \u6709\u7279\u6b8a\u60c5\u51b5func-def\u4e2d\u7684param def\u53ef\u80fd\u4f1a\u51fa\u73b0\u5faa\u73af\u5f15\u7528\\n                \u53e6\u5916\u5faa\u73af\u5f15\u7528\u771f\u5b9e\u5b58\u5728\uff0c\u5bf9\u4e8e\u4e00\u4e9bbind\u7c7b\u7684\u63a5\u53e3\u771f\u7684\u4f1a\u53d1\u751f\uff0c\u6bd4\u5982\uff1a\\n                ChatDev/WareHouse/Gomoku_HumanAgentInteraction_20230920135038/main.py\u91cc\u9762\u7684: on-click\u3001show-winner\u3001restart\\n\"\n                best_break_level = 0\n                second_best_break_level = 0\n                for _, child in item.children.items():\n                    if task_available_func(child) and child not in deal_items:\n                        best_break_level += 1\n                for referenced, special in zip(\n                    item.reference_who, item.special_reference_type\n                ):\n                    if task_available_func(referenced) and referenced not in deal_items:\n                        best_break_level += 1\n                    if (\n                        task_available_func(referenced)\n                        and (not special)\n                        and (referenced not in deal_items)\n                    ):\n                        second_best_break_level += 1\n                if best_break_level == 0:\n                    min_break_level = -1\n                    target_item = item\n                    break\n                if second_best_break_level &lt; min_break_level:\n                    target_item = item\n                    min_break_level = second_best_break_level\n            if min_break_level &gt; 0:\n                print(\n                    f\"circle-reference(second-best still failed), level={min_break_level}: {target_item.get_full_name()}\"\n                )\n            item_denp_task_ids = []\n            for _, child in target_item.children.items():\n                if child.multithread_task_id != -1:\n                    item_denp_task_ids.append(child.multithread_task_id)\n            for referenced_item in target_item.reference_who:\n                if referenced_item.multithread_task_id in task_manager.task_dict.keys():\n                    item_denp_task_ids.append(referenced_item.multithread_task_id)\n            item_denp_task_ids = list(set(item_denp_task_ids))\n            if task_available_func == None or task_available_func(target_item):\n                task_id = task_manager.add_task(\n                    dependency_task_id=item_denp_task_ids, extra=target_item\n                )\n                target_item.multithread_task_id = task_id\n            deal_items.append(target_item)\n            doc_items.remove(target_item)\n            bar.update(1)\n        return task_manager\n\n    def get_topology(self, task_available_func) -&gt; TaskManager:\n        \"\"\"\n        Recursively traverses the repository's hierarchical tree, applying a provided function to each item encountered.\n\n\n        Args:\n            older_meta: The older MetaInfo object to merge from.\n\n        Returns:\n            Optional[DocItem]: The found DocItem or None if not found.\n\n\n        \"\"\"\n\n        self.parse_reference()\n        task_manager = self.get_task_manager(\n            self.target_repo_hierarchical_tree, task_available_func=task_available_func\n        )\n        return task_manager\n\n    def _map(self, deal_func: Callable):\n        \"\"\"\n        Recursively locates the root DocItem in the repository hierarchy.\n\n        Args:\n            now_item: The current item being processed.\n            root_item: The initial root item.\n\n        Returns:\n            The root item if found, otherwise None.\n\n        \"\"\"\n\n        def travel(now_item: DocItem):\n            deal_func(now_item)\n            for _, child in now_item.children.items():\n                travel(child)\n\n        travel(self.target_repo_hierarchical_tree)\n\n    def load_doc_from_older_meta(self, older_meta: MetaInfo):\n        \"\"\"\n        No valid docstring found.\n        \"\"\"\n        logger.info(\"merge doc from an older version of metainfo\")\n        root_item = self.target_repo_hierarchical_tree\n        deleted_items = []\n\n        def find_item(now_item: DocItem) -&gt; Optional[DocItem]:\n            nonlocal root_item\n            if now_item.father == None:\n                return root_item\n            father_find_result = find_item(now_item.father)\n            if not father_find_result:\n                return None\n            real_name = None\n            for child_real_name, temp_item in now_item.father.children.items():\n                if temp_item == now_item:\n                    real_name = child_real_name\n                    break\n            assert real_name != None\n            if real_name in father_find_result.children.keys():\n                result_item = father_find_result.children[real_name]\n                return result_item\n            return None\n\n        def travel(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item:\n                deleted_items.append(\n                    [now_older_item.get_full_name(), now_older_item.item_type.name]\n                )\n                return\n            result_item.md_content = now_older_item.md_content\n            result_item.item_status = now_older_item.item_status\n            if \"code_content\" in now_older_item.content.keys():\n                assert \"code_content\" in result_item.content.keys()\n                if remove_docstrings(\n                    now_older_item.content[\"code_content\"]\n                ) != remove_docstrings(result_item.content[\"code_content\"]):\n                    result_item.item_status = DocItemStatus.code_changed\n            for _, child in now_older_item.children.items():\n                travel(child)\n\n        travel(older_meta.target_repo_hierarchical_tree)\n        \"\u63a5\u4e0b\u6765\uff0cparse\u73b0\u5728\u7684\u53cc\u5411\u5f15\u7528\uff0c\u89c2\u5bdf\u8c01\u7684\u5f15\u7528\u8005\u6539\u4e86\"\n        self.parse_reference()\n\n        def travel2(now_older_item: DocItem):\n            result_item = find_item(now_older_item)\n            if not result_item:\n                return\n            \"result_item\u5f15\u7528\u7684\u4eba\u662f\u5426\u53d8\u5316\u4e86\"\n            new_reference_names = [\n                name.get_full_name(strict=True) for name in result_item.who_reference_me\n            ]\n            old_reference_names = now_older_item.who_reference_me_name_list\n            if (\n                not set(new_reference_names) == set(old_reference_names)\n                and result_item.item_status == DocItemStatus.doc_up_to_date\n            ):\n                if set(new_reference_names) &lt;= set(old_reference_names):\n                    result_item.item_status = DocItemStatus.referencer_not_exist\n                else:\n                    result_item.item_status = DocItemStatus.add_new_referencer\n            for _, child in now_older_item.children.items():\n                travel2(child)\n\n        travel2(older_meta.target_repo_hierarchical_tree)\n        self.deleted_items_from_older_meta = deleted_items\n\n    @staticmethod\n    def from_project_hierarchy_path(repo_path: str) -&gt; MetaInfo:\n        \"\"\"\n        Loads a project hierarchy from a JSON file and converts it into a MetaInfo object.\n\n        Args:\n            ce_relation: A boolean flag indicating whether to include cross-entity relations.\n\n        Returns:\n            dict: A dictionary representing the file hierarchy in JSON format.\n\n        \"\"\"\n\n        project_hierarchy_json_path = os.path.join(repo_path, \"project_hierarchy.json\")\n        logger.info(f\"parsing from {project_hierarchy_json_path}\")\n        if not os.path.exists(project_hierarchy_json_path):\n            raise NotImplementedError(\"Invalid operation detected\")\n        with open(project_hierarchy_json_path, \"r\", encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        return MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n\n    def to_hierarchy_json(self, flash_reference_relation=False):\n        \"\"\"\n        Transforms a project\u2019s documentation structure into a JSON representation, detailing relationships and content metadata for each item. It recursively processes files and directories to build a hierarchical view of the documented elements.\n\n        Args:\n            flash_reference_relation: A boolean indicating whether to use full names for references.\n\n        Returns:\n            dict: A dictionary representing the document hierarchy in JSON format.\n\n\n        \"\"\"\n\n        hierachy_json = {}\n        file_item_list = self.get_all_files()\n        for file_item in file_item_list:\n            file_hierarchy_content = []\n\n            def walk_file(now_obj: DocItem):\n                nonlocal file_hierarchy_content, flash_reference_relation\n                temp_json_obj = now_obj.content\n                if \"source_node\" in temp_json_obj:\n                    temp_json_obj.pop(\"source_node\")\n                temp_json_obj[\"name\"] = now_obj.obj_name\n                temp_json_obj[\"type\"] = now_obj.item_type.to_str()\n                temp_json_obj[\"md_content\"] = now_obj.md_content\n                temp_json_obj[\"item_status\"] = now_obj.item_status.name\n                if flash_reference_relation:\n                    temp_json_obj[\"who_reference_me\"] = [\n                        cont.get_full_name(strict=True)\n                        for cont in now_obj.who_reference_me\n                    ]\n                    temp_json_obj[\"reference_who\"] = [\n                        cont.get_full_name(strict=True)\n                        for cont in now_obj.reference_who\n                    ]\n                    temp_json_obj[\"special_reference_type\"] = (\n                        now_obj.special_reference_type\n                    )\n                else:\n                    temp_json_obj[\"who_reference_me\"] = (\n                        now_obj.who_reference_me_name_list\n                    )\n                    temp_json_obj[\"reference_who\"] = now_obj.reference_who_name_list\n                file_hierarchy_content.append(temp_json_obj)\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _, child in file_item.children.items():\n                walk_file(child)\n            if file_item.item_type == DocItemType._dir:\n                temp_json_obj = {}\n                temp_json_obj[\"name\"] = file_item.obj_name\n                temp_json_obj[\"type\"] = file_item.item_type.to_str()\n                temp_json_obj[\"md_content\"] = file_item.md_content\n                temp_json_obj[\"item_status\"] = file_item.item_status.name\n                hierachy_json[file_item.get_full_name()] = [temp_json_obj]\n            else:\n                hierachy_json[file_item.get_full_name()] = file_hierarchy_content\n        return hierachy_json\n\n    @staticmethod\n    def from_project_hierarchy_json(\n        project_hierarchy_json, repo_structure: Optional[Dict[str, Any]] = None\n    ) -&gt; MetaInfo:\n        \"\"\"\n        Constructs a hierarchical representation of the project by parsing file information and establishing relationships between documentation items, including handling potential naming conflicts and determining item types based on content.\n\n        This method processes file content and integrates it into a hierarchical tree,\n        handling cases where files are missing, empty, or have duplicate names. It also\n        determines item types (e.g., directory, file, class, function) based on content analysis\n        and establishes parent-child relationships between items in the tree.\n\n        Args:\n            file_name: The name of the file being processed.\n            file_content: A list containing information about the file's contents.\n            repo_structure:  The existing repository structure (optional).\n            target_meta_info: An object to store and update project metadata, including the hierarchical tree.\n\n        Returns:\n            target_meta_info: The updated target_meta_info object with the populated or modified hierarchical tree.\n\n        \"\"\"\n\n        setting = SettingsManager.get_setting()\n        target_meta_info = MetaInfo(\n            repo_structure=project_hierarchy_json,\n            target_repo_hierarchical_tree=DocItem(\n                item_type=DocItemType._repo, obj_name=\"full_repo\"\n            ),\n        )\n        for file_name, file_content in tqdm(\n            project_hierarchy_json.items(), desc=\"parsing parent relationship\"\n        ):\n            if not os.path.exists(os.path.join(setting.project.target_repo, file_name)):\n                logger.info(f\"deleted content: {file_name}\")\n                continue\n            elif (\n                os.path.getsize(os.path.join(setting.project.target_repo, file_name))\n                == 0\n                and file_content\n                and (file_content[0][\"type\"] != \"Dir\")\n            ):\n                logger.info(f\"blank content: {file_name}\")\n                continue\n            recursive_file_path = file_name.split(\"/\")\n            pos = 0\n            now_structure = target_meta_info.target_repo_hierarchical_tree\n            while pos &lt; len(recursive_file_path) - 1:\n                if recursive_file_path[pos] not in now_structure.children.keys():\n                    now_structure.children[recursive_file_path[pos]] = DocItem(\n                        item_type=DocItemType._dir,\n                        md_content=\"\",\n                        obj_name=recursive_file_path[pos],\n                    )\n                    now_structure.children[recursive_file_path[pos]].father = (\n                        now_structure\n                    )\n                now_structure = now_structure.children[recursive_file_path[pos]]\n                pos += 1\n            if recursive_file_path[-1] not in now_structure.children.keys():\n                if file_content and file_content[0].get(\"type\") == \"Dir\":\n                    doctype = DocItemType._dir\n                    now_structure.children[recursive_file_path[pos]] = DocItem(\n                        item_type=doctype, obj_name=recursive_file_path[-1]\n                    )\n                    now_structure.children[recursive_file_path[pos]].father = (\n                        now_structure\n                    )\n                else:\n                    doctype = DocItemType._file\n                    now_structure.children[recursive_file_path[pos]] = DocItem(\n                        item_type=doctype, obj_name=recursive_file_path[-1]\n                    )\n                    now_structure.children[recursive_file_path[pos]].father = (\n                        now_structure\n                    )\n            if repo_structure:\n                actual_item = repo_structure[file_name]\n            else:\n                actual_item = deepcopy(file_content)\n            assert type(file_content) == list\n            file_item = target_meta_info.target_repo_hierarchical_tree.find(\n                recursive_file_path\n            )\n            \"\u7528\u7c7b\u7ebf\u6bb5\u6811\u7684\u65b9\u5f0f\uff1a\\n            1.\u5148parse\u6240\u6709\u8282\u70b9\uff0c\u518d\u627e\u7236\u5b50\u5173\u7cfb\\n            2.\u4e00\u4e2a\u8282\u70b9\u7684\u7236\u8282\u70b9\uff0c\u6240\u6709\u5305\u542b\u4ed6\u7684code\u8303\u56f4\u7684\u8282\u70b9\u91cc\u7684\uff0c\u6700\u5c0f\u7684\u8282\u70b9\\n            \u590d\u6742\u5ea6\u662fO(n^2)\\n            3.\u6700\u540e\u6765\u5904\u7406\u8282\u70b9\u7684type\u95ee\u9898\\n            \"\n            obj_item_list: List[DocItem] = []\n            for value, actual in zip(file_content, actual_item):\n                if value.get(\"source_node\"):\n                    source_node = value.get(\"source_node\")\n                else:\n                    source_node = actual.get(\"source_node\")\n                obj_doc_item = DocItem(\n                    obj_name=value[\"name\"],\n                    content=value,\n                    md_content=value[\"md_content\"],\n                    code_start_line=value.get(\"code_start_line\"),\n                    code_end_line=value.get(\"code_end_line\"),\n                    source_node=source_node,\n                )\n                if \"item_status\" in value.keys():\n                    obj_doc_item.item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    obj_doc_item.reference_who_name_list = value[\"reference_who\"]\n                if \"special_reference_type\" in value.keys():\n                    obj_doc_item.special_reference_type = value[\n                        \"special_reference_type\"\n                    ]\n                if \"who_reference_me\" in value.keys():\n                    obj_doc_item.who_reference_me_name_list = value[\"who_reference_me\"]\n                obj_item_list.append(obj_doc_item)\n            for item in obj_item_list:\n                potential_father = None\n                for other_item in obj_item_list:\n\n                    def code_contain(item, other_item) -&gt; bool:\n                        if (\n                            other_item.code_end_line == item.code_end_line\n                            and other_item.code_start_line == item.code_start_line\n                        ):\n                            return False\n                        if (\n                            other_item.code_end_line &lt; item.code_end_line\n                            or other_item.code_start_line &gt; item.code_start_line\n                        ):\n                            return False\n                        return True\n\n                    if code_contain(item, other_item):\n                        if (\n                            potential_father == None\n                            or other_item.code_end_line - other_item.code_start_line\n                            &lt; potential_father.code_end_line\n                            - potential_father.code_start_line\n                        ):\n                            potential_father = other_item\n                if potential_father == None:\n                    potential_father = file_item\n                item.father = potential_father\n                child_name = item.obj_name\n                if child_name in potential_father.children.keys():\n                    now_name_id = 0\n                    while (\n                        child_name + f\"_{now_name_id}\"\n                        in potential_father.children.keys()\n                    ):\n                        now_name_id += 1\n                    child_name = child_name + f\"_{now_name_id}\"\n                    logger.warning(\n                        f\"Name duplicate in {file_item.get_full_name()}: rename to {item.obj_name}-&gt;{child_name}\"\n                    )\n                if potential_father.item_type != DocItemType._dir:\n                    potential_father.children[child_name] = item\n\n            def change_items(now_item: DocItem):\n                if now_item.item_type == DocItemType._dir:\n                    return target_meta_info\n                if now_item.item_type != DocItemType._file:\n                    if now_item.content[\"type\"] == \"ClassDef\":\n                        now_item.item_type = DocItemType._class\n                    elif now_item.content[\"type\"] == \"FunctionDef\":\n                        now_item.item_type = DocItemType._function\n                        if now_item.father.item_type == DocItemType._class:\n                            now_item.item_type = DocItemType._class_function\n                        elif now_item.father.item_type in [\n                            DocItemType._function,\n                            DocItemType._sub_function,\n                        ]:\n                            now_item.item_type = DocItemType._sub_function\n                for _, child in now_item.children.items():\n                    change_items(child)\n\n            change_items(file_item)\n        target_meta_info.target_repo_hierarchical_tree.parse_tree_path(now_path=[])\n        target_meta_info.target_repo_hierarchical_tree.check_depth()\n        return target_meta_info\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.MetaInfo.checkpoint","title":"<code>checkpoint(target_dir_path, flash_reference_relation=False)</code>","text":"<p>Persists the project\u2019s metadata and hierarchy to a specified directory, ensuring data is saved for later use or recovery. Includes options to control the level of detail in the saved hierarchy representation.</p> <p>Parameters:</p> Name Type Description Default <code>target_dir_path</code> <code>str | Path</code> <p>The path to the directory where the checkpoint should be saved.</p> required <code>flash_reference_relation</code> <p>A boolean indicating whether to include flash reference relations in the hierarchy JSON.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def checkpoint(self, target_dir_path: str | Path, flash_reference_relation=False):\n    \"\"\"\n    Persists the project\u2019s metadata and hierarchy to a specified directory, ensuring data is saved for later use or recovery. Includes options to control the level of detail in the saved hierarchy representation.\n\n    Args:\n        target_dir_path: The path to the directory where the checkpoint should be saved.\n        flash_reference_relation: A boolean indicating whether to include flash reference relations in the hierarchy JSON.\n\n    Returns:\n        None\n\n\n    \"\"\"\n\n    with self.checkpoint_lock:\n        target_dir = Path(target_dir_path)\n        logger.debug(f\"Checkpointing MetaInfo to directory: {target_dir}\")\n        print(f\"{Fore.GREEN}MetaInfo is Refreshed and Saved{Style.RESET_ALL}\")\n        if not target_dir.exists():\n            target_dir.mkdir(parents=True, exist_ok=True)\n            logger.debug(f\"Created directory: {target_dir}\")\n        now_hierarchy_json = self.to_hierarchy_json(\n            flash_reference_relation=flash_reference_relation\n        )\n        hierarchy_file = target_dir / \"project_hierarchy.json\"\n        try:\n            with hierarchy_file.open(\"w\", encoding=\"utf-8\") as writer:\n                json.dump(now_hierarchy_json, writer, indent=2, ensure_ascii=False)\n            logger.debug(f\"Saved hierarchy JSON to {hierarchy_file}\")\n        except IOError as e:\n            logger.error(f\"Failed to save hierarchy JSON to {hierarchy_file}: {e}\")\n        meta_info_file = target_dir / \"meta-info.json\"\n        meta = {\n            \"main_idea\": SettingsManager().get_setting().project.main_idea,\n            \"doc_version\": self.document_version,\n            \"in_generation_process\": self.in_generation_process,\n            \"fake_file_reflection\": self.fake_file_reflection,\n            \"jump_files\": self.jump_files,\n            \"deleted_items_from_older_meta\": self.deleted_items_from_older_meta,\n        }\n        try:\n            with meta_info_file.open(\"w\", encoding=\"utf-8\") as writer:\n                json.dump(meta, writer, indent=2, ensure_ascii=False)\n            logger.debug(f\"Saved meta-info JSON to {meta_info_file}\")\n        except IOError as e:\n            logger.error(f\"Failed to save meta-info JSON to {meta_info_file}: {e}\")\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.MetaInfo.find_obj_with_lineno","title":"<code>find_obj_with_lineno(file_node, start_line_num)</code>","text":"<p>No valid docstring found.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def find_obj_with_lineno(self, file_node: DocItem, start_line_num) -&gt; DocItem:\n    \"\"\"\n    No valid docstring found.\n\n    \"\"\"\n\n    now_node = file_node\n    assert now_node != None\n    while len(now_node.children) &gt; 0:\n        find_qualify_child = False\n        for _, child in now_node.children.items():\n            assert child.content != None\n            if (\n                child.content[\"code_start_line\"] &lt;= start_line_num\n                and child.content[\"code_end_line\"] &gt;= start_line_num\n            ):\n                now_node = child\n                find_qualify_child = True\n                break\n        if not find_qualify_child:\n            return now_node\n    return now_node\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.MetaInfo.from_checkpoint_path","title":"<code>from_checkpoint_path(checkpoint_dir_path, repo_structure=None)</code>  <code>staticmethod</code>","text":"<p>Loads project metadata from a checkpoint directory to restore a previous state.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir_path</code> <code>Path</code> <p>The path to the checkpoint directory.</p> required <code>t_dir_path</code> <p>The path to the temporary directory.</p> required <code>repo_structure</code> <code>Optional[Dict[str, Any]]</code> <p>An optional dictionary representing the repository structure.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>MetaInfo</code> <code>MetaInfo</code> <p>A MetaInfo object loaded from the checkpoint data.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>@staticmethod\ndef from_checkpoint_path(\n    checkpoint_dir_path: Path, repo_structure: Optional[Dict[str, Any]] = None\n) -&gt; MetaInfo:\n    \"\"\"\n    Loads project metadata from a checkpoint directory to restore a previous state.\n\n    Args:\n        checkpoint_dir_path: The path to the checkpoint directory.\n        t_dir_path:  The path to the temporary directory.\n        repo_structure: An optional dictionary representing the repository structure.\n\n    Returns:\n        MetaInfo: A MetaInfo object loaded from the checkpoint data.\n\n\n    \"\"\"\n\n    setting = SettingsManager.get_setting()\n    project_hierarchy_json_path = checkpoint_dir_path / \"project_hierarchy.json\"\n    with open(project_hierarchy_json_path, \"r\", encoding=\"utf-8\") as reader:\n        project_hierarchy_json = json.load(reader)\n    metainfo = MetaInfo.from_project_hierarchy_json(\n        project_hierarchy_json, repo_structure\n    )\n    with open(\n        checkpoint_dir_path / \"meta-info.json\", \"r\", encoding=\"utf-8\"\n    ) as reader:\n        meta_data = json.load(reader)\n        metainfo.repo_path = setting.project.target_repo\n        metainfo.main_idea = meta_data[\"main_idea\"]\n        metainfo.document_version = meta_data[\"doc_version\"]\n        metainfo.fake_file_reflection = meta_data[\"fake_file_reflection\"]\n        metainfo.jump_files = meta_data[\"jump_files\"]\n        metainfo.in_generation_process = meta_data[\"in_generation_process\"]\n        metainfo.deleted_items_from_older_meta = meta_data[\n            \"deleted_items_from_older_meta\"\n        ]\n    print(f\"{Fore.CYAN}Loading MetaInfo:{Style.RESET_ALL} {checkpoint_dir_path}\")\n    return metainfo\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.MetaInfo.from_project_hierarchy_json","title":"<code>from_project_hierarchy_json(project_hierarchy_json, repo_structure=None)</code>  <code>staticmethod</code>","text":"<p>Constructs a hierarchical representation of the project by parsing file information and establishing relationships between documentation items, including handling potential naming conflicts and determining item types based on content.</p> <p>This method processes file content and integrates it into a hierarchical tree, handling cases where files are missing, empty, or have duplicate names. It also determines item types (e.g., directory, file, class, function) based on content analysis and establishes parent-child relationships between items in the tree.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <p>The name of the file being processed.</p> required <code>file_content</code> <p>A list containing information about the file's contents.</p> required <code>repo_structure</code> <code>Optional[Dict[str, Any]]</code> <p>The existing repository structure (optional).</p> <code>None</code> <code>target_meta_info</code> <p>An object to store and update project metadata, including the hierarchical tree.</p> required <p>Returns:</p> Name Type Description <code>target_meta_info</code> <code>MetaInfo</code> <p>The updated target_meta_info object with the populated or modified hierarchical tree.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>@staticmethod\ndef from_project_hierarchy_json(\n    project_hierarchy_json, repo_structure: Optional[Dict[str, Any]] = None\n) -&gt; MetaInfo:\n    \"\"\"\n    Constructs a hierarchical representation of the project by parsing file information and establishing relationships between documentation items, including handling potential naming conflicts and determining item types based on content.\n\n    This method processes file content and integrates it into a hierarchical tree,\n    handling cases where files are missing, empty, or have duplicate names. It also\n    determines item types (e.g., directory, file, class, function) based on content analysis\n    and establishes parent-child relationships between items in the tree.\n\n    Args:\n        file_name: The name of the file being processed.\n        file_content: A list containing information about the file's contents.\n        repo_structure:  The existing repository structure (optional).\n        target_meta_info: An object to store and update project metadata, including the hierarchical tree.\n\n    Returns:\n        target_meta_info: The updated target_meta_info object with the populated or modified hierarchical tree.\n\n    \"\"\"\n\n    setting = SettingsManager.get_setting()\n    target_meta_info = MetaInfo(\n        repo_structure=project_hierarchy_json,\n        target_repo_hierarchical_tree=DocItem(\n            item_type=DocItemType._repo, obj_name=\"full_repo\"\n        ),\n    )\n    for file_name, file_content in tqdm(\n        project_hierarchy_json.items(), desc=\"parsing parent relationship\"\n    ):\n        if not os.path.exists(os.path.join(setting.project.target_repo, file_name)):\n            logger.info(f\"deleted content: {file_name}\")\n            continue\n        elif (\n            os.path.getsize(os.path.join(setting.project.target_repo, file_name))\n            == 0\n            and file_content\n            and (file_content[0][\"type\"] != \"Dir\")\n        ):\n            logger.info(f\"blank content: {file_name}\")\n            continue\n        recursive_file_path = file_name.split(\"/\")\n        pos = 0\n        now_structure = target_meta_info.target_repo_hierarchical_tree\n        while pos &lt; len(recursive_file_path) - 1:\n            if recursive_file_path[pos] not in now_structure.children.keys():\n                now_structure.children[recursive_file_path[pos]] = DocItem(\n                    item_type=DocItemType._dir,\n                    md_content=\"\",\n                    obj_name=recursive_file_path[pos],\n                )\n                now_structure.children[recursive_file_path[pos]].father = (\n                    now_structure\n                )\n            now_structure = now_structure.children[recursive_file_path[pos]]\n            pos += 1\n        if recursive_file_path[-1] not in now_structure.children.keys():\n            if file_content and file_content[0].get(\"type\") == \"Dir\":\n                doctype = DocItemType._dir\n                now_structure.children[recursive_file_path[pos]] = DocItem(\n                    item_type=doctype, obj_name=recursive_file_path[-1]\n                )\n                now_structure.children[recursive_file_path[pos]].father = (\n                    now_structure\n                )\n            else:\n                doctype = DocItemType._file\n                now_structure.children[recursive_file_path[pos]] = DocItem(\n                    item_type=doctype, obj_name=recursive_file_path[-1]\n                )\n                now_structure.children[recursive_file_path[pos]].father = (\n                    now_structure\n                )\n        if repo_structure:\n            actual_item = repo_structure[file_name]\n        else:\n            actual_item = deepcopy(file_content)\n        assert type(file_content) == list\n        file_item = target_meta_info.target_repo_hierarchical_tree.find(\n            recursive_file_path\n        )\n        \"\u7528\u7c7b\u7ebf\u6bb5\u6811\u7684\u65b9\u5f0f\uff1a\\n            1.\u5148parse\u6240\u6709\u8282\u70b9\uff0c\u518d\u627e\u7236\u5b50\u5173\u7cfb\\n            2.\u4e00\u4e2a\u8282\u70b9\u7684\u7236\u8282\u70b9\uff0c\u6240\u6709\u5305\u542b\u4ed6\u7684code\u8303\u56f4\u7684\u8282\u70b9\u91cc\u7684\uff0c\u6700\u5c0f\u7684\u8282\u70b9\\n            \u590d\u6742\u5ea6\u662fO(n^2)\\n            3.\u6700\u540e\u6765\u5904\u7406\u8282\u70b9\u7684type\u95ee\u9898\\n            \"\n        obj_item_list: List[DocItem] = []\n        for value, actual in zip(file_content, actual_item):\n            if value.get(\"source_node\"):\n                source_node = value.get(\"source_node\")\n            else:\n                source_node = actual.get(\"source_node\")\n            obj_doc_item = DocItem(\n                obj_name=value[\"name\"],\n                content=value,\n                md_content=value[\"md_content\"],\n                code_start_line=value.get(\"code_start_line\"),\n                code_end_line=value.get(\"code_end_line\"),\n                source_node=source_node,\n            )\n            if \"item_status\" in value.keys():\n                obj_doc_item.item_status = DocItemStatus[value[\"item_status\"]]\n            if \"reference_who\" in value.keys():\n                obj_doc_item.reference_who_name_list = value[\"reference_who\"]\n            if \"special_reference_type\" in value.keys():\n                obj_doc_item.special_reference_type = value[\n                    \"special_reference_type\"\n                ]\n            if \"who_reference_me\" in value.keys():\n                obj_doc_item.who_reference_me_name_list = value[\"who_reference_me\"]\n            obj_item_list.append(obj_doc_item)\n        for item in obj_item_list:\n            potential_father = None\n            for other_item in obj_item_list:\n\n                def code_contain(item, other_item) -&gt; bool:\n                    if (\n                        other_item.code_end_line == item.code_end_line\n                        and other_item.code_start_line == item.code_start_line\n                    ):\n                        return False\n                    if (\n                        other_item.code_end_line &lt; item.code_end_line\n                        or other_item.code_start_line &gt; item.code_start_line\n                    ):\n                        return False\n                    return True\n\n                if code_contain(item, other_item):\n                    if (\n                        potential_father == None\n                        or other_item.code_end_line - other_item.code_start_line\n                        &lt; potential_father.code_end_line\n                        - potential_father.code_start_line\n                    ):\n                        potential_father = other_item\n            if potential_father == None:\n                potential_father = file_item\n            item.father = potential_father\n            child_name = item.obj_name\n            if child_name in potential_father.children.keys():\n                now_name_id = 0\n                while (\n                    child_name + f\"_{now_name_id}\"\n                    in potential_father.children.keys()\n                ):\n                    now_name_id += 1\n                child_name = child_name + f\"_{now_name_id}\"\n                logger.warning(\n                    f\"Name duplicate in {file_item.get_full_name()}: rename to {item.obj_name}-&gt;{child_name}\"\n                )\n            if potential_father.item_type != DocItemType._dir:\n                potential_father.children[child_name] = item\n\n        def change_items(now_item: DocItem):\n            if now_item.item_type == DocItemType._dir:\n                return target_meta_info\n            if now_item.item_type != DocItemType._file:\n                if now_item.content[\"type\"] == \"ClassDef\":\n                    now_item.item_type = DocItemType._class\n                elif now_item.content[\"type\"] == \"FunctionDef\":\n                    now_item.item_type = DocItemType._function\n                    if now_item.father.item_type == DocItemType._class:\n                        now_item.item_type = DocItemType._class_function\n                    elif now_item.father.item_type in [\n                        DocItemType._function,\n                        DocItemType._sub_function,\n                    ]:\n                        now_item.item_type = DocItemType._sub_function\n            for _, child in now_item.children.items():\n                change_items(child)\n\n        change_items(file_item)\n    target_meta_info.target_repo_hierarchical_tree.parse_tree_path(now_path=[])\n    target_meta_info.target_repo_hierarchical_tree.check_depth()\n    return target_meta_info\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.MetaInfo.from_project_hierarchy_path","title":"<code>from_project_hierarchy_path(repo_path)</code>  <code>staticmethod</code>","text":"<p>Loads a project hierarchy from a JSON file and converts it into a MetaInfo object.</p> <p>Parameters:</p> Name Type Description Default <code>ce_relation</code> <p>A boolean flag indicating whether to include cross-entity relations.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>MetaInfo</code> <p>A dictionary representing the file hierarchy in JSON format.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>@staticmethod\ndef from_project_hierarchy_path(repo_path: str) -&gt; MetaInfo:\n    \"\"\"\n    Loads a project hierarchy from a JSON file and converts it into a MetaInfo object.\n\n    Args:\n        ce_relation: A boolean flag indicating whether to include cross-entity relations.\n\n    Returns:\n        dict: A dictionary representing the file hierarchy in JSON format.\n\n    \"\"\"\n\n    project_hierarchy_json_path = os.path.join(repo_path, \"project_hierarchy.json\")\n    logger.info(f\"parsing from {project_hierarchy_json_path}\")\n    if not os.path.exists(project_hierarchy_json_path):\n        raise NotImplementedError(\"Invalid operation detected\")\n    with open(project_hierarchy_json_path, \"r\", encoding=\"utf-8\") as reader:\n        project_hierarchy_json = json.load(reader)\n    return MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.MetaInfo.get_all_files","title":"<code>get_all_files(count_repo=False)</code>","text":"<p>Returns a list of all items \u2013 files and directories \u2013 within the project's structure. Optionally includes the root repository item in the results.</p> <p>Parameters:</p> Name Type Description Default <code>count_repo</code> <p>Whether to include the root repo item in the results.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[DocItem]</code> <p>A list of DocItem objects representing all files and directories.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def get_all_files(self, count_repo=False) -&gt; List[DocItem]:\n    \"\"\"\n    Returns a list of all items \u2013 files and directories \u2013 within the project's structure. Optionally includes the root repository item in the results.\n\n    Args:\n        count_repo: Whether to include the root repo item in the results.\n\n    Returns:\n        A list of DocItem objects representing all files and directories.\n\n\n    \"\"\"\n\n    files = []\n\n    def walk_tree(now_node):\n        if now_node.item_type in [DocItemType._file, DocItemType._dir]:\n            files.append(now_node)\n        if count_repo and now_node.item_type == DocItemType._repo:\n            files.append(now_node)\n        for _, child in now_node.children.items():\n            walk_tree(child)\n\n    walk_tree(self.target_repo_hierarchical_tree)\n    return files\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.MetaInfo.get_task_manager","title":"<code>get_task_manager(now_node, task_available_func)</code>","text":"<p>Creates a task manager from documentation items, establishing dependencies based on references and the hierarchical structure of the code. Items can be filtered by a whitelist or a provided availability function. The process prioritizes tasks with fewer dependencies to resolve potential circular references.</p> <p>Parameters:</p> Name Type Description Default <code>target_repo_hierarchical_tree</code> <p>The root node of the hierarchical tree representing the repository's structure.</p> required <code>task_available_func</code> <p>A function that determines whether a given documentation item should be included as a task.  Can be None.</p> required <p>Returns:</p> Name Type Description <code>TaskManager</code> <code>TaskManager</code> <p>A TaskManager object containing tasks generated from the documentation items, with dependencies based on references and children within the tree.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def get_task_manager(self, now_node: DocItem, task_available_func) -&gt; TaskManager:\n    \"\"\"\n    Creates a task manager from documentation items, establishing dependencies based on references and the hierarchical structure of the code. Items can be filtered by a whitelist or a provided availability function. The process prioritizes tasks with fewer dependencies to resolve potential circular references.\n\n    Args:\n        target_repo_hierarchical_tree: The root node of the hierarchical tree representing the repository's structure.\n        task_available_func: A function that determines whether a given documentation item should be included as a task.  Can be None.\n\n    Returns:\n        TaskManager: A TaskManager object containing tasks generated from the documentation items,\n            with dependencies based on references and children within the tree.\n\n\n    \"\"\"\n\n    doc_items = now_node.get_travel_list()\n    if self.white_list != None:\n\n        def in_white_list(item: DocItem):\n            for cont in self.white_list:\n                if (\n                    item.get_file_name() == cont[\"file_path\"]\n                    and item.obj_name == cont[\"id_text\"]\n                ):\n                    return True\n            return False\n\n        doc_items = list(filter(in_white_list, doc_items))\n    doc_items = list(filter(task_available_func, doc_items))\n    doc_items = sorted(doc_items, key=lambda x: x.depth)\n    deal_items = []\n    task_manager = TaskManager()\n    bar = tqdm(total=len(doc_items), desc=\"parsing topology task-list\")\n    while doc_items:\n        min_break_level = 10000000.0\n        target_item = None\n        for item in doc_items:\n            \"\u4e00\u4e2a\u4efb\u52a1\u4f9d\u8d56\u4e8e\u6240\u6709\u5f15\u7528\u8005\u548c\u4ed6\u7684\u5b50\u8282\u70b9,\u6211\u4eec\u4e0d\u80fd\u4fdd\u8bc1\u5f15\u7528\u4e0d\u6210\u73af(\u4e5f\u8bb8\u6709\u4e9b\u4ed3\u5e93\u7684\u5e9f\u4ee3\u7801\u4f1a\u51fa\u73b0\u6210\u73af)\u3002\\n                \u8fd9\u65f6\u5c31\u53ea\u80fd\u9009\u62e9\u4e00\u4e2a\u76f8\u5bf9\u6765\u8bf4\u9075\u5b88\u7a0b\u5ea6\u6700\u597d\u7684\u4e86\\n                \u6709\u7279\u6b8a\u60c5\u51b5func-def\u4e2d\u7684param def\u53ef\u80fd\u4f1a\u51fa\u73b0\u5faa\u73af\u5f15\u7528\\n                \u53e6\u5916\u5faa\u73af\u5f15\u7528\u771f\u5b9e\u5b58\u5728\uff0c\u5bf9\u4e8e\u4e00\u4e9bbind\u7c7b\u7684\u63a5\u53e3\u771f\u7684\u4f1a\u53d1\u751f\uff0c\u6bd4\u5982\uff1a\\n                ChatDev/WareHouse/Gomoku_HumanAgentInteraction_20230920135038/main.py\u91cc\u9762\u7684: on-click\u3001show-winner\u3001restart\\n\"\n            best_break_level = 0\n            second_best_break_level = 0\n            for _, child in item.children.items():\n                if task_available_func(child) and child not in deal_items:\n                    best_break_level += 1\n            for referenced, special in zip(\n                item.reference_who, item.special_reference_type\n            ):\n                if task_available_func(referenced) and referenced not in deal_items:\n                    best_break_level += 1\n                if (\n                    task_available_func(referenced)\n                    and (not special)\n                    and (referenced not in deal_items)\n                ):\n                    second_best_break_level += 1\n            if best_break_level == 0:\n                min_break_level = -1\n                target_item = item\n                break\n            if second_best_break_level &lt; min_break_level:\n                target_item = item\n                min_break_level = second_best_break_level\n        if min_break_level &gt; 0:\n            print(\n                f\"circle-reference(second-best still failed), level={min_break_level}: {target_item.get_full_name()}\"\n            )\n        item_denp_task_ids = []\n        for _, child in target_item.children.items():\n            if child.multithread_task_id != -1:\n                item_denp_task_ids.append(child.multithread_task_id)\n        for referenced_item in target_item.reference_who:\n            if referenced_item.multithread_task_id in task_manager.task_dict.keys():\n                item_denp_task_ids.append(referenced_item.multithread_task_id)\n        item_denp_task_ids = list(set(item_denp_task_ids))\n        if task_available_func == None or task_available_func(target_item):\n            task_id = task_manager.add_task(\n                dependency_task_id=item_denp_task_ids, extra=target_item\n            )\n            target_item.multithread_task_id = task_id\n        deal_items.append(target_item)\n        doc_items.remove(target_item)\n        bar.update(1)\n    return task_manager\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.MetaInfo.get_topology","title":"<code>get_topology(task_available_func)</code>","text":"<p>Recursively traverses the repository's hierarchical tree, applying a provided function to each item encountered.</p> <p>Parameters:</p> Name Type Description Default <code>older_meta</code> <p>The older MetaInfo object to merge from.</p> required <p>Returns:</p> Type Description <code>TaskManager</code> <p>Optional[DocItem]: The found DocItem or None if not found.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def get_topology(self, task_available_func) -&gt; TaskManager:\n    \"\"\"\n    Recursively traverses the repository's hierarchical tree, applying a provided function to each item encountered.\n\n\n    Args:\n        older_meta: The older MetaInfo object to merge from.\n\n    Returns:\n        Optional[DocItem]: The found DocItem or None if not found.\n\n\n    \"\"\"\n\n    self.parse_reference()\n    task_manager = self.get_task_manager(\n        self.target_repo_hierarchical_tree, task_available_func=task_available_func\n    )\n    return task_manager\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.MetaInfo.init_meta_info","title":"<code>init_meta_info(file_path_reflections, jump_files)</code>  <code>staticmethod</code>","text":"<p>Creates a MetaInfo object representing the project\u2019s structure by analyzing files and directories.</p> <p>Parameters:</p> Name Type Description Default <code>file_path_reflections</code> <p>The file path reflections to use.</p> required <code>jump_files</code> <p>The jump files to use.</p> required <p>Returns:</p> Name Type Description <code>MetaInfo</code> <code>MetaInfo</code> <p>A MetaInfo object representing the project's metadata.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>@staticmethod\ndef init_meta_info(file_path_reflections, jump_files) -&gt; MetaInfo:\n    \"\"\"\n    Creates a MetaInfo object representing the project\u2019s structure by analyzing files and directories.\n\n    Args:\n        file_path_reflections: The file path reflections to use.\n        jump_files: The jump files to use.\n\n    Returns:\n        MetaInfo: A MetaInfo object representing the project's metadata.\n\n\n    \"\"\"\n\n    setting = SettingsManager.get_setting()\n    project_abs_path = setting.project.target_repo\n    print(\n        f\"{Fore.LIGHTRED_EX}Initializing MetaInfo: {Style.RESET_ALL}from {project_abs_path}\"\n    )\n    file_handler = FileHandler(project_abs_path, None)\n    repo_structure = file_handler.generate_overall_structure(\n        file_path_reflections, jump_files\n    )\n    metainfo = MetaInfo.from_project_hierarchy_json(repo_structure)\n    metainfo.repo_path = project_abs_path\n    metainfo.fake_file_reflection = file_path_reflections\n    metainfo.jump_files = jump_files\n    return metainfo\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.MetaInfo.load_doc_from_older_meta","title":"<code>load_doc_from_older_meta(older_meta)</code>","text":"<p>No valid docstring found.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def load_doc_from_older_meta(self, older_meta: MetaInfo):\n    \"\"\"\n    No valid docstring found.\n    \"\"\"\n    logger.info(\"merge doc from an older version of metainfo\")\n    root_item = self.target_repo_hierarchical_tree\n    deleted_items = []\n\n    def find_item(now_item: DocItem) -&gt; Optional[DocItem]:\n        nonlocal root_item\n        if now_item.father == None:\n            return root_item\n        father_find_result = find_item(now_item.father)\n        if not father_find_result:\n            return None\n        real_name = None\n        for child_real_name, temp_item in now_item.father.children.items():\n            if temp_item == now_item:\n                real_name = child_real_name\n                break\n        assert real_name != None\n        if real_name in father_find_result.children.keys():\n            result_item = father_find_result.children[real_name]\n            return result_item\n        return None\n\n    def travel(now_older_item: DocItem):\n        result_item = find_item(now_older_item)\n        if not result_item:\n            deleted_items.append(\n                [now_older_item.get_full_name(), now_older_item.item_type.name]\n            )\n            return\n        result_item.md_content = now_older_item.md_content\n        result_item.item_status = now_older_item.item_status\n        if \"code_content\" in now_older_item.content.keys():\n            assert \"code_content\" in result_item.content.keys()\n            if remove_docstrings(\n                now_older_item.content[\"code_content\"]\n            ) != remove_docstrings(result_item.content[\"code_content\"]):\n                result_item.item_status = DocItemStatus.code_changed\n        for _, child in now_older_item.children.items():\n            travel(child)\n\n    travel(older_meta.target_repo_hierarchical_tree)\n    \"\u63a5\u4e0b\u6765\uff0cparse\u73b0\u5728\u7684\u53cc\u5411\u5f15\u7528\uff0c\u89c2\u5bdf\u8c01\u7684\u5f15\u7528\u8005\u6539\u4e86\"\n    self.parse_reference()\n\n    def travel2(now_older_item: DocItem):\n        result_item = find_item(now_older_item)\n        if not result_item:\n            return\n        \"result_item\u5f15\u7528\u7684\u4eba\u662f\u5426\u53d8\u5316\u4e86\"\n        new_reference_names = [\n            name.get_full_name(strict=True) for name in result_item.who_reference_me\n        ]\n        old_reference_names = now_older_item.who_reference_me_name_list\n        if (\n            not set(new_reference_names) == set(old_reference_names)\n            and result_item.item_status == DocItemStatus.doc_up_to_date\n        ):\n            if set(new_reference_names) &lt;= set(old_reference_names):\n                result_item.item_status = DocItemStatus.referencer_not_exist\n            else:\n                result_item.item_status = DocItemStatus.add_new_referencer\n        for _, child in now_older_item.children.items():\n            travel2(child)\n\n    travel2(older_meta.target_repo_hierarchical_tree)\n    self.deleted_items_from_older_meta = deleted_items\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.MetaInfo.parse_reference","title":"<code>parse_reference()</code>","text":"<p>No valid docstring found.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def parse_reference(self):\n    \"\"\"\n    No valid docstring found.\n\n    \"\"\"\n\n    file_nodes = self.get_all_files()\n    white_list_file_names, white_list_obj_names = ([], [])\n    if self.white_list != None:\n        white_list_file_names = [cont[\"file_path\"] for cont in self.white_list]\n        white_list_obj_names = [cont[\"id_text\"] for cont in self.white_list]\n    for file_node in tqdm(file_nodes, desc=\"parsing bidirectional reference\"):\n        \"\u68c0\u6d4b\u4e00\u4e2a\u6587\u4ef6\u5185\u7684\u6240\u6709\u5f15\u7528\u4fe1\u606f\uff0c\u53ea\u80fd\u68c0\u6d4b\u5f15\u7528\u8be5\u6587\u4ef6\u5185\u67d0\u4e2aobj\u7684\u5176\u4ed6\u5185\u5bb9\u3002\\n            1. \u5982\u679c\u67d0\u4e2a\u6587\u4ef6\u662fjump-files\uff0c\u5c31\u4e0d\u5e94\u8be5\u51fa\u73b0\u5728\u8fd9\u4e2a\u5faa\u73af\u91cc\\n            2. \u5982\u679c\u68c0\u6d4b\u5230\u7684\u5f15\u7528\u4fe1\u606f\u6765\u6e90\u4e8e\u4e00\u4e2ajump-files, \u5ffd\u7565\u5b83\\n            3. \u5982\u679c\u68c0\u6d4b\u5230\u4e00\u4e2a\u5f15\u7528\u6765\u6e90\u4e8efake-file,\u5219\u8ba4\u4e3a\u4ed6\u7684\u6bcd\u6587\u4ef6\u662f\u539f\u6765\u7684\u6587\u4ef6\\n\"\n        assert not file_node.get_full_name().endswith(latest_verison_substring)\n        ref_count = 0\n        rel_file_path = file_node.get_full_name()\n        assert rel_file_path not in self.jump_files\n        if (\n            white_list_file_names != []\n            and file_node.get_file_name() not in white_list_file_names\n        ):\n            continue\n\n        def walk_file(now_obj: DocItem):\n            \"\"\"\u5728\u6587\u4ef6\u5185\u904d\u5386\u6240\u6709\u53d8\u91cf\"\"\"\n            nonlocal ref_count, white_list_file_names\n            in_file_only = False\n            if (\n                white_list_obj_names != []\n                and now_obj.obj_name not in white_list_obj_names\n            ):\n                in_file_only = True\n            if SettingsManager().get_setting().project.parse_references:\n                reference_list = find_all_referencer(\n                    repo_path=self.repo_path,\n                    variable_name=now_obj.obj_name,\n                    file_path=rel_file_path,\n                    line_number=now_obj.content[\"code_start_line\"],\n                    column_number=now_obj.content[\"name_column\"],\n                    in_file_only=in_file_only,\n                )\n            else:\n                reference_list = []\n            for referencer_pos in reference_list:\n                referencer_file_ral_path = referencer_pos[0]\n                if referencer_file_ral_path in self.fake_file_reflection.values():\n                    \"\u68c0\u6d4b\u5230\u7684\u5f15\u7528\u8005\u6765\u81ea\u4e8eunstaged files\uff0c\u8df3\u8fc7\u8be5\u5f15\u7528\"\n                    print(\n                        f\"{Fore.LIGHTBLUE_EX}[Reference From Unstaged Version, skip]{Style.RESET_ALL} {referencer_file_ral_path} -&gt; {now_obj.get_full_name()}\"\n                    )\n                    continue\n                elif referencer_file_ral_path in self.jump_files:\n                    \"\u68c0\u6d4b\u5230\u7684\u5f15\u7528\u8005\u6765\u81ea\u4e8euntracked files\uff0c\u8df3\u8fc7\u8be5\u5f15\u7528\"\n                    print(\n                        f\"{Fore.LIGHTBLUE_EX}[Reference From Unstracked Version, skip]{Style.RESET_ALL} {referencer_file_ral_path} -&gt; {now_obj.get_full_name()}\"\n                    )\n                    continue\n                target_file_hiera = referencer_file_ral_path.split(\"/\")\n                referencer_file_item = self.target_repo_hierarchical_tree.find(\n                    target_file_hiera\n                )\n                if referencer_file_item == None:\n                    print(\n                        f'{Fore.LIGHTRED_EX}Error: Find \"{referencer_file_ral_path}\"(not in target repo){Style.RESET_ALL} referenced {now_obj.get_full_name()}'\n                    )\n                    continue\n                referencer_node = self.find_obj_with_lineno(\n                    referencer_file_item, referencer_pos[1]\n                )\n                if referencer_node.obj_name == now_obj.obj_name:\n                    logger.info(\n                        f\"Jedi find {now_obj.get_full_name()} with name_duplicate_reference, skipped\"\n                    )\n                    continue\n                if DocItem.has_ans_relation(now_obj, referencer_node) == None:\n                    if now_obj not in referencer_node.reference_who:\n                        special_reference_type = (\n                            referencer_node.item_type\n                            in [\n                                DocItemType._function,\n                                DocItemType._sub_function,\n                                DocItemType._class_function,\n                            ]\n                            and referencer_node.code_start_line == referencer_pos[1]\n                        )\n                        referencer_node.special_reference_type.append(\n                            special_reference_type\n                        )\n                        referencer_node.reference_who.append(now_obj)\n                        now_obj.who_reference_me.append(referencer_node)\n                        ref_count += 1\n            for _, child in now_obj.children.items():\n                walk_file(child)\n\n        for _, child in file_node.children.items():\n            walk_file(child)\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.MetaInfo.print_task_list","title":"<code>print_task_list(task_dict)</code>","text":"<p>Displays a table summarizing task details, including ID, reason for documentation generation, file path, and dependencies. Dependency lists are truncated for brevity if they exceed a certain length.</p> <p>Parameters:</p> Name Type Description Default <code>task_dict</code> <code>Dict[Task]</code> <p>A dictionary where keys are task IDs and values are Task objects.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def print_task_list(self, task_dict: Dict[Task]):\n    \"\"\"\n    Displays a table summarizing task details, including ID, reason for documentation generation, file path, and dependencies. Dependency lists are truncated for brevity if they exceed a certain length.\n\n    Args:\n        task_dict: A dictionary where keys are task IDs and values are Task objects.\n\n    Returns:\n        None\n\n\n    \"\"\"\n\n    task_table = PrettyTable(\n        [\"task_id\", \"Doc Generation Reason\", \"Path\", \"dependency\"]\n    )\n    for task_id, task_info in task_dict.items():\n        remain_str = \"None\"\n        if task_info.dependencies != []:\n            remain_str = \",\".join(\n                [str(d_task.task_id) for d_task in task_info.dependencies]\n            )\n            if len(remain_str) &gt; 20:\n                remain_str = remain_str[:8] + \"...\" + remain_str[-8:]\n        task_table.add_row(\n            [\n                task_id,\n                task_info.extra_info.item_status.name,\n                task_info.extra_info.get_full_name(strict=True),\n                remain_str,\n            ]\n        )\n    print(task_table)\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.MetaInfo.to_hierarchy_json","title":"<code>to_hierarchy_json(flash_reference_relation=False)</code>","text":"<p>Transforms a project\u2019s documentation structure into a JSON representation, detailing relationships and content metadata for each item. It recursively processes files and directories to build a hierarchical view of the documented elements.</p> <p>Parameters:</p> Name Type Description Default <code>flash_reference_relation</code> <p>A boolean indicating whether to use full names for references.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary representing the document hierarchy in JSON format.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def to_hierarchy_json(self, flash_reference_relation=False):\n    \"\"\"\n    Transforms a project\u2019s documentation structure into a JSON representation, detailing relationships and content metadata for each item. It recursively processes files and directories to build a hierarchical view of the documented elements.\n\n    Args:\n        flash_reference_relation: A boolean indicating whether to use full names for references.\n\n    Returns:\n        dict: A dictionary representing the document hierarchy in JSON format.\n\n\n    \"\"\"\n\n    hierachy_json = {}\n    file_item_list = self.get_all_files()\n    for file_item in file_item_list:\n        file_hierarchy_content = []\n\n        def walk_file(now_obj: DocItem):\n            nonlocal file_hierarchy_content, flash_reference_relation\n            temp_json_obj = now_obj.content\n            if \"source_node\" in temp_json_obj:\n                temp_json_obj.pop(\"source_node\")\n            temp_json_obj[\"name\"] = now_obj.obj_name\n            temp_json_obj[\"type\"] = now_obj.item_type.to_str()\n            temp_json_obj[\"md_content\"] = now_obj.md_content\n            temp_json_obj[\"item_status\"] = now_obj.item_status.name\n            if flash_reference_relation:\n                temp_json_obj[\"who_reference_me\"] = [\n                    cont.get_full_name(strict=True)\n                    for cont in now_obj.who_reference_me\n                ]\n                temp_json_obj[\"reference_who\"] = [\n                    cont.get_full_name(strict=True)\n                    for cont in now_obj.reference_who\n                ]\n                temp_json_obj[\"special_reference_type\"] = (\n                    now_obj.special_reference_type\n                )\n            else:\n                temp_json_obj[\"who_reference_me\"] = (\n                    now_obj.who_reference_me_name_list\n                )\n                temp_json_obj[\"reference_who\"] = now_obj.reference_who_name_list\n            file_hierarchy_content.append(temp_json_obj)\n            for _, child in now_obj.children.items():\n                walk_file(child)\n\n        for _, child in file_item.children.items():\n            walk_file(child)\n        if file_item.item_type == DocItemType._dir:\n            temp_json_obj = {}\n            temp_json_obj[\"name\"] = file_item.obj_name\n            temp_json_obj[\"type\"] = file_item.item_type.to_str()\n            temp_json_obj[\"md_content\"] = file_item.md_content\n            temp_json_obj[\"item_status\"] = file_item.item_status.name\n            hierachy_json[file_item.get_full_name()] = [temp_json_obj]\n        else:\n            hierachy_json[file_item.get_full_name()] = file_hierarchy_content\n    return hierachy_json\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.find_all_referencer","title":"<code>find_all_referencer(repo_path, variable_name, file_path, line_number, column_number, in_file_only=False)</code>","text":"<p>Locates all uses of a variable throughout the codebase.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <p>The name of the variable to find references for.</p> required <code>file_path</code> <p>The path to the file where the variable is defined.</p> required <code>line_number</code> <p>The line number where the variable is defined.</p> required <code>column_number</code> <p>The column number where the variable is defined.</p> required <code>in_file_only</code> <p>If True, limits the search to within the defining file.</p> <code>False</code> <p>Returns:</p> Name Type Description <p>A list of tuples, each containing the relative path to a referencing file, and the line and column numbers of the reference.  The original definition location is excluded. Returns an empty list if any error occurs during processing.</p> <code>Args</code> <p>variable_name: The name of the variable to search for. file_path: The path to the file containing the variable. line_number: The line number where the variable is defined. column_number: The column number where the variable is defined. in_file_only: If True, only searches for references within the same file.</p> <code>Returns</code> <p>list[tuple[str, int, int]]: A list of tuples containing the relative path to the referencing file,                              the line number, and the column number of each reference, excluding                              the original definition location.  Returns an empty list if an error occurs.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def find_all_referencer(\n    repo_path, variable_name, file_path, line_number, column_number, in_file_only=False\n):\n    \"\"\"\n    Locates all uses of a variable throughout the codebase.\n\n    Args:\n        variable_name: The name of the variable to find references for.\n        file_path: The path to the file where the variable is defined.\n        line_number: The line number where the variable is defined.\n        column_number: The column number where the variable is defined.\n        in_file_only: If True, limits the search to within the defining file.\n\n    Returns:\n        A list of tuples, each containing the relative path to a referencing file, and the line and column numbers of the reference.  The original definition location is excluded. Returns an empty list if any error occurs during processing.\n\n\n        Args:\n            variable_name: The name of the variable to search for.\n            file_path: The path to the file containing the variable.\n            line_number: The line number where the variable is defined.\n            column_number: The column number where the variable is defined.\n            in_file_only: If True, only searches for references within the same file.\n\n        Returns:\n            list[tuple[str, int, int]]: A list of tuples containing the relative path to the referencing file,\n                                         the line number, and the column number of each reference, excluding\n                                         the original definition location.  Returns an empty list if an error occurs.\n\n\n    \"\"\"\n\n    script = jedi.Script(path=os.path.join(repo_path, file_path))\n    try:\n        if in_file_only:\n            references = script.get_references(\n                line=line_number, column=column_number, scope=\"file\"\n            )\n        else:\n            references = script.get_references(line=line_number, column=column_number)\n        variable_references = [ref for ref in references if ref.name == variable_name]\n        return [\n            (os.path.relpath(ref.module_path, repo_path), ref.line, ref.column)\n            for ref in variable_references\n            if not (ref.line == line_number and ref.column == column_number)\n        ]\n    except Exception as e:\n        logger.error(f\"Error occurred: {e}\")\n        logger.error(\n            f\"Parameters: variable_name={variable_name}, file_path={file_path}, line_number={line_number}, column_number={column_number}\"\n        )\n        return []\n</code></pre>"},{"location":"repo_agent/doc_meta_info/#repo_agent.doc_meta_info.need_to_generate","title":"<code>need_to_generate(doc_item, ignore_list=[])</code>","text":"<p>No valid docstring found.</p> Source code in <code>repo_agent/doc_meta_info.py</code> <pre><code>def need_to_generate(doc_item: DocItem, ignore_list: List[str] = []) -&gt; bool:\n    \"\"\"\n    No valid docstring found.\n\n    \"\"\"\n\n    if doc_item.item_status == DocItemStatus.doc_up_to_date:\n        return False\n    rel_file_path = doc_item.get_full_name()\n    doc_item = doc_item.father\n    while doc_item:\n        if doc_item.item_type == DocItemType._file:\n            if any(\n                (rel_file_path.startswith(ignore_item) for ignore_item in ignore_list)\n            ):\n                return False\n            else:\n                return True\n        doc_item = doc_item.father\n    return False\n</code></pre>"},{"location":"repo_agent/file_handler/","title":"File Handler","text":""},{"location":"repo_agent/file_handler/#repo_agent.file_handler.FileHandler","title":"<code>FileHandler</code>","text":"<p>FileHandler class for managing and analyzing files within a repository.</p> <p>This class provides methods for reading, writing, and extracting information from files in a given repository, including code structure analysis using AST parsing. It also handles versioning through Git integration.</p> Source code in <code>repo_agent/file_handler.py</code> <pre><code>class FileHandler:\n    \"\"\"\n    FileHandler class for managing and analyzing files within a repository.\n\n    This class provides methods for reading, writing, and extracting information from\n    files in a given repository, including code structure analysis using AST parsing.\n    It also handles versioning through Git integration.\n    \"\"\"\n\n    def __init__(self, repo_path, file_path):\n        \"\"\"\n        Stores the repository and file paths, and determines the project hierarchy based on configured settings.\n\n        Args:\n            repo_path: The path to the repository.\n            file_path: The path to the file.\n\n        Returns:\n            None\n\n        \"\"\"\n\n        self.file_path = file_path\n        self.repo_path = repo_path\n        setting = SettingsManager.get_setting()\n        self.project_hierarchy = (\n            setting.project.target_repo / setting.project.hierarchy_name\n        )\n\n    def read_file(self):\n        \"\"\"\n        No valid docstring found.\n\n        \"\"\"\n\n        abs_file_path = os.path.join(self.repo_path, self.file_path)\n        with open(abs_file_path, \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n        return content\n\n    def get_obj_code_info(\n        self,\n        code_type,\n        code_name,\n        start_line,\n        end_line,\n        params,\n        file_path=None,\n        docstring=\"\",\n        source_node=None,\n    ):\n        \"\"\"\n        Collects detailed information about a code element, including its type, name, location, parameters, documentation, and content. It determines if the code includes a return statement and identifies the starting column of the element's name within its source file.\n\n        Args:\n            code_type: The type of the code object (e.g., 'function', 'class').\n            code_name: The name of the code object.\n            start_line: The starting line number of the code object in its file.\n            end_line: The ending line number of the code object in its file.\n            params: A list of parameters associated with the code object.\n            file_path: The path to the file containing the code object. Defaults to self.file_path if None.\n            docstring: The docstring of the code object. Defaults to ''.\n            source_node: The AST node representing the code object. Defaults to None.\n\n        Returns:\n            dict: A dictionary containing information about the code object, including its type, name,\n                  code content, line numbers, parameters, docstring, whether it has a return statement,\n                  and the column number where the name appears on the starting line.\n\n\n        \"\"\"\n\n        code_info = {}\n        code_info[\"type\"] = code_type\n        code_info[\"name\"] = code_name\n        code_info[\"md_content\"] = []\n        code_info[\"code_start_line\"] = start_line\n        code_info[\"code_end_line\"] = end_line\n        code_info[\"params\"] = params\n        code_info[\"docstring\"] = docstring\n        code_info[\"source_node\"] = source_node\n        with open(\n            os.path.join(\n                self.repo_path, file_path if file_path != None else self.file_path\n            ),\n            \"r\",\n            encoding=\"utf-8\",\n        ) as code_file:\n            lines = code_file.readlines()\n            code_content = \"\".join(lines[start_line - 1 : end_line])\n            name_column = lines[start_line - 1].find(code_name)\n            if \"return\" in code_content:\n                have_return = True\n            else:\n                have_return = False\n            code_info[\"have_return\"] = have_return\n            code_info[\"code_content\"] = code_content\n            code_info[\"name_column\"] = name_column\n        return code_info\n\n    def write_file(self, file_path, content):\n        \"\"\"\n        Creates or overwrites a file at the specified path within the repository with the given content. Paths are relative to the repository root and any leading slashes are removed. Ensures necessary directories exist before writing.\n\n        Args:\n            file_path: The path to the file, relative to the repository root.\n                       Leading slashes are removed.\n            content: The string content to write to the file.\n\n        Returns:\n            None\n\n        \"\"\"\n\n        if file_path.startswith(\"/\"):\n            file_path = file_path[1:]\n        abs_file_path = os.path.join(self.repo_path, file_path)\n        os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n        with open(abs_file_path, \"w\", encoding=\"utf-8\") as file:\n            file.write(content)\n\n    def get_modified_file_versions(self):\n        \"\"\"\n        Obtains the content of the latest and prior revisions of a file within a Git repository. Returns `None` for the prior revision if no previous commits exist.\n\n        Args:\n            self: The instance containing repo_path and file_path attributes.\n\n        Returns:\n            tuple: A tuple containing the current version and previous version of the file as strings.\n                   If the file has no prior commits, the previous version will be None.\n\n\n        \"\"\"\n\n        repo = git.Repo(self.repo_path)\n        current_version_path = os.path.join(self.repo_path, self.file_path)\n        with open(current_version_path, \"r\", encoding=\"utf-8\") as file:\n            current_version = file.read()\n        commits = list(repo.iter_commits(paths=self.file_path, max_count=1))\n        previous_version = None\n        if commits:\n            commit = commits[0]\n            try:\n                previous_version = (\n                    (commit.tree / self.file_path).data_stream.read().decode(\"utf-8\")\n                )\n            except KeyError:\n                previous_version = None\n        return (current_version, previous_version)\n\n    def get_end_lineno(self, node):\n        \"\"\"\n        Determines the final line number spanned by a node and its descendants within the abstract syntax tree.\n\n        Args:\n            node: The AST node to get the end line number for.\n\n        Returns:\n            int: The end line number of the node, or -1 if it doesn't have one.\n\n        \"\"\"\n\n        if not hasattr(node, \"lineno\"):\n            return -1\n        end_lineno = node.lineno\n        for child in ast.iter_child_nodes(node):\n            child_end = getattr(child, \"end_lineno\", None) or self.get_end_lineno(child)\n            if child_end &gt; -1:\n                end_lineno = max(end_lineno, child_end)\n        return end_lineno\n\n    def add_parent_references(self, node, parent=None):\n        \"\"\"\n        Recursively sets the `parent` attribute of each child node to the current node, enabling upward traversal of the AST.\n\n        This method recursively iterates through the children of an AST node and\n        sets their `parent` attribute to the current node. This is useful for\n        traversing the AST in a bottom-up manner or accessing parent nodes from\n        child nodes.\n\n        Args:\n            node: The AST node to start adding parent references from.\n            parent: The parent of the current node.\n\n        Returns:\n            None\n\n        \"\"\"\n\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n            self.add_parent_references(child, node)\n\n    def get_functions_and_classes(self, code_content):\n        \"\"\"\n        Identifies and extracts function and class definitions from source code, providing details such as name, location, parameters, and documentation.\n\n        Args:\n            code_content: The source code to analyze.\n\n        Returns:\n            list: A list of tuples, where each tuple contains information about a\n                function or class: (type name, name, start line number, end line\n                number, parameter names, docstring, AST node).\n\n\n        \"\"\"\n\n        tree = ast.parse(code_content)\n        self.add_parent_references(tree)\n        functions_and_classes = []\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                start_line = node.lineno\n                end_line = self.get_end_lineno(node)\n                parameters = (\n                    [arg.arg for arg in node.args.args] if \"args\" in dir(node) else []\n                )\n                all_names = [item[1] for item in functions_and_classes]\n                functions_and_classes.append(\n                    (\n                        type(node).__name__,\n                        node.name,\n                        start_line,\n                        end_line,\n                        parameters,\n                        ast.get_docstring(node),\n                        node,\n                    )\n                )\n        return functions_and_classes\n\n    def generate_file_structure(self, file_path):\n        \"\"\"\n        Creates a structured representation of a file's contents. Returns directory information if the path points to a directory, or details about functions and classes within the file otherwise. Each element in the returned structure provides metadata like type, name, location, and documentation.\n\n        Args:\n            file_path: The path to the file within the repository.\n\n        Returns:\n            list: A list of dictionaries, where each dictionary represents either a directory or a code object (function/class)\n                  within the specified file.  If the path is a directory, returns a single dictionary representing that directory.\n                  Otherwise, it returns a list of dictionaries containing information about functions and classes found in the file.\n\n\n        \"\"\"\n\n        if os.path.isdir(os.path.join(self.repo_path, file_path)):\n            return [\n                {\n                    \"type\": \"Dir\",\n                    \"name\": file_path,\n                    \"content\": \"\",\n                    \"md_content\": [],\n                    \"code_start_line\": -1,\n                    \"code_end_line\": -1,\n                }\n            ]\n        else:\n            with open(\n                os.path.join(self.repo_path, file_path), \"r\", encoding=\"utf-8\"\n            ) as f:\n                content = f.read()\n                structures = self.get_functions_and_classes(content)\n                file_objects = []\n                for struct in structures:\n                    (\n                        structure_type,\n                        name,\n                        start_line,\n                        end_line,\n                        params,\n                        docstring,\n                        source_node,\n                    ) = struct\n                    code_info = self.get_obj_code_info(\n                        structure_type,\n                        name,\n                        start_line,\n                        end_line,\n                        params,\n                        file_path,\n                        docstring,\n                        source_node,\n                    )\n                    file_objects.append(code_info)\n        return file_objects\n\n    def generate_overall_structure(self, file_path_reflections, jump_files) -&gt; dict:\n        \"\"\"\n        Analyzes files and folders within the repository, respecting `.gitignore` rules and specified exclusions, to construct a structured representation of the project's contents. The structure maps each file to its detailed content analysis generated by `generate_file_structure`. Files are skipped if they are present in a list of jump files or match a latest version substring. Errors during individual file processing are logged but do not halt overall structure generation.\n\n        Args:\n            file_path_reflections: A reference to file paths.\n            jump_files: A list of files to ignore during structure generation.\n\n        Returns:\n            dict: A dictionary representing the repository structure, where keys are file names\n                  and values are their corresponding structures generated by `generate_file_structure`.\n                  Returns an empty dictionary if any error occurs during processing.\n\n\n        \"\"\"\n\n        repo_structure = {}\n        gitignore_checker = GitignoreChecker(\n            directory=self.repo_path,\n            gitignore_path=os.path.join(self.repo_path, \".gitignore\"),\n        )\n        bar = tqdm(gitignore_checker.check_files_and_folders())\n        for not_ignored_files in bar:\n            normal_file_names = not_ignored_files\n            if not_ignored_files in jump_files:\n                print(\n                    f\"{Fore.LIGHTYELLOW_EX}[File-Handler] Unstaged AddFile, ignore this file: {Style.RESET_ALL}{normal_file_names}\"\n                )\n                continue\n            elif not_ignored_files.endswith(latest_verison_substring):\n                print(\n                    f\"{Fore.LIGHTYELLOW_EX}[File-Handler] Skip Latest Version, Using Git-Status Version]: {Style.RESET_ALL}{normal_file_names}\"\n                )\n                continue\n            try:\n                repo_structure[normal_file_names] = self.generate_file_structure(\n                    not_ignored_files\n                )\n            except Exception as e:\n                logger.error(\n                    f\"Alert: An error occurred while generating file structure for {not_ignored_files}: {e}\"\n                )\n                continue\n            bar.set_description(f\"generating repo structure: {not_ignored_files}\")\n        return repo_structure\n\n    def convert_to_markdown_file(self, file_path=None):\n        \"\"\"\n        Generates a markdown representation of a file's content and structure based on the project hierarchy data.\n\n        Args:\n            file_path: The path to the file within the project hierarchy to convert.\n                If None, uses the self.file_path attribute.\n\n        Returns:\n            str: A markdown formatted string representing the specified file's content and structure.\n\n\n        \"\"\"\n\n        with open(self.project_hierarchy, \"r\", encoding=\"utf-8\") as f:\n            json_data = json.load(f)\n        if file_path is None:\n            file_path = self.file_path\n        file_dict = json_data.get(file_path)\n        if file_dict is None:\n            raise ValueError(\n                f\"No file object found for {self.file_path} in project_hierarchy.json\"\n            )\n        markdown = \"\"\n        parent_dict = {}\n        objects = sorted(file_dict.values(), key=lambda obj: obj[\"code_start_line\"])\n        for obj in objects:\n            if obj[\"parent\"] is not None:\n                parent_dict[obj[\"name\"]] = obj[\"parent\"]\n        current_parent = None\n        for obj in objects:\n            level = 1\n            parent = obj[\"parent\"]\n            while parent is not None:\n                level += 1\n                parent = parent_dict.get(parent)\n            if level == 1 and current_parent is not None:\n                markdown += \"***\\n\"\n            current_parent = obj[\"name\"]\n            params_str = \"\"\n            if obj[\"type\"] in [\"FunctionDef\", \"AsyncFunctionDef\"]:\n                params_str = \"()\"\n                if obj[\"params\"]:\n                    params_str = f\"({', '.join(obj['params'])})\"\n            markdown += f\"{'#' * level} {obj['type']} {obj['name']}{params_str}:\\n\"\n            markdown += (\n                f\"{(obj['md_content'][-1] if len(obj['md_content']) &gt; 0 else '')}\\n\"\n            )\n        markdown += \"***\\n\"\n        return markdown\n</code></pre>"},{"location":"repo_agent/file_handler/#repo_agent.file_handler.FileHandler.__init__","title":"<code>__init__(repo_path, file_path)</code>","text":"<p>Stores the repository and file paths, and determines the project hierarchy based on configured settings.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <p>The path to the repository.</p> required <code>file_path</code> <p>The path to the file.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/file_handler.py</code> <pre><code>def __init__(self, repo_path, file_path):\n    \"\"\"\n    Stores the repository and file paths, and determines the project hierarchy based on configured settings.\n\n    Args:\n        repo_path: The path to the repository.\n        file_path: The path to the file.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    self.file_path = file_path\n    self.repo_path = repo_path\n    setting = SettingsManager.get_setting()\n    self.project_hierarchy = (\n        setting.project.target_repo / setting.project.hierarchy_name\n    )\n</code></pre>"},{"location":"repo_agent/file_handler/#repo_agent.file_handler.FileHandler.add_parent_references","title":"<code>add_parent_references(node, parent=None)</code>","text":"<p>Recursively sets the <code>parent</code> attribute of each child node to the current node, enabling upward traversal of the AST.</p> <p>This method recursively iterates through the children of an AST node and sets their <code>parent</code> attribute to the current node. This is useful for traversing the AST in a bottom-up manner or accessing parent nodes from child nodes.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <p>The AST node to start adding parent references from.</p> required <code>parent</code> <p>The parent of the current node.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/file_handler.py</code> <pre><code>def add_parent_references(self, node, parent=None):\n    \"\"\"\n    Recursively sets the `parent` attribute of each child node to the current node, enabling upward traversal of the AST.\n\n    This method recursively iterates through the children of an AST node and\n    sets their `parent` attribute to the current node. This is useful for\n    traversing the AST in a bottom-up manner or accessing parent nodes from\n    child nodes.\n\n    Args:\n        node: The AST node to start adding parent references from.\n        parent: The parent of the current node.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    for child in ast.iter_child_nodes(node):\n        child.parent = node\n        self.add_parent_references(child, node)\n</code></pre>"},{"location":"repo_agent/file_handler/#repo_agent.file_handler.FileHandler.convert_to_markdown_file","title":"<code>convert_to_markdown_file(file_path=None)</code>","text":"<p>Generates a markdown representation of a file's content and structure based on the project hierarchy data.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <p>The path to the file within the project hierarchy to convert. If None, uses the self.file_path attribute.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <p>A markdown formatted string representing the specified file's content and structure.</p> Source code in <code>repo_agent/file_handler.py</code> <pre><code>def convert_to_markdown_file(self, file_path=None):\n    \"\"\"\n    Generates a markdown representation of a file's content and structure based on the project hierarchy data.\n\n    Args:\n        file_path: The path to the file within the project hierarchy to convert.\n            If None, uses the self.file_path attribute.\n\n    Returns:\n        str: A markdown formatted string representing the specified file's content and structure.\n\n\n    \"\"\"\n\n    with open(self.project_hierarchy, \"r\", encoding=\"utf-8\") as f:\n        json_data = json.load(f)\n    if file_path is None:\n        file_path = self.file_path\n    file_dict = json_data.get(file_path)\n    if file_dict is None:\n        raise ValueError(\n            f\"No file object found for {self.file_path} in project_hierarchy.json\"\n        )\n    markdown = \"\"\n    parent_dict = {}\n    objects = sorted(file_dict.values(), key=lambda obj: obj[\"code_start_line\"])\n    for obj in objects:\n        if obj[\"parent\"] is not None:\n            parent_dict[obj[\"name\"]] = obj[\"parent\"]\n    current_parent = None\n    for obj in objects:\n        level = 1\n        parent = obj[\"parent\"]\n        while parent is not None:\n            level += 1\n            parent = parent_dict.get(parent)\n        if level == 1 and current_parent is not None:\n            markdown += \"***\\n\"\n        current_parent = obj[\"name\"]\n        params_str = \"\"\n        if obj[\"type\"] in [\"FunctionDef\", \"AsyncFunctionDef\"]:\n            params_str = \"()\"\n            if obj[\"params\"]:\n                params_str = f\"({', '.join(obj['params'])})\"\n        markdown += f\"{'#' * level} {obj['type']} {obj['name']}{params_str}:\\n\"\n        markdown += (\n            f\"{(obj['md_content'][-1] if len(obj['md_content']) &gt; 0 else '')}\\n\"\n        )\n    markdown += \"***\\n\"\n    return markdown\n</code></pre>"},{"location":"repo_agent/file_handler/#repo_agent.file_handler.FileHandler.generate_file_structure","title":"<code>generate_file_structure(file_path)</code>","text":"<p>Creates a structured representation of a file's contents. Returns directory information if the path points to a directory, or details about functions and classes within the file otherwise. Each element in the returned structure provides metadata like type, name, location, and documentation.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <p>The path to the file within the repository.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of dictionaries, where each dictionary represents either a directory or a code object (function/class)   within the specified file.  If the path is a directory, returns a single dictionary representing that directory.   Otherwise, it returns a list of dictionaries containing information about functions and classes found in the file.</p> Source code in <code>repo_agent/file_handler.py</code> <pre><code>def generate_file_structure(self, file_path):\n    \"\"\"\n    Creates a structured representation of a file's contents. Returns directory information if the path points to a directory, or details about functions and classes within the file otherwise. Each element in the returned structure provides metadata like type, name, location, and documentation.\n\n    Args:\n        file_path: The path to the file within the repository.\n\n    Returns:\n        list: A list of dictionaries, where each dictionary represents either a directory or a code object (function/class)\n              within the specified file.  If the path is a directory, returns a single dictionary representing that directory.\n              Otherwise, it returns a list of dictionaries containing information about functions and classes found in the file.\n\n\n    \"\"\"\n\n    if os.path.isdir(os.path.join(self.repo_path, file_path)):\n        return [\n            {\n                \"type\": \"Dir\",\n                \"name\": file_path,\n                \"content\": \"\",\n                \"md_content\": [],\n                \"code_start_line\": -1,\n                \"code_end_line\": -1,\n            }\n        ]\n    else:\n        with open(\n            os.path.join(self.repo_path, file_path), \"r\", encoding=\"utf-8\"\n        ) as f:\n            content = f.read()\n            structures = self.get_functions_and_classes(content)\n            file_objects = []\n            for struct in structures:\n                (\n                    structure_type,\n                    name,\n                    start_line,\n                    end_line,\n                    params,\n                    docstring,\n                    source_node,\n                ) = struct\n                code_info = self.get_obj_code_info(\n                    structure_type,\n                    name,\n                    start_line,\n                    end_line,\n                    params,\n                    file_path,\n                    docstring,\n                    source_node,\n                )\n                file_objects.append(code_info)\n    return file_objects\n</code></pre>"},{"location":"repo_agent/file_handler/#repo_agent.file_handler.FileHandler.generate_overall_structure","title":"<code>generate_overall_structure(file_path_reflections, jump_files)</code>","text":"<p>Analyzes files and folders within the repository, respecting <code>.gitignore</code> rules and specified exclusions, to construct a structured representation of the project's contents. The structure maps each file to its detailed content analysis generated by <code>generate_file_structure</code>. Files are skipped if they are present in a list of jump files or match a latest version substring. Errors during individual file processing are logged but do not halt overall structure generation.</p> <p>Parameters:</p> Name Type Description Default <code>file_path_reflections</code> <p>A reference to file paths.</p> required <code>jump_files</code> <p>A list of files to ignore during structure generation.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representing the repository structure, where keys are file names   and values are their corresponding structures generated by <code>generate_file_structure</code>.   Returns an empty dictionary if any error occurs during processing.</p> Source code in <code>repo_agent/file_handler.py</code> <pre><code>def generate_overall_structure(self, file_path_reflections, jump_files) -&gt; dict:\n    \"\"\"\n    Analyzes files and folders within the repository, respecting `.gitignore` rules and specified exclusions, to construct a structured representation of the project's contents. The structure maps each file to its detailed content analysis generated by `generate_file_structure`. Files are skipped if they are present in a list of jump files or match a latest version substring. Errors during individual file processing are logged but do not halt overall structure generation.\n\n    Args:\n        file_path_reflections: A reference to file paths.\n        jump_files: A list of files to ignore during structure generation.\n\n    Returns:\n        dict: A dictionary representing the repository structure, where keys are file names\n              and values are their corresponding structures generated by `generate_file_structure`.\n              Returns an empty dictionary if any error occurs during processing.\n\n\n    \"\"\"\n\n    repo_structure = {}\n    gitignore_checker = GitignoreChecker(\n        directory=self.repo_path,\n        gitignore_path=os.path.join(self.repo_path, \".gitignore\"),\n    )\n    bar = tqdm(gitignore_checker.check_files_and_folders())\n    for not_ignored_files in bar:\n        normal_file_names = not_ignored_files\n        if not_ignored_files in jump_files:\n            print(\n                f\"{Fore.LIGHTYELLOW_EX}[File-Handler] Unstaged AddFile, ignore this file: {Style.RESET_ALL}{normal_file_names}\"\n            )\n            continue\n        elif not_ignored_files.endswith(latest_verison_substring):\n            print(\n                f\"{Fore.LIGHTYELLOW_EX}[File-Handler] Skip Latest Version, Using Git-Status Version]: {Style.RESET_ALL}{normal_file_names}\"\n            )\n            continue\n        try:\n            repo_structure[normal_file_names] = self.generate_file_structure(\n                not_ignored_files\n            )\n        except Exception as e:\n            logger.error(\n                f\"Alert: An error occurred while generating file structure for {not_ignored_files}: {e}\"\n            )\n            continue\n        bar.set_description(f\"generating repo structure: {not_ignored_files}\")\n    return repo_structure\n</code></pre>"},{"location":"repo_agent/file_handler/#repo_agent.file_handler.FileHandler.get_end_lineno","title":"<code>get_end_lineno(node)</code>","text":"<p>Determines the final line number spanned by a node and its descendants within the abstract syntax tree.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <p>The AST node to get the end line number for.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The end line number of the node, or -1 if it doesn't have one.</p> Source code in <code>repo_agent/file_handler.py</code> <pre><code>def get_end_lineno(self, node):\n    \"\"\"\n    Determines the final line number spanned by a node and its descendants within the abstract syntax tree.\n\n    Args:\n        node: The AST node to get the end line number for.\n\n    Returns:\n        int: The end line number of the node, or -1 if it doesn't have one.\n\n    \"\"\"\n\n    if not hasattr(node, \"lineno\"):\n        return -1\n    end_lineno = node.lineno\n    for child in ast.iter_child_nodes(node):\n        child_end = getattr(child, \"end_lineno\", None) or self.get_end_lineno(child)\n        if child_end &gt; -1:\n            end_lineno = max(end_lineno, child_end)\n    return end_lineno\n</code></pre>"},{"location":"repo_agent/file_handler/#repo_agent.file_handler.FileHandler.get_functions_and_classes","title":"<code>get_functions_and_classes(code_content)</code>","text":"<p>Identifies and extracts function and class definitions from source code, providing details such as name, location, parameters, and documentation.</p> <p>Parameters:</p> Name Type Description Default <code>code_content</code> <p>The source code to analyze.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of tuples, where each tuple contains information about a function or class: (type name, name, start line number, end line number, parameter names, docstring, AST node).</p> Source code in <code>repo_agent/file_handler.py</code> <pre><code>def get_functions_and_classes(self, code_content):\n    \"\"\"\n    Identifies and extracts function and class definitions from source code, providing details such as name, location, parameters, and documentation.\n\n    Args:\n        code_content: The source code to analyze.\n\n    Returns:\n        list: A list of tuples, where each tuple contains information about a\n            function or class: (type name, name, start line number, end line\n            number, parameter names, docstring, AST node).\n\n\n    \"\"\"\n\n    tree = ast.parse(code_content)\n    self.add_parent_references(tree)\n    functions_and_classes = []\n    for node in ast.walk(tree):\n        if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n            start_line = node.lineno\n            end_line = self.get_end_lineno(node)\n            parameters = (\n                [arg.arg for arg in node.args.args] if \"args\" in dir(node) else []\n            )\n            all_names = [item[1] for item in functions_and_classes]\n            functions_and_classes.append(\n                (\n                    type(node).__name__,\n                    node.name,\n                    start_line,\n                    end_line,\n                    parameters,\n                    ast.get_docstring(node),\n                    node,\n                )\n            )\n    return functions_and_classes\n</code></pre>"},{"location":"repo_agent/file_handler/#repo_agent.file_handler.FileHandler.get_modified_file_versions","title":"<code>get_modified_file_versions()</code>","text":"<p>Obtains the content of the latest and prior revisions of a file within a Git repository. Returns <code>None</code> for the prior revision if no previous commits exist.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>The instance containing repo_path and file_path attributes.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the current version and previous version of the file as strings.    If the file has no prior commits, the previous version will be None.</p> Source code in <code>repo_agent/file_handler.py</code> <pre><code>def get_modified_file_versions(self):\n    \"\"\"\n    Obtains the content of the latest and prior revisions of a file within a Git repository. Returns `None` for the prior revision if no previous commits exist.\n\n    Args:\n        self: The instance containing repo_path and file_path attributes.\n\n    Returns:\n        tuple: A tuple containing the current version and previous version of the file as strings.\n               If the file has no prior commits, the previous version will be None.\n\n\n    \"\"\"\n\n    repo = git.Repo(self.repo_path)\n    current_version_path = os.path.join(self.repo_path, self.file_path)\n    with open(current_version_path, \"r\", encoding=\"utf-8\") as file:\n        current_version = file.read()\n    commits = list(repo.iter_commits(paths=self.file_path, max_count=1))\n    previous_version = None\n    if commits:\n        commit = commits[0]\n        try:\n            previous_version = (\n                (commit.tree / self.file_path).data_stream.read().decode(\"utf-8\")\n            )\n        except KeyError:\n            previous_version = None\n    return (current_version, previous_version)\n</code></pre>"},{"location":"repo_agent/file_handler/#repo_agent.file_handler.FileHandler.get_obj_code_info","title":"<code>get_obj_code_info(code_type, code_name, start_line, end_line, params, file_path=None, docstring='', source_node=None)</code>","text":"<p>Collects detailed information about a code element, including its type, name, location, parameters, documentation, and content. It determines if the code includes a return statement and identifies the starting column of the element's name within its source file.</p> <p>Parameters:</p> Name Type Description Default <code>code_type</code> <p>The type of the code object (e.g., 'function', 'class').</p> required <code>code_name</code> <p>The name of the code object.</p> required <code>start_line</code> <p>The starting line number of the code object in its file.</p> required <code>end_line</code> <p>The ending line number of the code object in its file.</p> required <code>params</code> <p>A list of parameters associated with the code object.</p> required <code>file_path</code> <p>The path to the file containing the code object. Defaults to self.file_path if None.</p> <code>None</code> <code>docstring</code> <p>The docstring of the code object. Defaults to ''.</p> <code>''</code> <code>source_node</code> <p>The AST node representing the code object. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing information about the code object, including its type, name,   code content, line numbers, parameters, docstring, whether it has a return statement,   and the column number where the name appears on the starting line.</p> Source code in <code>repo_agent/file_handler.py</code> <pre><code>def get_obj_code_info(\n    self,\n    code_type,\n    code_name,\n    start_line,\n    end_line,\n    params,\n    file_path=None,\n    docstring=\"\",\n    source_node=None,\n):\n    \"\"\"\n    Collects detailed information about a code element, including its type, name, location, parameters, documentation, and content. It determines if the code includes a return statement and identifies the starting column of the element's name within its source file.\n\n    Args:\n        code_type: The type of the code object (e.g., 'function', 'class').\n        code_name: The name of the code object.\n        start_line: The starting line number of the code object in its file.\n        end_line: The ending line number of the code object in its file.\n        params: A list of parameters associated with the code object.\n        file_path: The path to the file containing the code object. Defaults to self.file_path if None.\n        docstring: The docstring of the code object. Defaults to ''.\n        source_node: The AST node representing the code object. Defaults to None.\n\n    Returns:\n        dict: A dictionary containing information about the code object, including its type, name,\n              code content, line numbers, parameters, docstring, whether it has a return statement,\n              and the column number where the name appears on the starting line.\n\n\n    \"\"\"\n\n    code_info = {}\n    code_info[\"type\"] = code_type\n    code_info[\"name\"] = code_name\n    code_info[\"md_content\"] = []\n    code_info[\"code_start_line\"] = start_line\n    code_info[\"code_end_line\"] = end_line\n    code_info[\"params\"] = params\n    code_info[\"docstring\"] = docstring\n    code_info[\"source_node\"] = source_node\n    with open(\n        os.path.join(\n            self.repo_path, file_path if file_path != None else self.file_path\n        ),\n        \"r\",\n        encoding=\"utf-8\",\n    ) as code_file:\n        lines = code_file.readlines()\n        code_content = \"\".join(lines[start_line - 1 : end_line])\n        name_column = lines[start_line - 1].find(code_name)\n        if \"return\" in code_content:\n            have_return = True\n        else:\n            have_return = False\n        code_info[\"have_return\"] = have_return\n        code_info[\"code_content\"] = code_content\n        code_info[\"name_column\"] = name_column\n    return code_info\n</code></pre>"},{"location":"repo_agent/file_handler/#repo_agent.file_handler.FileHandler.read_file","title":"<code>read_file()</code>","text":"<p>No valid docstring found.</p> Source code in <code>repo_agent/file_handler.py</code> <pre><code>def read_file(self):\n    \"\"\"\n    No valid docstring found.\n\n    \"\"\"\n\n    abs_file_path = os.path.join(self.repo_path, self.file_path)\n    with open(abs_file_path, \"r\", encoding=\"utf-8\") as file:\n        content = file.read()\n    return content\n</code></pre>"},{"location":"repo_agent/file_handler/#repo_agent.file_handler.FileHandler.write_file","title":"<code>write_file(file_path, content)</code>","text":"<p>Creates or overwrites a file at the specified path within the repository with the given content. Paths are relative to the repository root and any leading slashes are removed. Ensures necessary directories exist before writing.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <p>The path to the file, relative to the repository root.        Leading slashes are removed.</p> required <code>content</code> <p>The string content to write to the file.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/file_handler.py</code> <pre><code>def write_file(self, file_path, content):\n    \"\"\"\n    Creates or overwrites a file at the specified path within the repository with the given content. Paths are relative to the repository root and any leading slashes are removed. Ensures necessary directories exist before writing.\n\n    Args:\n        file_path: The path to the file, relative to the repository root.\n                   Leading slashes are removed.\n        content: The string content to write to the file.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    if file_path.startswith(\"/\"):\n        file_path = file_path[1:]\n    abs_file_path = os.path.join(self.repo_path, file_path)\n    os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n    with open(abs_file_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(content)\n</code></pre>"},{"location":"repo_agent/log/","title":"Log","text":""},{"location":"repo_agent/log/#repo_agent.log.InterceptHandler","title":"<code>InterceptHandler</code>","text":"<p>               Bases: <code>Handler</code></p> <p>Handles intercepted messages and dispatches them to registered handlers.</p> <p>This class acts as a central point for receiving messages that have been intercepted, and then distributing those messages to specific handler functions based on message type or other criteria. It allows for flexible and extensible message processing without tightly coupling the interception logic with the handling logic.</p> Source code in <code>repo_agent/log.py</code> <pre><code>class InterceptHandler(logging.Handler):\n    \"\"\"\n    Handles intercepted messages and dispatches them to registered handlers.\n\n    This class acts as a central point for receiving messages that have been\n    intercepted, and then distributing those messages to specific handler\n    functions based on message type or other criteria. It allows for flexible\n    and extensible message processing without tightly coupling the interception\n    logic with the handling logic.\n    \"\"\"\n\n    def emit(self, record: logging.LogRecord) -&gt; None:\n        \"\"\"\n        No valid docstring found.\n\n        \"\"\"\n\n        level: str | int\n        try:\n            level = logger.level(record.levelname).name\n        except ValueError:\n            level = record.levelno\n        frame, depth = (inspect.currentframe(), 0)\n        while frame and (depth == 0 or frame.f_code.co_filename == logging.__file__):\n            frame = frame.f_back\n            depth += 1\n        logger.opt(depth=depth, exception=record.exc_info).log(\n            level, record.getMessage()\n        )\n</code></pre>"},{"location":"repo_agent/log/#repo_agent.log.InterceptHandler.emit","title":"<code>emit(record)</code>","text":"<p>No valid docstring found.</p> Source code in <code>repo_agent/log.py</code> <pre><code>def emit(self, record: logging.LogRecord) -&gt; None:\n    \"\"\"\n    No valid docstring found.\n\n    \"\"\"\n\n    level: str | int\n    try:\n        level = logger.level(record.levelname).name\n    except ValueError:\n        level = record.levelno\n    frame, depth = (inspect.currentframe(), 0)\n    while frame and (depth == 0 or frame.f_code.co_filename == logging.__file__):\n        frame = frame.f_back\n        depth += 1\n    logger.opt(depth=depth, exception=record.exc_info).log(\n        level, record.getMessage()\n    )\n</code></pre>"},{"location":"repo_agent/log/#repo_agent.log.set_logger_level_from_config","title":"<code>set_logger_level_from_config(log_level)</code>","text":"<p>Configures the logging output to display messages at or above the specified level and confirms successful configuration.</p> <p>Parameters:</p> Name Type Description Default <code>log_level</code> <p>The desired log level (e.g., \"DEBUG\", \"INFO\").</p> required <p>Returns:</p> Type Description <p>None</p> <p>Sets the logging level for the root logger and prints a success message.</p> Source code in <code>repo_agent/log.py</code> <pre><code>def set_logger_level_from_config(log_level):\n    \"\"\"\n    Configures the logging output to display messages at or above the specified level and confirms successful configuration.\n\n    Args:\n        log_level: The desired log level (e.g., \"DEBUG\", \"INFO\").\n\n    Returns:\n        None\n        Sets the logging level for the root logger and prints a success message.\n\n    \"\"\"\n\n    logger.remove()\n    logger.add(\n        sys.stderr, level=log_level, enqueue=True, backtrace=False, diagnose=False\n    )\n    logging.basicConfig(handlers=[InterceptHandler()], level=0, force=True)\n    logger.success(f\"Log level set to {log_level}!\")\n</code></pre>"},{"location":"repo_agent/main/","title":"Main","text":""},{"location":"repo_agent/main/#repo_agent.main.clean","title":"<code>clean()</code>","text":"<p>Removes generated files and reports completion.</p> <p>This command cleans up any generated fake files from the project.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/main.py</code> <pre><code>@cli.command()\ndef clean():\n    \"\"\"\n    Removes generated files and reports completion.\n\n    This command cleans up any generated fake files from the project.\n\n    Args:\n        None\n\n    Returns:\n        None\n\n    \"\"\"\n\n    delete_fake_files()\n    logger.success(\"Fake files have been cleaned up.\")\n</code></pre>"},{"location":"repo_agent/main/#repo_agent.main.cli","title":"<code>cli()</code>","text":"<p>Entry point for the command-line interface. Provides access to all application commands and options.</p> <p>Parameters:</p> Name Type Description Default <code>version_number</code> <p>The version number of the application.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/main.py</code> <pre><code>@click.group()\n@click.version_option(version_number)\ndef cli():\n    \"\"\"\n    Entry point for the command-line interface. Provides access to all application commands and options.\n\n    Args:\n        version_number: The version number of the application.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"repo_agent/main/#repo_agent.main.diff","title":"<code>diff()</code>","text":"<p>Identifies documentation updates needed based on changes in the source code. It simulates a documentation generation run to preview which documents would be created or modified, without altering existing files.</p> <p>This command compares the existing documentation with the source code to identify which documents need to be generated or updated. It uses fake files to simulate the documentation generation process without actually modifying any real files.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/main.py</code> <pre><code>@cli.command()\ndef diff():\n    \"\"\"\n    Identifies documentation updates needed based on changes in the source code. It simulates a documentation generation run to preview which documents would be created or modified, without altering existing files.\n\n    This command compares the existing documentation with the source code to identify\n    which documents need to be generated or updated. It uses fake files to simulate\n    the documentation generation process without actually modifying any real files.\n\n    Args:\n        None\n\n    Returns:\n        None\n\n    \"\"\"\n\n    try:\n        setting = SettingsManager.get_setting()\n    except ValidationError as e:\n        handle_setting_error(e)\n        return\n    runner = Runner()\n    if runner.meta_info.in_generation_process:\n        click.echo(\"This command only supports pre-check\")\n        raise click.Abort()\n    file_path_reflections, jump_files = make_fake_files()\n    new_meta_info = MetaInfo.init_meta_info(file_path_reflections, jump_files)\n    new_meta_info.load_doc_from_older_meta(runner.meta_info)\n    delete_fake_files()\n    DocItem.check_has_task(\n        new_meta_info.target_repo_hierarchical_tree,\n        ignore_list=setting.project.ignore_list,\n    )\n    if new_meta_info.target_repo_hierarchical_tree.has_task:\n        click.echo(\"The following docs will be generated/updated:\")\n        new_meta_info.target_repo_hierarchical_tree.print_recursive(\n            diff_status=True, ignore_list=setting.project.ignore_list\n        )\n    else:\n        click.echo(\"No docs will be generated/updated, check your source-code update\")\n</code></pre>"},{"location":"repo_agent/main/#repo_agent.main.handle_setting_error","title":"<code>handle_setting_error(e)</code>","text":"<p>Reports configuration issues to the user and terminates execution. Specifically, it identifies missing or invalid settings, provides informative error messages highlighting the problematic fields, and then halts program operation with a clear indication of the failure.</p> <p>Parameters:</p> Name Type Description Default <code>e</code> <code>ValidationError</code> <p>The ValidationError instance containing the error details.</p> required <p>Returns:</p> Type Description <p>None</p> <p>Raises a click.ClickException if there are configuration errors.</p> Source code in <code>repo_agent/main.py</code> <pre><code>def handle_setting_error(e: ValidationError):\n    \"\"\"\n    Reports configuration issues to the user and terminates execution. Specifically, it identifies missing or invalid settings, provides informative error messages highlighting the problematic fields, and then halts program operation with a clear indication of the failure.\n\n    Args:\n        e: The ValidationError instance containing the error details.\n\n    Returns:\n        None\n        Raises a click.ClickException if there are configuration errors.\n\n\n    \"\"\"\n\n    for error in e.errors():\n        field = error[\"loc\"][-1]\n        if error[\"type\"] == \"missing\":\n            message = click.style(\n                f\"Missing required field `{field}`. Please set the `{field}` environment variable.\",\n                fg=\"yellow\",\n            )\n        else:\n            message = click.style(error[\"msg\"], fg=\"yellow\")\n        click.echo(message, err=True, color=True)\n    raise click.ClickException(\n        click.style(\n            \"Program terminated due to configuration errors.\", fg=\"red\", bold=True\n        )\n    )\n</code></pre>"},{"location":"repo_agent/main/#repo_agent.main.run","title":"<code>run(model, temperature, request_timeout, base_url, target_repo_path, hierarchy_path, markdown_docs_path, ignore_list, language, max_thread_count, log_level, print_hierarchy)</code>","text":"<p>Orchestrates the documentation creation for a codebase, configuring settings like the target repository, output paths, and model parameters before executing the generation process. Optionally prints the project's hierarchical structure after completion.</p> <p>This method initializes settings, runs the documentation runner, and optionally prints the repository hierarchy.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Specifies the model to use for completion.</p> required <code>temperature</code> <p>Sets the generation temperature for the model. Lower values make the model more deterministic.</p> required <code>request_timeout</code> <p>Defines the timeout in seconds for the API request.</p> required <code>base_url</code> <p>The base URL for the API calls.</p> required <code>target_repo_path</code> <p>The file system path to the target repository. This path is used as the root for documentation generation.</p> required <code>hierarchy_path</code> <p>The name or path for the project hierarchy file, used to organize documentation structure.</p> required <code>markdown_docs_path</code> <p>The folder path where Markdown documentation will be stored or generated.</p> required <code>ignore_list</code> <p>A comma-separated list of files or directories to ignore during documentation generation.</p> required <code>language</code> <p>The ISO 639 code or language name for the documentation.</p> required <code>max_thread_count</code> <p>The maximum number of threads to use for processing.</p> required <code>log_level</code> <p>Sets the logging level (e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL) for the application.</p> required <code>print_hierarchy</code> <p>If set, prints the hierarchy of the target repository when finished running the main task.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/main.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--model\",\n    \"-m\",\n    default=\"gpt-4o-mini\",\n    show_default=True,\n    help=\"Specifies the model to use for completion.\",\n    type=str,\n)\n@click.option(\n    \"--temperature\",\n    \"-t\",\n    default=0.2,\n    show_default=True,\n    help=\"Sets the generation temperature for the model. Lower values make the model more deterministic.\",\n    type=float,\n)\n@click.option(\n    \"--request-timeout\",\n    \"-r\",\n    default=60,\n    show_default=True,\n    help=\"Defines the timeout in seconds for the API request.\",\n    type=int,\n)\n@click.option(\n    \"--base-url\",\n    \"-b\",\n    default=\"https://api.openai.com/v1\",\n    show_default=True,\n    help=\"The base URL for the API calls.\",\n    type=str,\n)\n@click.option(\n    \"--target-repo-path\",\n    \"-tp\",\n    default=\"\",\n    show_default=True,\n    help=\"The file system path to the target repository. This path is used as the root for documentation generation.\",\n    type=click.Path(file_okay=False),\n)\n@click.option(\n    \"--hierarchy-path\",\n    \"-hp\",\n    default=\".project_doc_record\",\n    show_default=True,\n    help=\"The name or path for the project hierarchy file, used to organize documentation structure.\",\n    type=str,\n)\n@click.option(\n    \"--markdown-docs-path\",\n    \"-mdp\",\n    default=\"markdown_docs\",\n    show_default=True,\n    help=\"The folder path where Markdown documentation will be stored or generated.\",\n    type=str,\n)\n@click.option(\n    \"--ignore-list\",\n    \"-i\",\n    default=\"\",\n    help=\"A comma-separated list of files or directories to ignore during documentation generation.\",\n)\n@click.option(\n    \"--language\",\n    \"-l\",\n    default=\"English\",\n    show_default=True,\n    help=\"The ISO 639 code or language name for the documentation. \",\n    type=str,\n)\n@click.option(\"--max-thread-count\", \"-mtc\", default=4, show_default=True)\n@click.option(\n    \"--log-level\",\n    \"-ll\",\n    default=\"INFO\",\n    show_default=True,\n    help=\"Sets the logging level (e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL) for the application. Default is INFO.\",\n    type=click.Choice([level.value for level in LogLevel], case_sensitive=False),\n)\n@click.option(\n    \"--print-hierarchy\",\n    \"-pr\",\n    is_flag=True,\n    show_default=True,\n    default=False,\n    help=\"If set, prints the hierarchy of the target repository when finished running the main task.\",\n)\ndef run(\n    model,\n    temperature,\n    request_timeout,\n    base_url,\n    target_repo_path,\n    hierarchy_path,\n    markdown_docs_path,\n    ignore_list,\n    language,\n    max_thread_count,\n    log_level,\n    print_hierarchy,\n):\n    \"\"\"\n    Orchestrates the documentation creation for a codebase, configuring settings like the target repository, output paths, and model parameters before executing the generation process. Optionally prints the project's hierarchical structure after completion.\n\n    This method initializes settings, runs the documentation runner, and optionally prints the repository hierarchy.\n\n    Args:\n        model: Specifies the model to use for completion.\n        temperature: Sets the generation temperature for the model. Lower values make the model more deterministic.\n        request_timeout: Defines the timeout in seconds for the API request.\n        base_url: The base URL for the API calls.\n        target_repo_path: The file system path to the target repository. This path is used as the root for documentation generation.\n        hierarchy_path: The name or path for the project hierarchy file, used to organize documentation structure.\n        markdown_docs_path: The folder path where Markdown documentation will be stored or generated.\n        ignore_list: A comma-separated list of files or directories to ignore during documentation generation.\n        language: The ISO 639 code or language name for the documentation.\n        max_thread_count:  The maximum number of threads to use for processing.\n        log_level: Sets the logging level (e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL) for the application.\n        print_hierarchy: If set, prints the hierarchy of the target repository when finished running the main task.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    try:\n        setting = SettingsManager.initialize_with_params(\n            target_repo=target_repo_path,\n            hierarchy_name=hierarchy_path,\n            markdown_docs_name=markdown_docs_path,\n            ignore_list=[item.strip() for item in ignore_list.split(\",\") if item],\n            language=language,\n            log_level=log_level,\n            model=model,\n            temperature=temperature,\n            request_timeout=request_timeout,\n            openai_base_url=base_url,\n            max_thread_count=max_thread_count,\n        )\n        set_logger_level_from_config(log_level=log_level)\n    except ValidationError as e:\n        handle_setting_error(e)\n        return\n    runner = Runner()\n    runner.run()\n    logger.success(\"Documentation task completed.\")\n    if print_hierarchy:\n        runner.meta_info.target_repo_hierarchical_tree.print_recursive()\n        logger.success(\"Hierarchy printed.\")\n</code></pre>"},{"location":"repo_agent/main/#repo_agent.main.run_outside_cli","title":"<code>run_outside_cli(model, temperature, request_timeout, base_url, target_repo_path, hierarchy_path, markdown_docs_path, ignore_list, language, max_thread_count, log_level, print_hierarchy)</code>","text":"<p>Orchestrates the documentation generation for a given repository, utilizing provided settings and running the core processing logic. It initializes configurations, handles potential errors during setup, executes the documentation runner, and outputs success messages along with an optional hierarchical view of the target repository.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model to use for generating documentation.</p> required <code>temperature</code> <p>The temperature to use for the model.</p> required <code>request_timeout</code> <p>The request timeout in seconds.</p> required <code>base_url</code> <p>The base URL for the OpenAI API.</p> required <code>target_repo_path</code> <p>The path to the target repository.</p> required <code>hierarchy_path</code> <p>The path to the hierarchy file.</p> required <code>markdown_docs_path</code> <p>The path to store markdown documentation.</p> required <code>ignore_list</code> <p>A comma-separated list of files or directories to ignore.</p> required <code>language</code> <p>The programming language of the repository.</p> required <code>max_thread_count</code> <p>The maximum number of threads to use.</p> required <code>log_level</code> <p>The log level to set.</p> required <code>print_hierarchy</code> <p>Whether to print the target repo hierarchical tree.</p> required <p>Returns:</p> Type Description <p>None. Prints success messages and optionally the hierarchy to the console.</p> Source code in <code>repo_agent/main.py</code> <pre><code>def run_outside_cli(\n    model,\n    temperature,\n    request_timeout,\n    base_url,\n    target_repo_path,\n    hierarchy_path,\n    markdown_docs_path,\n    ignore_list,\n    language,\n    max_thread_count,\n    log_level,\n    print_hierarchy,\n):\n    \"\"\"\n    Orchestrates the documentation generation for a given repository, utilizing provided settings and running the core processing logic. It initializes configurations, handles potential errors during setup, executes the documentation runner, and outputs success messages along with an optional hierarchical view of the target repository.\n\n    Args:\n        model: The model to use for generating documentation.\n        temperature: The temperature to use for the model.\n        request_timeout: The request timeout in seconds.\n        base_url: The base URL for the OpenAI API.\n        target_repo_path: The path to the target repository.\n        hierarchy_path: The path to the hierarchy file.\n        markdown_docs_path: The path to store markdown documentation.\n        ignore_list: A comma-separated list of files or directories to ignore.\n        language: The programming language of the repository.\n        max_thread_count: The maximum number of threads to use.\n        log_level: The log level to set.\n        print_hierarchy: Whether to print the target repo hierarchical tree.\n\n    Returns:\n        None. Prints success messages and optionally the hierarchy to the console.\n\n\n    \"\"\"\n\n    try:\n        setting = SettingsManager.initialize_with_params(\n            target_repo=target_repo_path,\n            hierarchy_name=hierarchy_path,\n            markdown_docs_name=markdown_docs_path,\n            ignore_list=[item.strip() for item in ignore_list.split(\",\") if item],\n            language=language,\n            log_level=log_level,\n            model=model,\n            temperature=temperature,\n            request_timeout=request_timeout,\n            openai_base_url=base_url,\n            max_thread_count=max_thread_count,\n        )\n        set_logger_level_from_config(log_level=log_level)\n    except ValidationError as e:\n        handle_setting_error(e)\n        return\n    runner = Runner()\n    runner.run()\n    logger.success(\"Documentation task completed.\")\n    if print_hierarchy:\n        runner.meta_info.target_repo_hierarchical_tree.print_recursive()\n        logger.success(\"Hierarchy printed.\")\n</code></pre>"},{"location":"repo_agent/module_summarization/","title":"Module Summarization","text":""},{"location":"repo_agent/module_summarization/#repo_agent.module_summarization.create_module_summary","title":"<code>create_module_summary(name, file_summaries, submodule_summaries, chat_engine)</code>","text":"<p>Combines file and submodule summaries to generate a concise overview of the module's content.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the module.</p> required <code>file_summaries</code> <code>List[str]</code> <p>A list of strings, where each string is a summary of a file in the module.</p> required <code>submodule_summaries</code> <code>List[str]</code> <p>A list of strings, where each string is a summary of a submodule.</p> required <code>chat_engine</code> <p>An instance of a chat engine used for summarizing the module content.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The summarized module content as a string.</p> Source code in <code>repo_agent/module_summarization.py</code> <pre><code>def create_module_summary(\n    name: str, file_summaries: List[str], submodule_summaries: List[str], chat_engine\n) -&gt; str:\n    \"\"\"\n    Combines file and submodule summaries to generate a concise overview of the module's content.\n\n    Args:\n        name: The name of the module.\n        file_summaries: A list of strings, where each string is a summary of a file in the module.\n        submodule_summaries: A list of strings, where each string is a summary of a submodule.\n        chat_engine: An instance of a chat engine used for summarizing the module content.\n\n    Returns:\n        str: The summarized module content as a string.\n\n\n    \"\"\"\n\n    summary_content = [\n        f\"Module: {name}\",\n        \"\\n## Files Summary:\\n\\n- \" + \"\\n- \".join(file_summaries).replace(\"#\", \"##\"),\n        \"\\n\\n## Submodules Summary:\\n\\n- \"\n        + \"\\n- \".join(submodule_summaries).replace(\"#\", \"##\"),\n    ]\n    result = chat_engine.summarize_module(\"\\n-----\\n\".join(summary_content))\n    return result\n</code></pre>"},{"location":"repo_agent/module_summarization/#repo_agent.module_summarization.summarize_repository","title":"<code>summarize_repository(root_dir, repo_structure, chat_engine)</code>","text":"<p>Recursively generates summaries for each directory within a repository, identifying and summarizing Python files based on their structure and content. It handles ignored folders and consolidates summaries to provide an overview of each module and its submodules.</p> <p>This function recursively traverses the directory structure of a given repository, identifies Python files, and generates summaries based on the provided repository structure and chat engine. It also handles ignored folders as specified in the settings. The tool is part of a comprehensive system designed to automate the generation and management of documentation for a Git repository, ensuring that documentation is up-to-date and accurate.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>The root directory of the repository.</p> required <code>repo_structure</code> <code>Dict[str, Any]</code> <p>A dictionary containing the structure of the repository, mapping file paths to their respective components.</p> required <code>chat_engine</code> <code>Any</code> <p>The chat engine used to generate summaries.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the summary of the repository, including the name, path, file summaries, submodules, and module summary for each directory.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the root directory does not exist or is not a valid directory.</p> Note <p>See also: The <code>summarize_modules</code> method in the <code>Runner</code> class for an example of how this function is used in a workflow.</p> Source code in <code>repo_agent/module_summarization.py</code> <pre><code>def summarize_repository(root_dir: str, repo_structure, chat_engine) -&gt; Dict[str, Any]:\n    \"\"\"\n    Recursively generates summaries for each directory within a repository, identifying and summarizing Python files based on their structure and content. It handles ignored folders and consolidates summaries to provide an overview of each module and its submodules.\n\n    This function recursively traverses the directory structure of a given repository, identifies Python files, and generates summaries based on the provided repository structure and chat engine. It also handles ignored folders as specified in the settings. The tool is part of a comprehensive system designed to automate the generation and management of documentation for a Git repository, ensuring that documentation is up-to-date and accurate.\n\n    Args:\n        root_dir (str): The root directory of the repository.\n        repo_structure (Dict[str, Any]): A dictionary containing the structure of the repository, mapping file paths to their respective components.\n        chat_engine (Any): The chat engine used to generate summaries.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the summary of the repository, including the name, path, file summaries, submodules, and module summary for each directory.\n\n    Raises:\n        ValueError: If the root directory does not exist or is not a valid directory.\n\n    Note:\n        See also: The `summarize_modules` method in the `Runner` class for an example of how this function is used in a workflow.\n\n    \"\"\"\n\n    def summarize_directory(\n        directory: Path, repo_structure, chat_engine, root_dir\n    ) -&gt; Dict[str, Any]:\n        settings = SettingsManager().get_setting()\n        ignored_folders = settings.project.ignore_list\n        file_paths = []\n        subdir_paths = []\n        for item in directory.iterdir():\n            if item.is_file() and item.suffix == \".py\":\n                file_paths.append(item)\n            elif item.is_dir() and str(item) not in ignored_folders:\n                subdir_paths.append(item)\n        file_summaries = []\n        for file_path in file_paths:\n            if file_path.is_file() and file_path.suffix == \".py\":\n                stripped_file_path = os.path.relpath(file_path, start=root_path)\n                desc = [f\"File {file_path} has such components:\"]\n                if (\n                    stripped_file_path in repo_structure\n                    and repo_structure[stripped_file_path]\n                ):\n                    for obj in list(\n                        filter(\n                            lambda x: x[\"type\"] in [\"ClassDef\", \"FunctionDef\"],\n                            repo_structure[stripped_file_path],\n                        )\n                    ):\n                        if obj[\"md_content\"]:\n                            desc.append(obj[\"md_content\"][-1].split(\"\\n\\n\")[0])\n                    if len(desc) &gt; 1:\n                        file_summaries.append(\"\\n\".join(desc))\n        submodule_summaries = list(\n            filter(\n                lambda x: x[\"module_summary\"],\n                [\n                    summarize_directory(subdir, repo_structure, chat_engine, root_dir)\n                    for subdir in subdir_paths\n                ],\n            )\n        )\n        submodules_summaries_text = [\n            submod[\"module_summary\"] for submod in submodule_summaries\n        ]\n        if not file_summaries and (not submodules_summaries_text):\n            module_summary = \"\"\n        else:\n            module_summary = create_module_summary(\n                directory.name, file_summaries, submodules_summaries_text, chat_engine\n            )\n        return {\n            \"name\": directory.name,\n            \"path\": str(directory),\n            \"file_summaries\": file_summaries,\n            \"submodules\": submodule_summaries,\n            \"module_summary\": module_summary,\n        }\n\n    root_path = Path(root_dir)\n    return summarize_directory(root_path, repo_structure, chat_engine, root_dir)\n</code></pre>"},{"location":"repo_agent/multi_task_dispatch/","title":"Multi Task Dispatch","text":""},{"location":"repo_agent/multi_task_dispatch/#repo_agent.multi_task_dispatch.Task","title":"<code>Task</code>","text":"<p>Represents a task with dependencies and extra information.</p> <p>Attributes:</p> Name Type Description <code>task_id</code> <p>The unique identifier for the task.</p> <code>dependencies</code> <p>A list of task IDs that must complete before this task can start.</p> <code>extra_info</code> <p>Any additional information associated with the task.</p> Source code in <code>repo_agent/multi_task_dispatch.py</code> <pre><code>class Task:\n    \"\"\"\n    Represents a task with dependencies and extra information.\n\n    Attributes:\n        task_id: The unique identifier for the task.\n        dependencies: A list of task IDs that must complete before this task can start.\n        extra_info: Any additional information associated with the task.\n    \"\"\"\n\n    def __init__(self, task_id: int, dependencies: List[Task], extra_info: Any = None):\n        self.task_id = task_id\n        self.extra_info = extra_info\n        self.dependencies = dependencies\n        self.status = 0\n</code></pre>"},{"location":"repo_agent/multi_task_dispatch/#repo_agent.multi_task_dispatch.TaskManager","title":"<code>TaskManager</code>","text":"<p>Manages a queue of tasks with dependencies and thread safety.</p> <p>Attributes:</p> Name Type Description <code>task_dict</code> <code>Dict[int, Task]</code> <p>Stores tasks, keyed by their IDs.</p> <code>lock</code> <code>Dict[int, Task]</code> <p>A lock for thread-safe access to the task dictionary.</p> <code>next_id</code> <code>Dict[int, Task]</code> <p>Generates unique IDs for new tasks.</p> <code>next_query_id</code> <code>Dict[int, Task]</code> <p>Generates unique IDs for queries.</p> Source code in <code>repo_agent/multi_task_dispatch.py</code> <pre><code>class TaskManager:\n    \"\"\"\n    Manages a queue of tasks with dependencies and thread safety.\n\n    Attributes:\n        task_dict: Stores tasks, keyed by their IDs.\n        lock: A lock for thread-safe access to the task dictionary.\n        next_id: Generates unique IDs for new tasks.\n        next_query_id: Generates unique IDs for queries.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the internal data structures for managing tasks and assigning unique identifiers.\n\n\n        This constructor initializes the internal data structures for managing tasks,\n        including a dictionary to store tasks, a lock for thread safety, and IDs for\n        new tasks and queries.\n\n        Args:\n            None\n\n        Returns:\n            None\n\n        \"\"\"\n\n        self.task_dict: Dict[int, Task] = {}\n        self.task_lock = threading.Lock()\n        self.now_id = 0\n        self.query_id = 0\n\n    @property\n    def all_success(self) -&gt; bool:\n        \"\"\"\n        Checks if all tasks have finished. Returns True if no tasks remain, False otherwise.\n\n                    Returns:\n                        bool: True if the task dictionary is empty (all tasks successful), False otherwise.\n\n        \"\"\"\n\n        return len(self.task_dict) == 0\n\n    def add_task(self, dependency_task_id: List[int], extra=None) -&gt; int:\n        \"\"\"\n        Registers a new task, specifying its dependencies and optional additional information.\n\n        Args:\n            dependency_task_id: A list of IDs representing tasks that this task depends on.\n            extra: Optional additional information associated with the task.\n\n        Returns:\n            int: The ID of the newly added task.\n\n\n        \"\"\"\n\n        with self.task_lock:\n            depend_tasks = [\n                self.task_dict[task_id]\n                for task_id in dependency_task_id\n                if task_id in self.task_dict\n            ]\n            self.task_dict[self.now_id] = Task(\n                task_id=self.now_id, dependencies=depend_tasks, extra_info=extra\n            )\n            self.now_id += 1\n            return self.now_id - 1\n\n    def get_next_task(self, process_id: int):\n        \"\"\"\n        Retrieves a ready-to-execute task for a specified process, considering dependencies and current status. Returns the task and its ID, or (None, -1) if no suitable task is found.\n\n        Args:\n            process_id: The ID of the process requesting a task.\n\n        Returns:\n            tuple: A tuple containing the task object and its ID. Returns (None, -1) if no task is available.\n\n\n        \"\"\"\n\n        with self.task_lock:\n            self.query_id += 1\n            for task_id in self.task_dict.keys():\n                ready = (\n                    len(self.task_dict[task_id].dependencies) == 0\n                    and self.task_dict[task_id].status == 0\n                )\n                if ready:\n                    self.task_dict[task_id].status = 1\n                    print(\n                        f\"{Fore.RED}[process {process_id}]{Style.RESET_ALL}: get task({task_id}), remain({len(self.task_dict)})\"\n                    )\n                    return (self.task_dict[task_id], task_id)\n            return (None, -1)\n\n    def mark_completed(self, task_id: int):\n        \"\"\"\n        Removes a task and updates any dependent tasks by removing it from their dependency lists.\n\n        Args:\n            task_id: The ID of the task to mark as completed.\n\n        Returns:\n            None\n\n\n        \"\"\"\n\n        with self.task_lock:\n            target_task = self.task_dict[task_id]\n            for task in self.task_dict.values():\n                if target_task in task.dependencies:\n                    task.dependencies.remove(target_task)\n            self.task_dict.pop(task_id)\n</code></pre>"},{"location":"repo_agent/multi_task_dispatch/#repo_agent.multi_task_dispatch.TaskManager.all_success","title":"<code>all_success</code>  <code>property</code>","text":"<p>Checks if all tasks have finished. Returns True if no tasks remain, False otherwise.</p> <pre><code>        Returns:\n            bool: True if the task dictionary is empty (all tasks successful), False otherwise.\n</code></pre>"},{"location":"repo_agent/multi_task_dispatch/#repo_agent.multi_task_dispatch.TaskManager.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the internal data structures for managing tasks and assigning unique identifiers.</p> <p>This constructor initializes the internal data structures for managing tasks, including a dictionary to store tasks, a lock for thread safety, and IDs for new tasks and queries.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/multi_task_dispatch.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the internal data structures for managing tasks and assigning unique identifiers.\n\n\n    This constructor initializes the internal data structures for managing tasks,\n    including a dictionary to store tasks, a lock for thread safety, and IDs for\n    new tasks and queries.\n\n    Args:\n        None\n\n    Returns:\n        None\n\n    \"\"\"\n\n    self.task_dict: Dict[int, Task] = {}\n    self.task_lock = threading.Lock()\n    self.now_id = 0\n    self.query_id = 0\n</code></pre>"},{"location":"repo_agent/multi_task_dispatch/#repo_agent.multi_task_dispatch.TaskManager.add_task","title":"<code>add_task(dependency_task_id, extra=None)</code>","text":"<p>Registers a new task, specifying its dependencies and optional additional information.</p> <p>Parameters:</p> Name Type Description Default <code>dependency_task_id</code> <code>List[int]</code> <p>A list of IDs representing tasks that this task depends on.</p> required <code>extra</code> <p>Optional additional information associated with the task.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The ID of the newly added task.</p> Source code in <code>repo_agent/multi_task_dispatch.py</code> <pre><code>def add_task(self, dependency_task_id: List[int], extra=None) -&gt; int:\n    \"\"\"\n    Registers a new task, specifying its dependencies and optional additional information.\n\n    Args:\n        dependency_task_id: A list of IDs representing tasks that this task depends on.\n        extra: Optional additional information associated with the task.\n\n    Returns:\n        int: The ID of the newly added task.\n\n\n    \"\"\"\n\n    with self.task_lock:\n        depend_tasks = [\n            self.task_dict[task_id]\n            for task_id in dependency_task_id\n            if task_id in self.task_dict\n        ]\n        self.task_dict[self.now_id] = Task(\n            task_id=self.now_id, dependencies=depend_tasks, extra_info=extra\n        )\n        self.now_id += 1\n        return self.now_id - 1\n</code></pre>"},{"location":"repo_agent/multi_task_dispatch/#repo_agent.multi_task_dispatch.TaskManager.get_next_task","title":"<code>get_next_task(process_id)</code>","text":"<p>Retrieves a ready-to-execute task for a specified process, considering dependencies and current status. Returns the task and its ID, or (None, -1) if no suitable task is found.</p> <p>Parameters:</p> Name Type Description Default <code>process_id</code> <code>int</code> <p>The ID of the process requesting a task.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the task object and its ID. Returns (None, -1) if no task is available.</p> Source code in <code>repo_agent/multi_task_dispatch.py</code> <pre><code>def get_next_task(self, process_id: int):\n    \"\"\"\n    Retrieves a ready-to-execute task for a specified process, considering dependencies and current status. Returns the task and its ID, or (None, -1) if no suitable task is found.\n\n    Args:\n        process_id: The ID of the process requesting a task.\n\n    Returns:\n        tuple: A tuple containing the task object and its ID. Returns (None, -1) if no task is available.\n\n\n    \"\"\"\n\n    with self.task_lock:\n        self.query_id += 1\n        for task_id in self.task_dict.keys():\n            ready = (\n                len(self.task_dict[task_id].dependencies) == 0\n                and self.task_dict[task_id].status == 0\n            )\n            if ready:\n                self.task_dict[task_id].status = 1\n                print(\n                    f\"{Fore.RED}[process {process_id}]{Style.RESET_ALL}: get task({task_id}), remain({len(self.task_dict)})\"\n                )\n                return (self.task_dict[task_id], task_id)\n        return (None, -1)\n</code></pre>"},{"location":"repo_agent/multi_task_dispatch/#repo_agent.multi_task_dispatch.TaskManager.mark_completed","title":"<code>mark_completed(task_id)</code>","text":"<p>Removes a task and updates any dependent tasks by removing it from their dependency lists.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>int</code> <p>The ID of the task to mark as completed.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/multi_task_dispatch.py</code> <pre><code>def mark_completed(self, task_id: int):\n    \"\"\"\n    Removes a task and updates any dependent tasks by removing it from their dependency lists.\n\n    Args:\n        task_id: The ID of the task to mark as completed.\n\n    Returns:\n        None\n\n\n    \"\"\"\n\n    with self.task_lock:\n        target_task = self.task_dict[task_id]\n        for task in self.task_dict.values():\n            if target_task in task.dependencies:\n                task.dependencies.remove(target_task)\n        self.task_dict.pop(task_id)\n</code></pre>"},{"location":"repo_agent/multi_task_dispatch/#repo_agent.multi_task_dispatch.worker","title":"<code>worker(task_manager, process_id, handler)</code>","text":"<p>Process that repeatedly fetches tasks and executes a provided handler on their associated data, until all tasks are successfully completed.</p> <p>Parameters:</p> Name Type Description Default <code>task_manager</code> <p>The task manager object providing access to tasks.</p> required <code>process_id</code> <code>int</code> <p>The ID of the current worker process.</p> required <code>handler</code> <code>Callable</code> <p>A callable that processes the extra information from a task.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/multi_task_dispatch.py</code> <pre><code>def worker(task_manager, process_id: int, handler: Callable):\n    \"\"\"\n    Process that repeatedly fetches tasks and executes a provided handler on their associated data, until all tasks are successfully completed.\n\n    Args:\n        task_manager: The task manager object providing access to tasks.\n        process_id: The ID of the current worker process.\n        handler: A callable that processes the extra information from a task.\n\n    Returns:\n        None\n\n\n    \"\"\"\n\n    while True:\n        if task_manager.all_success:\n            return\n        task, task_id = task_manager.get_next_task(process_id)\n        if task is None:\n            time.sleep(0.5)\n            continue\n        handler(task.extra_info)\n        task_manager.mark_completed(task.task_id)\n</code></pre>"},{"location":"repo_agent/project_manager/","title":"Roject Manager","text":""},{"location":"repo_agent/project_manager/#repo_agent.project_manager.ProjectManager","title":"<code>ProjectManager</code>","text":"<p>Manages a software project by analyzing its structure and dependencies.</p> <p>Attributes:</p> Name Type Description <code>repo_path</code> <p>The path to the repository being analyzed.</p> <code>project_hierarchy</code> <p>The relative path within the repository to the project_hierarchy.json file.</p> <code>graph</code> <p>A networkx graph representing the project's dependency structure.</p> Source code in <code>repo_agent/project_manager.py</code> <pre><code>class ProjectManager:\n    \"\"\"\n    Manages a software project by analyzing its structure and dependencies.\n\n    Attributes:\n        repo_path: The path to the repository being analyzed.\n        project_hierarchy: The relative path within the repository to the\n            project_hierarchy.json file.\n        graph: A networkx graph representing the project's dependency structure.\n    \"\"\"\n\n    def __init__(self, repo_path, project_hierarchy):\n        \"\"\"\n        Initializes a project analysis environment with the repository path and location of its project hierarchy definition.\n\n\n\n        Args:\n            repo_path: The path to the repository being analyzed.\n            project_hierarchy: The relative path within the repository to the\n                project_hierarchy.json file.\n\n        Returns:\n            None\n\n        \"\"\"\n\n        self.repo_path = repo_path\n        self.project = jedi.Project(self.repo_path)\n        self.project_hierarchy = os.path.join(\n            self.repo_path, project_hierarchy, \"project_hierarchy.json\"\n        )\n\n    def get_project_structure(self):\n        \"\"\"\n        No valid docstring found.\n\n        \"\"\"\n\n        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"  \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith(\".\"):\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    structure.append(new_prefix + name)\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith(\".py\"):\n                    structure.append(new_prefix + name)\n\n        structure = []\n        walk_dir(self.repo_path)\n        return \"\\n\".join(structure)\n\n    def build_path_tree(self, who_reference_me, reference_who, doc_item_path):\n        \"\"\"\n        Constructs a hierarchical tree structure representing relationships between documentation paths, visualizing references to and from a given item.\n\n        Args:\n            who_reference_me: A list of paths that reference this item.\n            reference_who: A list of paths that this item references.\n            doc_item_path: The path to the documentation item.\n\n        Returns:\n            str: A string representation of the constructed path tree.\n\n\n        \"\"\"\n\n        from collections import defaultdict\n\n        def tree():\n            return defaultdict(tree)\n\n        path_tree = tree()\n        for path_list in [who_reference_me, reference_who]:\n            for path in path_list:\n                parts = path.split(os.sep)\n                node = path_tree\n                for part in parts:\n                    node = node[part]\n        parts = doc_item_path.split(os.sep)\n        parts[-1] = \"\u2733\ufe0f\" + parts[-1]\n        node = path_tree\n        for part in parts:\n            node = node[part]\n\n        def tree_to_string(tree, indent=0):\n            s = \"\"\n            for key, value in sorted(tree.items()):\n                s += \"    \" * indent + key + \"\\n\"\n                if isinstance(value, dict):\n                    s += tree_to_string(value, indent + 1)\n            return s\n\n        return tree_to_string(path_tree)\n</code></pre>"},{"location":"repo_agent/project_manager/#repo_agent.project_manager.ProjectManager.__init__","title":"<code>__init__(repo_path, project_hierarchy)</code>","text":"<p>Initializes a project analysis environment with the repository path and location of its project hierarchy definition.</p> <p>Parameters:</p> Name Type Description Default <code>repo_path</code> <p>The path to the repository being analyzed.</p> required <code>project_hierarchy</code> <p>The relative path within the repository to the project_hierarchy.json file.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/project_manager.py</code> <pre><code>def __init__(self, repo_path, project_hierarchy):\n    \"\"\"\n    Initializes a project analysis environment with the repository path and location of its project hierarchy definition.\n\n\n\n    Args:\n        repo_path: The path to the repository being analyzed.\n        project_hierarchy: The relative path within the repository to the\n            project_hierarchy.json file.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    self.repo_path = repo_path\n    self.project = jedi.Project(self.repo_path)\n    self.project_hierarchy = os.path.join(\n        self.repo_path, project_hierarchy, \"project_hierarchy.json\"\n    )\n</code></pre>"},{"location":"repo_agent/project_manager/#repo_agent.project_manager.ProjectManager.build_path_tree","title":"<code>build_path_tree(who_reference_me, reference_who, doc_item_path)</code>","text":"<p>Constructs a hierarchical tree structure representing relationships between documentation paths, visualizing references to and from a given item.</p> <p>Parameters:</p> Name Type Description Default <code>who_reference_me</code> <p>A list of paths that reference this item.</p> required <code>reference_who</code> <p>A list of paths that this item references.</p> required <code>doc_item_path</code> <p>The path to the documentation item.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A string representation of the constructed path tree.</p> Source code in <code>repo_agent/project_manager.py</code> <pre><code>def build_path_tree(self, who_reference_me, reference_who, doc_item_path):\n    \"\"\"\n    Constructs a hierarchical tree structure representing relationships between documentation paths, visualizing references to and from a given item.\n\n    Args:\n        who_reference_me: A list of paths that reference this item.\n        reference_who: A list of paths that this item references.\n        doc_item_path: The path to the documentation item.\n\n    Returns:\n        str: A string representation of the constructed path tree.\n\n\n    \"\"\"\n\n    from collections import defaultdict\n\n    def tree():\n        return defaultdict(tree)\n\n    path_tree = tree()\n    for path_list in [who_reference_me, reference_who]:\n        for path in path_list:\n            parts = path.split(os.sep)\n            node = path_tree\n            for part in parts:\n                node = node[part]\n    parts = doc_item_path.split(os.sep)\n    parts[-1] = \"\u2733\ufe0f\" + parts[-1]\n    node = path_tree\n    for part in parts:\n        node = node[part]\n\n    def tree_to_string(tree, indent=0):\n        s = \"\"\n        for key, value in sorted(tree.items()):\n            s += \"    \" * indent + key + \"\\n\"\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n\n    return tree_to_string(path_tree)\n</code></pre>"},{"location":"repo_agent/project_manager/#repo_agent.project_manager.ProjectManager.get_project_structure","title":"<code>get_project_structure()</code>","text":"<p>No valid docstring found.</p> Source code in <code>repo_agent/project_manager.py</code> <pre><code>def get_project_structure(self):\n    \"\"\"\n    No valid docstring found.\n\n    \"\"\"\n\n    def walk_dir(root, prefix=\"\"):\n        structure.append(prefix + os.path.basename(root))\n        new_prefix = prefix + \"  \"\n        for name in sorted(os.listdir(root)):\n            if name.startswith(\".\"):\n                continue\n            path = os.path.join(root, name)\n            if os.path.isdir(path):\n                structure.append(new_prefix + name)\n                walk_dir(path, new_prefix)\n            elif os.path.isfile(path) and name.endswith(\".py\"):\n                structure.append(new_prefix + name)\n\n    structure = []\n    walk_dir(self.repo_path)\n    return \"\\n\".join(structure)\n</code></pre>"},{"location":"repo_agent/runner/","title":"Runner","text":""},{"location":"repo_agent/runner/#repo_agent.runner.Runner","title":"<code>Runner</code>","text":"<p>Runner class for orchestrating documentation generation and updates.</p> <p>This class manages the entire process of analyzing a project's code, generating documentation using a language model, and committing changes to a Git repository. It handles tasks such as identifying components, creating documentation stubs, generating detailed descriptions, and maintaining consistency between the codebase and the generated documentation.</p> <p>Class Methods: - init:</p> Source code in <code>repo_agent/runner.py</code> <pre><code>class Runner:\n    \"\"\"\n    Runner class for orchestrating documentation generation and updates.\n\n    This class manages the entire process of analyzing a project's code, generating\n    documentation using a language model, and committing changes to a Git repository.\n    It handles tasks such as identifying components, creating documentation stubs,\n    generating detailed descriptions, and maintaining consistency between the codebase\n    and the generated documentation.\n\n    Class Methods:\n    - __init__:\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the processing pipeline by loading project settings, preparing the repository, and setting up tools for analysis and interaction. It handles initial metadata creation or loading from existing checkpoints, ensuring a consistent view of the project's structure and content.\n\n        This constructor sets up the necessary components for processing project documentation,\n        including loading settings, copying the mkdocs configuration file, initializing project and\n        change detectors, setting up a chat engine, and creating or loading meta-information about the project.\n\n        Args:\n            None\n\n        Returns:\n            None\n\n        \"\"\"\n\n        self.setting = SettingsManager.get_setting()\n        self.absolute_project_hierarchy_path = (\n            self.setting.project.target_repo / self.setting.project.hierarchy_name\n        )\n        shutil.copy(\"mkdocs.yml\", Path(self.setting.project.target_repo, \"mkdocs.yml\"))\n        self.project_manager = ProjectManager(\n            repo_path=self.setting.project.target_repo,\n            project_hierarchy=self.setting.project.hierarchy_name,\n        )\n        self.change_detector = ChangeDetector(\n            repo_path=self.setting.project.target_repo\n        )\n        self.chat_engine = ChatEngine(project_manager=self.project_manager)\n        file_path_reflections, jump_files = make_fake_files()\n        setting = SettingsManager.get_setting()\n        if not self.absolute_project_hierarchy_path.exists():\n            self.meta_info = MetaInfo.init_meta_info(file_path_reflections, jump_files)\n            self.meta_info.checkpoint(\n                target_dir_path=self.absolute_project_hierarchy_path\n            )\n\n        else:\n            project_abs_path = setting.project.target_repo\n            file_handler = FileHandler(project_abs_path, None)\n            repo_structure = file_handler.generate_overall_structure(\n                file_path_reflections, jump_files\n            )\n            self.meta_info = MetaInfo.from_checkpoint_path(\n                self.absolute_project_hierarchy_path, repo_structure\n            )\n            SettingsManager.get_setting().project.main_idea = self.meta_info.main_idea\n        self.runner_lock = threading.Lock()\n\n    def get_all_pys(self, directory):\n        \"\"\"\n        No valid docstring found.\n\n        \"\"\"\n\n        python_files = []\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                if file.endswith(\".py\"):\n                    python_files.append(os.path.join(root, file))\n        return python_files\n\n    def generate_doc_for_a_single_item(self, doc_item: DocItem):\n        \"\"\"\n        Processes a single documentation item, generating content with a chat engine and updating its status. It respects ignore lists to avoid unnecessary generation and handles potential errors gracefully by logging them and ensuring an empty entry in the output. A checkpoint save occurs after successful or failed processing.\n\n        This method checks if the DocItem needs to be generated based on ignore lists,\n        generates the documentation using a chat engine, appends it to the DocItem's content,\n        updates the item status and performs a checkpoint save. It handles potential exceptions\n        during document generation by logging them and appending an empty string to the content.\n\n        Args:\n            doc_item: The DocItem for which documentation needs to be generated.\n\n        Returns:\n            None\n\n\n        \"\"\"\n\n        settings = SettingsManager.get_setting()\n        try:\n            if not need_to_generate(doc_item, self.setting.project.ignore_list):\n                print(\n                    f\"Content ignored/Document generated, skipping: {doc_item.get_full_name()}\"\n                )\n            else:\n                print(\n                    f\" -- Generating document  {Fore.LIGHTYELLOW_EX}{doc_item.item_type.name}: {doc_item.get_full_name()}{Style.RESET_ALL}\"\n                )\n                response_message = self.chat_engine.generate_doc(doc_item=doc_item)\n                doc_item.md_content.append(response_message)\n                if settings.project.main_idea:\n                    doc_item.item_status = DocItemStatus.doc_up_to_date\n                self.meta_info.checkpoint(\n                    target_dir_path=self.absolute_project_hierarchy_path\n                )\n        except Exception:\n            logger.exception(\n                f\"Document generation failed after multiple attempts, skipping: {doc_item.get_full_name()}\"\n            )\n            doc_item.md_content.append(\"\")\n            if settings.project.main_idea:\n                doc_item.item_status = DocItemStatus.doc_up_to_date\n\n    def generate_main_project_idea(self, docs: List[Dict]):\n        \"\"\"\n        Synthesizes a cohesive project concept by analyzing the functionality and relationships between software components.\n\n        Args:\n            docs: A list of dictionaries, each detailing a component's name, description, and position within the system architecture.\n\n        Returns:\n            str: A concise summary outlining the overall project direction derived from the component details.\n\n\n                Args:\n                    docs: A list of dictionaries, where each dictionary represents a\n                        component and contains its name, description, and hierarchical path.\n\n                Returns:\n                    str: The generated project idea as a string.\n\n\n        \"\"\"\n\n        str_obj = []\n        for doc in docs:\n            str_obj.append(\n                f\"Component name: {doc['obj_name']}\\nComponent description: {doc['md_content']}\\nComponent place in hierarchy: {doc['tree_path']}\"\n            )\n        response_message = self.chat_engine.generate_idea(\"\\n\\n\".join(str_obj))\n        return response_message\n\n    def generate_doc(self):\n        \"\"\"\n        Creates documentation for the project by processing a task list in parallel threads and updating markdown files. It manages task persistence, tracks progress, and handles potential errors during generation.\n\n        Args:\n            None\n\n        Returns:\n            None\n\n\n        \"\"\"\n\n        logger.info(\"Starting to generate documentation\")\n        check_task_available_func = partial(\n            need_to_generate, ignore_list=self.setting.project.ignore_list\n        )\n        task_manager = self.meta_info.get_topology(check_task_available_func)\n        before_task_len = len(task_manager.task_dict)\n        if not self.meta_info.in_generation_process:\n            self.meta_info.in_generation_process = True\n            logger.info(\"Init a new task-list\")\n        else:\n            logger.info(\"Load from an existing task-list\")\n        self.meta_info.print_task_list(task_manager.task_dict)\n        try:\n            threads = [\n                threading.Thread(\n                    target=worker,\n                    args=(\n                        task_manager,\n                        process_id,\n                        self.generate_doc_for_a_single_item,\n                    ),\n                )\n                for process_id in range(self.setting.project.max_thread_count)\n            ]\n            for thread in threads:\n                thread.start()\n            for thread in threads:\n                thread.join()\n            self.markdown_refresh()\n            if self.setting.project.main_idea:\n                self.meta_info.document_version = (\n                    self.change_detector.repo.head.commit.hexsha\n                )\n                self.meta_info.in_generation_process = False\n                self.meta_info.checkpoint(\n                    target_dir_path=self.absolute_project_hierarchy_path\n                )\n            logger.info(\n                f\"Successfully generated {before_task_len - len(task_manager.task_dict)} documents.\"\n            )\n        except BaseException as e:\n            logger.error(\n                f\"An error occurred: {e}. {before_task_len - len(task_manager.task_dict)} docs are generated at this time\"\n            )\n\n    def get_top_n_components(self, doc_item: DocItem):\n        \"\"\"\n        Extracts and formats top-level classes from a DocItem, excluding files matching specified ignore patterns. Returns a list of markdown strings with links for each class.\n\n        Args:\n            doc_item: The DocItem object to extract components from.\n\n        Returns:\n            list: A list of markdown strings and links representing the top-level\n                components found within the DocItem, excluding those matching ignore patterns.\n\n\n        \"\"\"\n\n        components = []\n        for file in doc_item.children:\n            skip = False\n            for ignore in self.setting.project.ignore_list:\n                if ignore in file:\n                    skip = True\n                    break\n            if skip:\n                continue\n            for class_ in doc_item.children[file].children:\n                curr_obj = doc_item.children[file].children[class_]\n                components.append(self._get_md_and_links_from_doc(curr_obj))\n        return components\n\n    def _get_md_and_links_from_doc(self, doc_item: DocItem):\n        \"\"\"\n        Collects key information about a documented item, including its name, introductory content, references to and from other items, and location within the project structure.\n\n        Args:\n            doc_item: The DocItem object to extract data from.\n\n        Returns:\n            dict: A dictionary containing the extracted information, including\n                the object name, markdown content (first paragraph), referencing objects,\n                referenced objects, and the tree path as a string.\n\n\n        \"\"\"\n\n        return {\n            \"obj_name\": doc_item.obj_name,\n            \"md_content\": doc_item.md_content[-1].split(\"\\n\\n\")[0],\n            \"who_reference_me\": doc_item.who_reference_me,\n            \"reference_who\": doc_item.reference_who,\n            \"tree_path\": \"-&gt;\".join([obj.obj_name for obj in doc_item.tree_path]),\n        }\n\n    def generate_main_idea(self, docs):\n        \"\"\"\n        Extracts the core concept of a project from its documentation.\n\n\n        Args:\n            docs: The input documents used to generate the main idea.\n\n        Returns:\n            The main project idea extracted from the documents.\n\n        \"\"\"\n\n        logger.info(\"Generation of the main idea\")\n        main_project_idea = self.generate_main_project_idea(docs)\n        logger.info(f\"Successfully generated the main idea\")\n        return main_project_idea\n\n    def summarize_modules(self):\n        \"\"\"\n        No valid docstring found.\n\n        \"\"\"\n\n        logger.info(\"Modules documentation generation\")\n        res = summarize_repository(\n            self.meta_info.repo_path, self.meta_info.repo_structure, self.chat_engine\n        )\n        self.update_modules(res)\n        self.meta_info.checkpoint(target_dir_path=self.absolute_project_hierarchy_path)\n        logger.info(f\"Successfully generated module summaries\")\n        return res\n\n    def update_modules(self, module):\n        \"\"\"\n        Recursively updates the documentation for a module and its submodules by appending the module summary to the corresponding location in the documentation tree and marking it as up-to-date.\n\n        Args:\n            module: A dictionary containing information about the module,\n                including its path and summary.  It also contains a list of\n                submodules under the 'submodules' key.\n\n        Returns:\n            None\n\n\n        \"\"\"\n\n        rel_path = os.path.relpath(module[\"path\"], self.meta_info.repo_path)\n        doc_item = self.search_tree(\n            self.meta_info.target_repo_hierarchical_tree, rel_path\n        )\n        doc_item.md_content.append(module[\"module_summary\"])\n        doc_item.item_status = DocItemStatus.doc_up_to_date\n        for sm in module[\"submodules\"]:\n            self.update_modules(sm)\n\n    def search_tree(self, doc: DocItem, path: str):\n        \"\"\"\n        Recursively traverses the document tree to locate a specific path, returning the corresponding DocItem if found.\n\n        Args:\n            doc: The root DocItem of the tree to search.\n            path: The path to search for within the tree.\n\n        Returns:\n            DocItem: The DocItem at the specified path, or None if not found.\n\n        \"\"\"\n\n        if path == \".\":\n            return doc\n        else:\n            for ch_doc in doc.children:\n                if ch_doc == path:\n                    return doc.children[ch_doc]\n                else:\n                    found_res = self.search_tree(doc.children[ch_doc], path)\n                if found_res:\n                    return found_res\n\n    def convert_path_to_dot_notation(self, path: Path, class_: str):\n        \"\"\"\n        Transforms a file system path into a structured string representation suitable for identifying code elements.\n\n        Args:\n            path: The path to the file or directory.\n            class_: The name of the class within the file.\n\n        Returns:\n            str: A string representing the dot notation path, formatted as '::: &lt;dot_path&gt;.&lt;class_&gt;'.\n\n        \"\"\"\n\n        path_obj = Path(path) if isinstance(path, str) else path\n        processed_parts = []\n        for part in path_obj.parts:\n            if part.endswith(\".py\"):\n                part = part[:-3]\n            if part == \"__init__\":\n                continue\n            processed_parts.append(part)\n        dot_path = \".\".join(processed_parts)\n        return f\"::: {dot_path}.{class_}\"\n\n    def markdown_refresh(self):\n        \"\"\"\n        Rebuilds markdown files in the target repository, generating documentation from code structure and docstrings. It processes directories, files, and repositories to create up-to-date content, including retry logic for file writing.\n\n        This method rebuilds the markdown files in the target repository, ensuring they are up-to-date with the latest code structure and docstrings. It handles directories, files, and repositories differently to generate appropriate content.  It also includes retry logic for file writing operations.\n\n        Args:\n            None\n\n        Returns:\n            None\n\n        \"\"\"\n\n        with self.runner_lock:\n            markdown_folder = (\n                Path(self.setting.project.target_repo)\n                / self.setting.project.markdown_docs_name\n            )\n            if markdown_folder.exists():\n                logger.debug(f\"Deleting existing contents of {markdown_folder}\")\n                shutil.rmtree(markdown_folder)\n            markdown_folder.mkdir(parents=True, exist_ok=True)\n            logger.debug(f\"Created markdown folder at {markdown_folder}\")\n        file_item_list = self.meta_info.get_all_files(count_repo=True)\n        logger.debug(f\"Found {len(file_item_list)} files to process.\")\n        for file_item in tqdm(file_item_list):\n\n            def recursive_check(doc_item) -&gt; bool:\n                if doc_item.md_content:\n                    return True\n                for child in doc_item.children.values():\n                    if recursive_check(child):\n                        return True\n                return False\n\n            if (\n                not recursive_check(file_item)\n                and file_item.item_type == DocItemType._file\n            ):\n                logger.debug(\n                    f\"No documentation content for: {file_item.get_full_name()}, skipping.\"\n                )\n                continue\n            markdown = \"\"\n            if file_item.item_type == DocItemType._dir:\n                if file_item.md_content:\n                    markdown = file_item.md_content[-1]\n            elif file_item.item_type == DocItemType._repo:\n                markdown += SettingsManager.get_setting().project.main_idea\n            else:\n                markdown += f\"# {Path(file_item.obj_name).name.strip('.py').replace('_', ' ').title()}\\n\\n\"\n                for child in file_item.children.values():\n                    update_doc(child.source_node, child.md_content[-1])\n                    markdown += f\"## {child.obj_name}\\n{self.convert_path_to_dot_notation(Path(file_item.obj_name), child.obj_name)}\\n\\n\"\n                    for n_child in child.children.values():\n                        update_doc(n_child.source_node, n_child.md_content[-1])\n                children_names = list(file_item.children.keys())\n                if children_names:\n                    with open(\n                        Path(self.setting.project.target_repo, file_item.obj_name),\n                        \"w+\",\n                        encoding=\"utf-8\",\n                    ) as f:\n                        value = ast.unparse(\n                            file_item.children[children_names[0]].source_node.parent\n                        )\n                        f.write(value)\n            if not markdown:\n                logger.warning(\n                    f\"No markdown content generated for: {file_item.get_full_name()}\"\n                )\n                continue\n            if file_item.item_type == DocItemType._dir:\n                file_path = (\n                    Path(self.setting.project.markdown_docs_name)\n                    / Path(file_item.obj_name)\n                    / \"index.md\"\n                )\n            elif file_item.item_type == DocItemType._repo:\n                file_path = Path(self.setting.project.markdown_docs_name) / \"index.md\"\n            else:\n                file_path = Path(\n                    self.setting.project.markdown_docs_name\n                ) / file_item.get_file_name().replace(\".py\", \".md\")\n            abs_file_path = self.setting.project.target_repo / file_path\n            logger.debug(f\"Writing markdown to: {abs_file_path}\")\n            abs_file_path.parent.mkdir(parents=True, exist_ok=True)\n            logger.debug(f\"Ensured directory exists: {abs_file_path.parent}\")\n            with self.runner_lock:\n                for attempt in range(3):\n                    try:\n                        with open(abs_file_path, \"w\", encoding=\"utf-8\") as file:\n                            file.write(markdown)\n                        logger.debug(f\"Successfully wrote to {abs_file_path}\")\n                        break\n                    except IOError as e:\n                        logger.error(\n                            f\"Failed to write {abs_file_path} on attempt {attempt + 1}: {e}\"\n                        )\n                        time.sleep(1)\n        logger.info(\n            f\"Markdown documents have been refreshed at {self.setting.project.markdown_docs_name}\"\n        )\n\n    def git_commit(self, commit_message):\n        \"\"\"\n        Records changes to the repository with a descriptive message.\n\n        Args:\n            commit_message: The message for the commit.\n\n        Returns:\n            None\n\n\n        \"\"\"\n\n        try:\n            subprocess.check_call(\n                [\"git\", \"commit\", \"--no-verify\", \"-m\", commit_message], shell=True\n            )\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while trying to commit {str(e)}\")\n\n    def run(self):\n        \"\"\"\n        No valid docstring found.\n\n        \"\"\"\n\n        if self.meta_info.document_version == \"\":\n            settings = SettingsManager.get_setting()\n            if settings.project.main_idea:\n                self.generate_doc()\n                self.summarize_modules()\n                self.markdown_refresh()\n            else:\n                self.generate_doc()\n                settings.project.main_idea = self.generate_main_idea(\n                    self.get_top_n_components(\n                        self.meta_info.target_repo_hierarchical_tree\n                    )\n                )\n                self.generate_doc()\n                self.summarize_modules()\n                self.markdown_refresh()\n            self.meta_info.checkpoint(\n                target_dir_path=self.absolute_project_hierarchy_path,\n                flash_reference_relation=True,\n            )\n            return\n        if not self.meta_info.in_generation_process:\n            logger.info(\"Starting to detect changes.\")\n            \"\u91c7\u7528\u65b0\u7684\u529e\u6cd5\\n            1.\u65b0\u5efa\u4e00\u4e2aproject-hierachy\\n            2.\u548c\u8001\u7684hierarchy\u505amerge,\u5904\u7406\u4ee5\u4e0b\u60c5\u51b5\uff1a\\n            - \u521b\u5efa\u4e00\u4e2a\u65b0\u6587\u4ef6\uff1a\u9700\u8981\u751f\u6210\u5bf9\u5e94\u7684doc\\n            - \u6587\u4ef6\u3001\u5bf9\u8c61\u88ab\u5220\u9664\uff1a\u5bf9\u5e94\u7684doc\u4e5f\u5220\u9664(\u6309\u7167\u76ee\u524d\u7684\u5b9e\u73b0\uff0c\u6587\u4ef6\u91cd\u547d\u540d\u7b97\u662f\u5220\u9664\u518d\u6dfb\u52a0)\\n            - \u5f15\u7528\u5173\u7cfb\u53d8\u4e86\uff1a\u5bf9\u5e94\u7684obj-doc\u9700\u8981\u91cd\u65b0\u751f\u6210\\n            \\n            merge\u540e\u7684new_meta_info\u4e2d\uff1a\\n            1.\u65b0\u5efa\u7684\u6587\u4ef6\u6ca1\u6709\u6587\u6863\uff0c\u56e0\u6b64metainfo merge\u540e\u8fd8\u662f\u6ca1\u6709\u6587\u6863\\n            2.\u88ab\u5220\u9664\u7684\u6587\u4ef6\u548cobj\uff0c\u672c\u6765\u5c31\u4e0d\u5728\u65b0\u7684meta\u91cc\u9762\uff0c\u76f8\u5f53\u4e8e\u6587\u6863\u88ab\u81ea\u52a8\u5220\u9664\u4e86\\n            3.\u53ea\u9700\u8981\u89c2\u5bdf\u88ab\u4fee\u6539\u7684\u6587\u4ef6\uff0c\u4ee5\u53ca\u5f15\u7528\u5173\u7cfb\u9700\u8981\u88ab\u901a\u77e5\u7684\u6587\u4ef6\u53bb\u91cd\u65b0\u751f\u6210\u6587\u6863\"\n            file_path_reflections, jump_files = make_fake_files()\n            new_meta_info = MetaInfo.init_meta_info(file_path_reflections, jump_files)\n            new_meta_info.load_doc_from_older_meta(self.meta_info)\n            self.meta_info = new_meta_info\n            self.meta_info.in_generation_process = True\n        check_task_available_func = partial(\n            need_to_generate, ignore_list=self.setting.project.ignore_list\n        )\n        task_manager = self.meta_info.get_task_manager(\n            self.meta_info.target_repo_hierarchical_tree,\n            task_available_func=check_task_available_func,\n        )\n        for item_name, item_type in self.meta_info.deleted_items_from_older_meta:\n            print(\n                f\"{Fore.LIGHTMAGENTA_EX}[Dir/File/Obj Delete Dected]: {Style.RESET_ALL} {item_type} {item_name}\"\n            )\n        self.meta_info.print_task_list(task_manager.task_dict)\n        if task_manager.all_success:\n            logger.info(\n                \"No tasks in the queue, all documents are completed and up to date.\"\n            )\n        threads = [\n            threading.Thread(\n                target=worker,\n                args=(task_manager, process_id, self.generate_doc_for_a_single_item),\n            )\n            for process_id in range(self.setting.project.max_thread_count)\n        ]\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n        self.meta_info.in_generation_process = False\n        self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n        self.meta_info.checkpoint(\n            target_dir_path=self.absolute_project_hierarchy_path,\n            flash_reference_relation=True,\n        )\n        logger.info(f\"Doc has been forwarded to the latest version\")\n        self.markdown_refresh()\n        delete_fake_files()\n        logger.info(f\"Starting to git-add DocMetaInfo and newly generated Docs\")\n        time.sleep(1)\n        git_add_result = self.change_detector.add_unstaged_files()\n        if len(git_add_result) &gt; 0:\n            logger.info(\n                f\"Added {[file for file in git_add_result]} to the staging area.\"\n            )\n</code></pre>"},{"location":"repo_agent/runner/#repo_agent.runner.Runner.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the processing pipeline by loading project settings, preparing the repository, and setting up tools for analysis and interaction. It handles initial metadata creation or loading from existing checkpoints, ensuring a consistent view of the project's structure and content.</p> <p>This constructor sets up the necessary components for processing project documentation, including loading settings, copying the mkdocs configuration file, initializing project and change detectors, setting up a chat engine, and creating or loading meta-information about the project.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/runner.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the processing pipeline by loading project settings, preparing the repository, and setting up tools for analysis and interaction. It handles initial metadata creation or loading from existing checkpoints, ensuring a consistent view of the project's structure and content.\n\n    This constructor sets up the necessary components for processing project documentation,\n    including loading settings, copying the mkdocs configuration file, initializing project and\n    change detectors, setting up a chat engine, and creating or loading meta-information about the project.\n\n    Args:\n        None\n\n    Returns:\n        None\n\n    \"\"\"\n\n    self.setting = SettingsManager.get_setting()\n    self.absolute_project_hierarchy_path = (\n        self.setting.project.target_repo / self.setting.project.hierarchy_name\n    )\n    shutil.copy(\"mkdocs.yml\", Path(self.setting.project.target_repo, \"mkdocs.yml\"))\n    self.project_manager = ProjectManager(\n        repo_path=self.setting.project.target_repo,\n        project_hierarchy=self.setting.project.hierarchy_name,\n    )\n    self.change_detector = ChangeDetector(\n        repo_path=self.setting.project.target_repo\n    )\n    self.chat_engine = ChatEngine(project_manager=self.project_manager)\n    file_path_reflections, jump_files = make_fake_files()\n    setting = SettingsManager.get_setting()\n    if not self.absolute_project_hierarchy_path.exists():\n        self.meta_info = MetaInfo.init_meta_info(file_path_reflections, jump_files)\n        self.meta_info.checkpoint(\n            target_dir_path=self.absolute_project_hierarchy_path\n        )\n\n    else:\n        project_abs_path = setting.project.target_repo\n        file_handler = FileHandler(project_abs_path, None)\n        repo_structure = file_handler.generate_overall_structure(\n            file_path_reflections, jump_files\n        )\n        self.meta_info = MetaInfo.from_checkpoint_path(\n            self.absolute_project_hierarchy_path, repo_structure\n        )\n        SettingsManager.get_setting().project.main_idea = self.meta_info.main_idea\n    self.runner_lock = threading.Lock()\n</code></pre>"},{"location":"repo_agent/runner/#repo_agent.runner.Runner.convert_path_to_dot_notation","title":"<code>convert_path_to_dot_notation(path, class_)</code>","text":"<p>Transforms a file system path into a structured string representation suitable for identifying code elements.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file or directory.</p> required <code>class_</code> <code>str</code> <p>The name of the class within the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A string representing the dot notation path, formatted as '::: .'. Source code in <code>repo_agent/runner.py</code> <pre><code>def convert_path_to_dot_notation(self, path: Path, class_: str):\n    \"\"\"\n    Transforms a file system path into a structured string representation suitable for identifying code elements.\n\n    Args:\n        path: The path to the file or directory.\n        class_: The name of the class within the file.\n\n    Returns:\n        str: A string representing the dot notation path, formatted as '::: &lt;dot_path&gt;.&lt;class_&gt;'.\n\n    \"\"\"\n\n    path_obj = Path(path) if isinstance(path, str) else path\n    processed_parts = []\n    for part in path_obj.parts:\n        if part.endswith(\".py\"):\n            part = part[:-3]\n        if part == \"__init__\":\n            continue\n        processed_parts.append(part)\n    dot_path = \".\".join(processed_parts)\n    return f\"::: {dot_path}.{class_}\"\n</code></pre>"},{"location":"repo_agent/runner/#repo_agent.runner.Runner.generate_doc","title":"<code>generate_doc()</code>","text":"<p>Creates documentation for the project by processing a task list in parallel threads and updating markdown files. It manages task persistence, tracks progress, and handles potential errors during generation.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/runner.py</code> <pre><code>def generate_doc(self):\n    \"\"\"\n    Creates documentation for the project by processing a task list in parallel threads and updating markdown files. It manages task persistence, tracks progress, and handles potential errors during generation.\n\n    Args:\n        None\n\n    Returns:\n        None\n\n\n    \"\"\"\n\n    logger.info(\"Starting to generate documentation\")\n    check_task_available_func = partial(\n        need_to_generate, ignore_list=self.setting.project.ignore_list\n    )\n    task_manager = self.meta_info.get_topology(check_task_available_func)\n    before_task_len = len(task_manager.task_dict)\n    if not self.meta_info.in_generation_process:\n        self.meta_info.in_generation_process = True\n        logger.info(\"Init a new task-list\")\n    else:\n        logger.info(\"Load from an existing task-list\")\n    self.meta_info.print_task_list(task_manager.task_dict)\n    try:\n        threads = [\n            threading.Thread(\n                target=worker,\n                args=(\n                    task_manager,\n                    process_id,\n                    self.generate_doc_for_a_single_item,\n                ),\n            )\n            for process_id in range(self.setting.project.max_thread_count)\n        ]\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n        self.markdown_refresh()\n        if self.setting.project.main_idea:\n            self.meta_info.document_version = (\n                self.change_detector.repo.head.commit.hexsha\n            )\n            self.meta_info.in_generation_process = False\n            self.meta_info.checkpoint(\n                target_dir_path=self.absolute_project_hierarchy_path\n            )\n        logger.info(\n            f\"Successfully generated {before_task_len - len(task_manager.task_dict)} documents.\"\n        )\n    except BaseException as e:\n        logger.error(\n            f\"An error occurred: {e}. {before_task_len - len(task_manager.task_dict)} docs are generated at this time\"\n        )\n</code></pre>"},{"location":"repo_agent/runner/#repo_agent.runner.Runner.generate_doc_for_a_single_item","title":"<code>generate_doc_for_a_single_item(doc_item)</code>","text":"<p>Processes a single documentation item, generating content with a chat engine and updating its status. It respects ignore lists to avoid unnecessary generation and handles potential errors gracefully by logging them and ensuring an empty entry in the output. A checkpoint save occurs after successful or failed processing.</p> <p>This method checks if the DocItem needs to be generated based on ignore lists, generates the documentation using a chat engine, appends it to the DocItem's content, updates the item status and performs a checkpoint save. It handles potential exceptions during document generation by logging them and appending an empty string to the content.</p> <p>Parameters:</p> Name Type Description Default <code>doc_item</code> <code>DocItem</code> <p>The DocItem for which documentation needs to be generated.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/runner.py</code> <pre><code>def generate_doc_for_a_single_item(self, doc_item: DocItem):\n    \"\"\"\n    Processes a single documentation item, generating content with a chat engine and updating its status. It respects ignore lists to avoid unnecessary generation and handles potential errors gracefully by logging them and ensuring an empty entry in the output. A checkpoint save occurs after successful or failed processing.\n\n    This method checks if the DocItem needs to be generated based on ignore lists,\n    generates the documentation using a chat engine, appends it to the DocItem's content,\n    updates the item status and performs a checkpoint save. It handles potential exceptions\n    during document generation by logging them and appending an empty string to the content.\n\n    Args:\n        doc_item: The DocItem for which documentation needs to be generated.\n\n    Returns:\n        None\n\n\n    \"\"\"\n\n    settings = SettingsManager.get_setting()\n    try:\n        if not need_to_generate(doc_item, self.setting.project.ignore_list):\n            print(\n                f\"Content ignored/Document generated, skipping: {doc_item.get_full_name()}\"\n            )\n        else:\n            print(\n                f\" -- Generating document  {Fore.LIGHTYELLOW_EX}{doc_item.item_type.name}: {doc_item.get_full_name()}{Style.RESET_ALL}\"\n            )\n            response_message = self.chat_engine.generate_doc(doc_item=doc_item)\n            doc_item.md_content.append(response_message)\n            if settings.project.main_idea:\n                doc_item.item_status = DocItemStatus.doc_up_to_date\n            self.meta_info.checkpoint(\n                target_dir_path=self.absolute_project_hierarchy_path\n            )\n    except Exception:\n        logger.exception(\n            f\"Document generation failed after multiple attempts, skipping: {doc_item.get_full_name()}\"\n        )\n        doc_item.md_content.append(\"\")\n        if settings.project.main_idea:\n            doc_item.item_status = DocItemStatus.doc_up_to_date\n</code></pre>"},{"location":"repo_agent/runner/#repo_agent.runner.Runner.generate_main_idea","title":"<code>generate_main_idea(docs)</code>","text":"<p>Extracts the core concept of a project from its documentation.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <p>The input documents used to generate the main idea.</p> required <p>Returns:</p> Type Description <p>The main project idea extracted from the documents.</p> Source code in <code>repo_agent/runner.py</code> <pre><code>def generate_main_idea(self, docs):\n    \"\"\"\n    Extracts the core concept of a project from its documentation.\n\n\n    Args:\n        docs: The input documents used to generate the main idea.\n\n    Returns:\n        The main project idea extracted from the documents.\n\n    \"\"\"\n\n    logger.info(\"Generation of the main idea\")\n    main_project_idea = self.generate_main_project_idea(docs)\n    logger.info(f\"Successfully generated the main idea\")\n    return main_project_idea\n</code></pre>"},{"location":"repo_agent/runner/#repo_agent.runner.Runner.generate_main_project_idea","title":"<code>generate_main_project_idea(docs)</code>","text":"<p>Synthesizes a cohesive project concept by analyzing the functionality and relationships between software components.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Dict]</code> <p>A list of dictionaries, each detailing a component's name, description, and position within the system architecture.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A concise summary outlining the overall project direction derived from the component details.</p> <p>Args:     docs: A list of dictionaries, where each dictionary represents a         component and contains its name, description, and hierarchical path.</p> <p>Returns:     str: The generated project idea as a string.</p> Source code in <code>repo_agent/runner.py</code> <pre><code>def generate_main_project_idea(self, docs: List[Dict]):\n    \"\"\"\n    Synthesizes a cohesive project concept by analyzing the functionality and relationships between software components.\n\n    Args:\n        docs: A list of dictionaries, each detailing a component's name, description, and position within the system architecture.\n\n    Returns:\n        str: A concise summary outlining the overall project direction derived from the component details.\n\n\n            Args:\n                docs: A list of dictionaries, where each dictionary represents a\n                    component and contains its name, description, and hierarchical path.\n\n            Returns:\n                str: The generated project idea as a string.\n\n\n    \"\"\"\n\n    str_obj = []\n    for doc in docs:\n        str_obj.append(\n            f\"Component name: {doc['obj_name']}\\nComponent description: {doc['md_content']}\\nComponent place in hierarchy: {doc['tree_path']}\"\n        )\n    response_message = self.chat_engine.generate_idea(\"\\n\\n\".join(str_obj))\n    return response_message\n</code></pre>"},{"location":"repo_agent/runner/#repo_agent.runner.Runner.get_all_pys","title":"<code>get_all_pys(directory)</code>","text":"<p>No valid docstring found.</p> Source code in <code>repo_agent/runner.py</code> <pre><code>def get_all_pys(self, directory):\n    \"\"\"\n    No valid docstring found.\n\n    \"\"\"\n\n    python_files = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".py\"):\n                python_files.append(os.path.join(root, file))\n    return python_files\n</code></pre>"},{"location":"repo_agent/runner/#repo_agent.runner.Runner.get_top_n_components","title":"<code>get_top_n_components(doc_item)</code>","text":"<p>Extracts and formats top-level classes from a DocItem, excluding files matching specified ignore patterns. Returns a list of markdown strings with links for each class.</p> <p>Parameters:</p> Name Type Description Default <code>doc_item</code> <code>DocItem</code> <p>The DocItem object to extract components from.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of markdown strings and links representing the top-level components found within the DocItem, excluding those matching ignore patterns.</p> Source code in <code>repo_agent/runner.py</code> <pre><code>def get_top_n_components(self, doc_item: DocItem):\n    \"\"\"\n    Extracts and formats top-level classes from a DocItem, excluding files matching specified ignore patterns. Returns a list of markdown strings with links for each class.\n\n    Args:\n        doc_item: The DocItem object to extract components from.\n\n    Returns:\n        list: A list of markdown strings and links representing the top-level\n            components found within the DocItem, excluding those matching ignore patterns.\n\n\n    \"\"\"\n\n    components = []\n    for file in doc_item.children:\n        skip = False\n        for ignore in self.setting.project.ignore_list:\n            if ignore in file:\n                skip = True\n                break\n        if skip:\n            continue\n        for class_ in doc_item.children[file].children:\n            curr_obj = doc_item.children[file].children[class_]\n            components.append(self._get_md_and_links_from_doc(curr_obj))\n    return components\n</code></pre>"},{"location":"repo_agent/runner/#repo_agent.runner.Runner.git_commit","title":"<code>git_commit(commit_message)</code>","text":"<p>Records changes to the repository with a descriptive message.</p> <p>Parameters:</p> Name Type Description Default <code>commit_message</code> <p>The message for the commit.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/runner.py</code> <pre><code>def git_commit(self, commit_message):\n    \"\"\"\n    Records changes to the repository with a descriptive message.\n\n    Args:\n        commit_message: The message for the commit.\n\n    Returns:\n        None\n\n\n    \"\"\"\n\n    try:\n        subprocess.check_call(\n            [\"git\", \"commit\", \"--no-verify\", \"-m\", commit_message], shell=True\n        )\n    except subprocess.CalledProcessError as e:\n        print(f\"An error occurred while trying to commit {str(e)}\")\n</code></pre>"},{"location":"repo_agent/runner/#repo_agent.runner.Runner.markdown_refresh","title":"<code>markdown_refresh()</code>","text":"<p>Rebuilds markdown files in the target repository, generating documentation from code structure and docstrings. It processes directories, files, and repositories to create up-to-date content, including retry logic for file writing.</p> <p>This method rebuilds the markdown files in the target repository, ensuring they are up-to-date with the latest code structure and docstrings. It handles directories, files, and repositories differently to generate appropriate content.  It also includes retry logic for file writing operations.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/runner.py</code> <pre><code>def markdown_refresh(self):\n    \"\"\"\n    Rebuilds markdown files in the target repository, generating documentation from code structure and docstrings. It processes directories, files, and repositories to create up-to-date content, including retry logic for file writing.\n\n    This method rebuilds the markdown files in the target repository, ensuring they are up-to-date with the latest code structure and docstrings. It handles directories, files, and repositories differently to generate appropriate content.  It also includes retry logic for file writing operations.\n\n    Args:\n        None\n\n    Returns:\n        None\n\n    \"\"\"\n\n    with self.runner_lock:\n        markdown_folder = (\n            Path(self.setting.project.target_repo)\n            / self.setting.project.markdown_docs_name\n        )\n        if markdown_folder.exists():\n            logger.debug(f\"Deleting existing contents of {markdown_folder}\")\n            shutil.rmtree(markdown_folder)\n        markdown_folder.mkdir(parents=True, exist_ok=True)\n        logger.debug(f\"Created markdown folder at {markdown_folder}\")\n    file_item_list = self.meta_info.get_all_files(count_repo=True)\n    logger.debug(f\"Found {len(file_item_list)} files to process.\")\n    for file_item in tqdm(file_item_list):\n\n        def recursive_check(doc_item) -&gt; bool:\n            if doc_item.md_content:\n                return True\n            for child in doc_item.children.values():\n                if recursive_check(child):\n                    return True\n            return False\n\n        if (\n            not recursive_check(file_item)\n            and file_item.item_type == DocItemType._file\n        ):\n            logger.debug(\n                f\"No documentation content for: {file_item.get_full_name()}, skipping.\"\n            )\n            continue\n        markdown = \"\"\n        if file_item.item_type == DocItemType._dir:\n            if file_item.md_content:\n                markdown = file_item.md_content[-1]\n        elif file_item.item_type == DocItemType._repo:\n            markdown += SettingsManager.get_setting().project.main_idea\n        else:\n            markdown += f\"# {Path(file_item.obj_name).name.strip('.py').replace('_', ' ').title()}\\n\\n\"\n            for child in file_item.children.values():\n                update_doc(child.source_node, child.md_content[-1])\n                markdown += f\"## {child.obj_name}\\n{self.convert_path_to_dot_notation(Path(file_item.obj_name), child.obj_name)}\\n\\n\"\n                for n_child in child.children.values():\n                    update_doc(n_child.source_node, n_child.md_content[-1])\n            children_names = list(file_item.children.keys())\n            if children_names:\n                with open(\n                    Path(self.setting.project.target_repo, file_item.obj_name),\n                    \"w+\",\n                    encoding=\"utf-8\",\n                ) as f:\n                    value = ast.unparse(\n                        file_item.children[children_names[0]].source_node.parent\n                    )\n                    f.write(value)\n        if not markdown:\n            logger.warning(\n                f\"No markdown content generated for: {file_item.get_full_name()}\"\n            )\n            continue\n        if file_item.item_type == DocItemType._dir:\n            file_path = (\n                Path(self.setting.project.markdown_docs_name)\n                / Path(file_item.obj_name)\n                / \"index.md\"\n            )\n        elif file_item.item_type == DocItemType._repo:\n            file_path = Path(self.setting.project.markdown_docs_name) / \"index.md\"\n        else:\n            file_path = Path(\n                self.setting.project.markdown_docs_name\n            ) / file_item.get_file_name().replace(\".py\", \".md\")\n        abs_file_path = self.setting.project.target_repo / file_path\n        logger.debug(f\"Writing markdown to: {abs_file_path}\")\n        abs_file_path.parent.mkdir(parents=True, exist_ok=True)\n        logger.debug(f\"Ensured directory exists: {abs_file_path.parent}\")\n        with self.runner_lock:\n            for attempt in range(3):\n                try:\n                    with open(abs_file_path, \"w\", encoding=\"utf-8\") as file:\n                        file.write(markdown)\n                    logger.debug(f\"Successfully wrote to {abs_file_path}\")\n                    break\n                except IOError as e:\n                    logger.error(\n                        f\"Failed to write {abs_file_path} on attempt {attempt + 1}: {e}\"\n                    )\n                    time.sleep(1)\n    logger.info(\n        f\"Markdown documents have been refreshed at {self.setting.project.markdown_docs_name}\"\n    )\n</code></pre>"},{"location":"repo_agent/runner/#repo_agent.runner.Runner.run","title":"<code>run()</code>","text":"<p>No valid docstring found.</p> Source code in <code>repo_agent/runner.py</code> <pre><code>def run(self):\n    \"\"\"\n    No valid docstring found.\n\n    \"\"\"\n\n    if self.meta_info.document_version == \"\":\n        settings = SettingsManager.get_setting()\n        if settings.project.main_idea:\n            self.generate_doc()\n            self.summarize_modules()\n            self.markdown_refresh()\n        else:\n            self.generate_doc()\n            settings.project.main_idea = self.generate_main_idea(\n                self.get_top_n_components(\n                    self.meta_info.target_repo_hierarchical_tree\n                )\n            )\n            self.generate_doc()\n            self.summarize_modules()\n            self.markdown_refresh()\n        self.meta_info.checkpoint(\n            target_dir_path=self.absolute_project_hierarchy_path,\n            flash_reference_relation=True,\n        )\n        return\n    if not self.meta_info.in_generation_process:\n        logger.info(\"Starting to detect changes.\")\n        \"\u91c7\u7528\u65b0\u7684\u529e\u6cd5\\n            1.\u65b0\u5efa\u4e00\u4e2aproject-hierachy\\n            2.\u548c\u8001\u7684hierarchy\u505amerge,\u5904\u7406\u4ee5\u4e0b\u60c5\u51b5\uff1a\\n            - \u521b\u5efa\u4e00\u4e2a\u65b0\u6587\u4ef6\uff1a\u9700\u8981\u751f\u6210\u5bf9\u5e94\u7684doc\\n            - \u6587\u4ef6\u3001\u5bf9\u8c61\u88ab\u5220\u9664\uff1a\u5bf9\u5e94\u7684doc\u4e5f\u5220\u9664(\u6309\u7167\u76ee\u524d\u7684\u5b9e\u73b0\uff0c\u6587\u4ef6\u91cd\u547d\u540d\u7b97\u662f\u5220\u9664\u518d\u6dfb\u52a0)\\n            - \u5f15\u7528\u5173\u7cfb\u53d8\u4e86\uff1a\u5bf9\u5e94\u7684obj-doc\u9700\u8981\u91cd\u65b0\u751f\u6210\\n            \\n            merge\u540e\u7684new_meta_info\u4e2d\uff1a\\n            1.\u65b0\u5efa\u7684\u6587\u4ef6\u6ca1\u6709\u6587\u6863\uff0c\u56e0\u6b64metainfo merge\u540e\u8fd8\u662f\u6ca1\u6709\u6587\u6863\\n            2.\u88ab\u5220\u9664\u7684\u6587\u4ef6\u548cobj\uff0c\u672c\u6765\u5c31\u4e0d\u5728\u65b0\u7684meta\u91cc\u9762\uff0c\u76f8\u5f53\u4e8e\u6587\u6863\u88ab\u81ea\u52a8\u5220\u9664\u4e86\\n            3.\u53ea\u9700\u8981\u89c2\u5bdf\u88ab\u4fee\u6539\u7684\u6587\u4ef6\uff0c\u4ee5\u53ca\u5f15\u7528\u5173\u7cfb\u9700\u8981\u88ab\u901a\u77e5\u7684\u6587\u4ef6\u53bb\u91cd\u65b0\u751f\u6210\u6587\u6863\"\n        file_path_reflections, jump_files = make_fake_files()\n        new_meta_info = MetaInfo.init_meta_info(file_path_reflections, jump_files)\n        new_meta_info.load_doc_from_older_meta(self.meta_info)\n        self.meta_info = new_meta_info\n        self.meta_info.in_generation_process = True\n    check_task_available_func = partial(\n        need_to_generate, ignore_list=self.setting.project.ignore_list\n    )\n    task_manager = self.meta_info.get_task_manager(\n        self.meta_info.target_repo_hierarchical_tree,\n        task_available_func=check_task_available_func,\n    )\n    for item_name, item_type in self.meta_info.deleted_items_from_older_meta:\n        print(\n            f\"{Fore.LIGHTMAGENTA_EX}[Dir/File/Obj Delete Dected]: {Style.RESET_ALL} {item_type} {item_name}\"\n        )\n    self.meta_info.print_task_list(task_manager.task_dict)\n    if task_manager.all_success:\n        logger.info(\n            \"No tasks in the queue, all documents are completed and up to date.\"\n        )\n    threads = [\n        threading.Thread(\n            target=worker,\n            args=(task_manager, process_id, self.generate_doc_for_a_single_item),\n        )\n        for process_id in range(self.setting.project.max_thread_count)\n    ]\n    for thread in threads:\n        thread.start()\n    for thread in threads:\n        thread.join()\n    self.meta_info.in_generation_process = False\n    self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha\n    self.meta_info.checkpoint(\n        target_dir_path=self.absolute_project_hierarchy_path,\n        flash_reference_relation=True,\n    )\n    logger.info(f\"Doc has been forwarded to the latest version\")\n    self.markdown_refresh()\n    delete_fake_files()\n    logger.info(f\"Starting to git-add DocMetaInfo and newly generated Docs\")\n    time.sleep(1)\n    git_add_result = self.change_detector.add_unstaged_files()\n    if len(git_add_result) &gt; 0:\n        logger.info(\n            f\"Added {[file for file in git_add_result]} to the staging area.\"\n        )\n</code></pre>"},{"location":"repo_agent/runner/#repo_agent.runner.Runner.search_tree","title":"<code>search_tree(doc, path)</code>","text":"<p>Recursively traverses the document tree to locate a specific path, returning the corresponding DocItem if found.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>DocItem</code> <p>The root DocItem of the tree to search.</p> required <code>path</code> <code>str</code> <p>The path to search for within the tree.</p> required <p>Returns:</p> Name Type Description <code>DocItem</code> <p>The DocItem at the specified path, or None if not found.</p> Source code in <code>repo_agent/runner.py</code> <pre><code>def search_tree(self, doc: DocItem, path: str):\n    \"\"\"\n    Recursively traverses the document tree to locate a specific path, returning the corresponding DocItem if found.\n\n    Args:\n        doc: The root DocItem of the tree to search.\n        path: The path to search for within the tree.\n\n    Returns:\n        DocItem: The DocItem at the specified path, or None if not found.\n\n    \"\"\"\n\n    if path == \".\":\n        return doc\n    else:\n        for ch_doc in doc.children:\n            if ch_doc == path:\n                return doc.children[ch_doc]\n            else:\n                found_res = self.search_tree(doc.children[ch_doc], path)\n            if found_res:\n                return found_res\n</code></pre>"},{"location":"repo_agent/runner/#repo_agent.runner.Runner.summarize_modules","title":"<code>summarize_modules()</code>","text":"<p>No valid docstring found.</p> Source code in <code>repo_agent/runner.py</code> <pre><code>def summarize_modules(self):\n    \"\"\"\n    No valid docstring found.\n\n    \"\"\"\n\n    logger.info(\"Modules documentation generation\")\n    res = summarize_repository(\n        self.meta_info.repo_path, self.meta_info.repo_structure, self.chat_engine\n    )\n    self.update_modules(res)\n    self.meta_info.checkpoint(target_dir_path=self.absolute_project_hierarchy_path)\n    logger.info(f\"Successfully generated module summaries\")\n    return res\n</code></pre>"},{"location":"repo_agent/runner/#repo_agent.runner.Runner.update_modules","title":"<code>update_modules(module)</code>","text":"<p>Recursively updates the documentation for a module and its submodules by appending the module summary to the corresponding location in the documentation tree and marking it as up-to-date.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <p>A dictionary containing information about the module, including its path and summary.  It also contains a list of submodules under the 'submodules' key.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/runner.py</code> <pre><code>def update_modules(self, module):\n    \"\"\"\n    Recursively updates the documentation for a module and its submodules by appending the module summary to the corresponding location in the documentation tree and marking it as up-to-date.\n\n    Args:\n        module: A dictionary containing information about the module,\n            including its path and summary.  It also contains a list of\n            submodules under the 'submodules' key.\n\n    Returns:\n        None\n\n\n    \"\"\"\n\n    rel_path = os.path.relpath(module[\"path\"], self.meta_info.repo_path)\n    doc_item = self.search_tree(\n        self.meta_info.target_repo_hierarchical_tree, rel_path\n    )\n    doc_item.md_content.append(module[\"module_summary\"])\n    doc_item.item_status = DocItemStatus.doc_up_to_date\n    for sm in module[\"submodules\"]:\n        self.update_modules(sm)\n</code></pre>"},{"location":"repo_agent/settings/","title":"Settings","text":""},{"location":"repo_agent/settings/#repo_agent.settings.ChatCompletionSettings","title":"<code>ChatCompletionSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Represents settings for a chat completion request.</p> <p>This class encapsulates the configuration parameters needed to interact with a chat completion service, such as OpenAI's Chat Completions API. It provides attributes for specifying the model, temperature, timeout, base URL, and API key.</p> Source code in <code>repo_agent/settings.py</code> <pre><code>class ChatCompletionSettings(BaseSettings):\n    \"\"\"\n    Represents settings for a chat completion request.\n\n    This class encapsulates the configuration parameters needed to interact with\n    a chat completion service, such as OpenAI's Chat Completions API. It provides\n    attributes for specifying the model, temperature, timeout, base URL, and API key.\n    \"\"\"\n\n    model: str = \"gpt-4o-mini\"\n    temperature: PositiveFloat = 0.2\n    request_timeout: PositiveInt = 180\n    openai_base_url: str = \"https://api.openai.com/v1\"\n    openai_api_key: SecretStr = Field(..., exclude=True)\n\n    @field_validator(\"openai_base_url\", mode=\"before\")\n    @classmethod\n    def convert_base_url_to_str(cls, openai_base_url: HttpUrl) -&gt; str:\n        \"\"\"\n        Ensures the OpenAI base URL is handled as a string.\n\n        Args:\n            openai_base_url: The base URL for OpenAI.\n\n        Returns:\n            str: The string representation of the base URL.\n\n        \"\"\"\n\n        return str(openai_base_url)\n</code></pre>"},{"location":"repo_agent/settings/#repo_agent.settings.ChatCompletionSettings.convert_base_url_to_str","title":"<code>convert_base_url_to_str(openai_base_url)</code>  <code>classmethod</code>","text":"<p>Ensures the OpenAI base URL is handled as a string.</p> <p>Parameters:</p> Name Type Description Default <code>openai_base_url</code> <code>HttpUrl</code> <p>The base URL for OpenAI.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The string representation of the base URL.</p> Source code in <code>repo_agent/settings.py</code> <pre><code>@field_validator(\"openai_base_url\", mode=\"before\")\n@classmethod\ndef convert_base_url_to_str(cls, openai_base_url: HttpUrl) -&gt; str:\n    \"\"\"\n    Ensures the OpenAI base URL is handled as a string.\n\n    Args:\n        openai_base_url: The base URL for OpenAI.\n\n    Returns:\n        str: The string representation of the base URL.\n\n    \"\"\"\n\n    return str(openai_base_url)\n</code></pre>"},{"location":"repo_agent/settings/#repo_agent.settings.LogLevel","title":"<code>LogLevel</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Represents different log levels for categorizing log messages.</p> <p>This class provides a set of constants representing standard log levels, allowing for easy and consistent categorization of log output.</p> Source code in <code>repo_agent/settings.py</code> <pre><code>class LogLevel(StrEnum):\n    \"\"\"\n    Represents different log levels for categorizing log messages.\n\n    This class provides a set of constants representing standard log levels,\n    allowing for easy and consistent categorization of log output.\n    \"\"\"\n\n    DEBUG = \"DEBUG\"\n    INFO = \"INFO\"\n    WARNING = \"WARNING\"\n    ERROR = \"ERROR\"\n    CRITICAL = \"CRITICAL\"\n</code></pre>"},{"location":"repo_agent/settings/#repo_agent.settings.ProjectSettings","title":"<code>ProjectSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Project settings class to manage configuration for repository analysis.</p> <p>This class encapsulates all configurable parameters related to the project, such as repository details, documentation generation preferences, and logging options.</p> Source code in <code>repo_agent/settings.py</code> <pre><code>class ProjectSettings(BaseSettings):\n    \"\"\"\n    Project settings class to manage configuration for repository analysis.\n\n    This class encapsulates all configurable parameters related to the project,\n    such as repository details, documentation generation preferences, and logging options.\n    \"\"\"\n\n    target_repo: DirectoryPath = \"\"\n    hierarchy_name: str = \".project_doc_record\"\n    markdown_docs_name: str = \"markdown_docs\"\n    ignore_list: list[str] = []\n    language: str = \"English\"\n    max_thread_count: PositiveInt = 4\n    log_level: LogLevel = LogLevel.INFO\n    main_idea: Optional[str] = None\n    parse_references: bool = True\n\n    @field_validator(\"language\")\n    @classmethod\n    def validate_language_code(cls, v: str) -&gt; str:\n        \"\"\"\n        Ensures the provided input represents a recognized language, returning its standardized name.\n\n        Args:\n            v: The language code or name to validate.\n\n        Returns:\n            str: The validated language name.\n\n        Raises:\n            ValueError: If the input is not a valid ISO 639 code or language name.\n\n        \"\"\"\n\n        try:\n            language_name = Language.match(v).name\n            return language_name\n        except LanguageNotFoundError:\n            raise ValueError(\n                \"Invalid language input. Please enter a valid ISO 639 code or language name.\"\n            )\n\n    @field_validator(\"log_level\", mode=\"before\")\n    @classmethod\n    def set_log_level(cls, v: str) -&gt; LogLevel:\n        \"\"\"\n        Converts a string to a `LogLevel` enum member, raising an error for invalid inputs.\n\n        Args:\n            v: The log level string to set.\n\n        Returns:\n            LogLevel: The LogLevel enum member corresponding to the input string.\n\n        Raises:\n            ValueError: If the provided log level string is invalid.\n\n        \"\"\"\n\n        if isinstance(v, str):\n            v = v.upper()\n        if v in LogLevel._value2member_map_:\n            return LogLevel(v)\n        raise ValueError(f\"Invalid log level: {v}\")\n</code></pre>"},{"location":"repo_agent/settings/#repo_agent.settings.ProjectSettings.set_log_level","title":"<code>set_log_level(v)</code>  <code>classmethod</code>","text":"<p>Converts a string to a <code>LogLevel</code> enum member, raising an error for invalid inputs.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>str</code> <p>The log level string to set.</p> required <p>Returns:</p> Name Type Description <code>LogLevel</code> <code>LogLevel</code> <p>The LogLevel enum member corresponding to the input string.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided log level string is invalid.</p> Source code in <code>repo_agent/settings.py</code> <pre><code>@field_validator(\"log_level\", mode=\"before\")\n@classmethod\ndef set_log_level(cls, v: str) -&gt; LogLevel:\n    \"\"\"\n    Converts a string to a `LogLevel` enum member, raising an error for invalid inputs.\n\n    Args:\n        v: The log level string to set.\n\n    Returns:\n        LogLevel: The LogLevel enum member corresponding to the input string.\n\n    Raises:\n        ValueError: If the provided log level string is invalid.\n\n    \"\"\"\n\n    if isinstance(v, str):\n        v = v.upper()\n    if v in LogLevel._value2member_map_:\n        return LogLevel(v)\n    raise ValueError(f\"Invalid log level: {v}\")\n</code></pre>"},{"location":"repo_agent/settings/#repo_agent.settings.ProjectSettings.validate_language_code","title":"<code>validate_language_code(v)</code>  <code>classmethod</code>","text":"<p>Ensures the provided input represents a recognized language, returning its standardized name.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>str</code> <p>The language code or name to validate.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The validated language name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input is not a valid ISO 639 code or language name.</p> Source code in <code>repo_agent/settings.py</code> <pre><code>@field_validator(\"language\")\n@classmethod\ndef validate_language_code(cls, v: str) -&gt; str:\n    \"\"\"\n    Ensures the provided input represents a recognized language, returning its standardized name.\n\n    Args:\n        v: The language code or name to validate.\n\n    Returns:\n        str: The validated language name.\n\n    Raises:\n        ValueError: If the input is not a valid ISO 639 code or language name.\n\n    \"\"\"\n\n    try:\n        language_name = Language.match(v).name\n        return language_name\n    except LanguageNotFoundError:\n        raise ValueError(\n            \"Invalid language input. Please enter a valid ISO 639 code or language name.\"\n        )\n</code></pre>"},{"location":"repo_agent/settings/#repo_agent.settings.Setting","title":"<code>Setting</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Represents a configurable setting with a name, value, and description.</p> <p>This class is designed to hold configuration parameters for an application or system.  It allows storing settings with associated descriptions for better maintainability and understanding.</p> Source code in <code>repo_agent/settings.py</code> <pre><code>class Setting(BaseSettings):\n    \"\"\"\n    Represents a configurable setting with a name, value, and description.\n\n     This class is designed to hold configuration parameters for an application or system.\n     It allows storing settings with associated descriptions for better maintainability and understanding.\n\n    \"\"\"\n\n    project: ProjectSettings = {}\n    chat_completion: ChatCompletionSettings = {}\n</code></pre>"},{"location":"repo_agent/settings/#repo_agent.settings.SettingsManager","title":"<code>SettingsManager</code>","text":"<p>SettingsManager manages and provides access to application settings.</p> <p>This class acts as a central repository for configuration parameters, ensuring consistent access throughout the application. It utilizes a singleton pattern to maintain a single instance of the settings.</p> <p>Class Attributes: - _setting_instance</p> <p>Class Methods: - get_setting:</p> Source code in <code>repo_agent/settings.py</code> <pre><code>class SettingsManager:\n    \"\"\"\n    SettingsManager manages and provides access to application settings.\n\n    This class acts as a central repository for configuration parameters,\n    ensuring consistent access throughout the application. It utilizes a singleton\n    pattern to maintain a single instance of the settings.\n\n    Class Attributes:\n    - _setting_instance\n\n    Class Methods:\n    - get_setting:\n    \"\"\"\n\n    _setting_instance: Optional[Setting] = None\n\n    @classmethod\n    def get_setting(cls):\n        \"\"\"\n        Provides access to the application\u2019s configuration. Creates a new configuration object if one doesn't already exist, ensuring consistent settings throughout the application lifecycle.\n\n                This method ensures that only one instance of the Setting class is created\n                and returns it. If an instance doesn't exist, it creates one first.\n\n                Parameters:\n                    cls - The class itself.\n\n                Returns:\n                    Setting: The singleton instance of the Setting class.\n\n        \"\"\"\n\n        if cls._setting_instance is None:\n            cls._setting_instance = Setting()\n        return cls._setting_instance\n\n    @classmethod\n    def initialize_with_params(\n        cls,\n        target_repo: Path,\n        markdown_docs_name: str,\n        hierarchy_name: str,\n        ignore_list: list[str],\n        language: str,\n        max_thread_count: int,\n        log_level: str,\n        model: str,\n        temperature: float,\n        request_timeout: int,\n        openai_base_url: str,\n        parse_references: bool = True,\n    ):\n        \"\"\"\n        Configures the application with project-specific and OpenAI connection details.\n\n        Args:\n            target_repo: The path to the target repository.\n            markdown_docs_name: The name of the markdown documentation file.\n            hierarchy_name: The name used for hierarchy representation.\n            ignore_list: A list of files or directories to ignore during processing.\n            language: The programming language of the codebase.\n            max_thread_count: The maximum number of threads to use for parallel processing.\n            log_level: The logging level to be used.\n            model: The name of the OpenAI model to use.\n            temperature: The temperature setting for the OpenAI model.\n            request_timeout: The request timeout in seconds for OpenAI API calls.\n            openai_base_url: The base URL for the OpenAI API.\n            parse_references: Whether to parse references within the documentation.\n\n        Returns:\n            None\n\n\n        \"\"\"\n\n        project_settings = ProjectSettings(\n            target_repo=target_repo,\n            hierarchy_name=hierarchy_name,\n            markdown_docs_name=markdown_docs_name,\n            ignore_list=ignore_list,\n            language=language,\n            max_thread_count=max_thread_count,\n            log_level=LogLevel(log_level),\n            parse_references=parse_references,\n        )\n        chat_completion_settings = ChatCompletionSettings(\n            model=model,\n            temperature=temperature,\n            request_timeout=request_timeout,\n            openai_base_url=openai_base_url,\n        )\n        cls._setting_instance = Setting(\n            project=project_settings, chat_completion=chat_completion_settings\n        )\n</code></pre>"},{"location":"repo_agent/settings/#repo_agent.settings.SettingsManager.get_setting","title":"<code>get_setting()</code>  <code>classmethod</code>","text":"<p>Provides access to the application\u2019s configuration. Creates a new configuration object if one doesn't already exist, ensuring consistent settings throughout the application lifecycle.</p> <pre><code>    This method ensures that only one instance of the Setting class is created\n    and returns it. If an instance doesn't exist, it creates one first.\n\n    Parameters:\n        cls - The class itself.\n\n    Returns:\n        Setting: The singleton instance of the Setting class.\n</code></pre> Source code in <code>repo_agent/settings.py</code> <pre><code>@classmethod\ndef get_setting(cls):\n    \"\"\"\n    Provides access to the application\u2019s configuration. Creates a new configuration object if one doesn't already exist, ensuring consistent settings throughout the application lifecycle.\n\n            This method ensures that only one instance of the Setting class is created\n            and returns it. If an instance doesn't exist, it creates one first.\n\n            Parameters:\n                cls - The class itself.\n\n            Returns:\n                Setting: The singleton instance of the Setting class.\n\n    \"\"\"\n\n    if cls._setting_instance is None:\n        cls._setting_instance = Setting()\n    return cls._setting_instance\n</code></pre>"},{"location":"repo_agent/settings/#repo_agent.settings.SettingsManager.initialize_with_params","title":"<code>initialize_with_params(target_repo, markdown_docs_name, hierarchy_name, ignore_list, language, max_thread_count, log_level, model, temperature, request_timeout, openai_base_url, parse_references=True)</code>  <code>classmethod</code>","text":"<p>Configures the application with project-specific and OpenAI connection details.</p> <p>Parameters:</p> Name Type Description Default <code>target_repo</code> <code>Path</code> <p>The path to the target repository.</p> required <code>markdown_docs_name</code> <code>str</code> <p>The name of the markdown documentation file.</p> required <code>hierarchy_name</code> <code>str</code> <p>The name used for hierarchy representation.</p> required <code>ignore_list</code> <code>list[str]</code> <p>A list of files or directories to ignore during processing.</p> required <code>language</code> <code>str</code> <p>The programming language of the codebase.</p> required <code>max_thread_count</code> <code>int</code> <p>The maximum number of threads to use for parallel processing.</p> required <code>log_level</code> <code>str</code> <p>The logging level to be used.</p> required <code>model</code> <code>str</code> <p>The name of the OpenAI model to use.</p> required <code>temperature</code> <code>float</code> <p>The temperature setting for the OpenAI model.</p> required <code>request_timeout</code> <code>int</code> <p>The request timeout in seconds for OpenAI API calls.</p> required <code>openai_base_url</code> <code>str</code> <p>The base URL for the OpenAI API.</p> required <code>parse_references</code> <code>bool</code> <p>Whether to parse references within the documentation.</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/settings.py</code> <pre><code>@classmethod\ndef initialize_with_params(\n    cls,\n    target_repo: Path,\n    markdown_docs_name: str,\n    hierarchy_name: str,\n    ignore_list: list[str],\n    language: str,\n    max_thread_count: int,\n    log_level: str,\n    model: str,\n    temperature: float,\n    request_timeout: int,\n    openai_base_url: str,\n    parse_references: bool = True,\n):\n    \"\"\"\n    Configures the application with project-specific and OpenAI connection details.\n\n    Args:\n        target_repo: The path to the target repository.\n        markdown_docs_name: The name of the markdown documentation file.\n        hierarchy_name: The name used for hierarchy representation.\n        ignore_list: A list of files or directories to ignore during processing.\n        language: The programming language of the codebase.\n        max_thread_count: The maximum number of threads to use for parallel processing.\n        log_level: The logging level to be used.\n        model: The name of the OpenAI model to use.\n        temperature: The temperature setting for the OpenAI model.\n        request_timeout: The request timeout in seconds for OpenAI API calls.\n        openai_base_url: The base URL for the OpenAI API.\n        parse_references: Whether to parse references within the documentation.\n\n    Returns:\n        None\n\n\n    \"\"\"\n\n    project_settings = ProjectSettings(\n        target_repo=target_repo,\n        hierarchy_name=hierarchy_name,\n        markdown_docs_name=markdown_docs_name,\n        ignore_list=ignore_list,\n        language=language,\n        max_thread_count=max_thread_count,\n        log_level=LogLevel(log_level),\n        parse_references=parse_references,\n    )\n    chat_completion_settings = ChatCompletionSettings(\n        model=model,\n        temperature=temperature,\n        request_timeout=request_timeout,\n        openai_base_url=openai_base_url,\n    )\n    cls._setting_instance = Setting(\n        project=project_settings, chat_completion=chat_completion_settings\n    )\n</code></pre>"},{"location":"repo_agent/utils/","title":"Utils","text":""},{"location":"repo_agent/utils/#overview","title":"Overview","text":"<p>The <code>utils</code> module provides a collection of utility functions and classes to support core functionalities within the RepoAgent tool. It focuses on file system interactions, <code>.gitignore</code> handling, and temporary file management crucial for documentation generation and updates.</p>"},{"location":"repo_agent/utils/#purpose","title":"Purpose","text":"<p>This module serves to facilitate automated documentation processes by providing tools for: checking files against <code>.gitignore</code> rules; creating and deleting temporary files needed during analysis and documentation generation; and updating docstrings within the codebase. It supports RepoAgent's ability to analyze code changes, manage project files, and maintain synchronized documentation. Specifically, it enables identifying relevant files for documentation updates while respecting exclusion patterns defined in <code>.gitignore</code>, managing fake files created during the process, and modifying existing docstrings.</p>"},{"location":"repo_agent/utils/docstring_updater/","title":"Docstring Updater","text":""},{"location":"repo_agent/utils/docstring_updater/#repo_agent.utils.docstring_updater.remove_docstrings","title":"<code>remove_docstrings(code)</code>","text":"<p>No valid docstring found.</p> Source code in <code>repo_agent/utils/docstring_updater.py</code> <pre><code>def remove_docstrings(code):\n    \"\"\"\n    No valid docstring found.\n\n    \"\"\"\n\n    pattern = re.compile(r'^\\s*(\"\"\"|\\'\\'\\').*?^\\s*\\1', re.DOTALL | re.MULTILINE)\n    return pattern.sub(\"\", code)\n</code></pre>"},{"location":"repo_agent/utils/docstring_updater/#repo_agent.utils.docstring_updater.update_doc","title":"<code>update_doc(node, new_docstring)</code>","text":"<p>Replaces the docstring of a given AST node with new content, handling indentation to maintain code structure. If no docstring exists, it adds one; otherwise, it updates the existing one.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <p>The AST node to update.</p> required <code>new_docstring</code> <p>The new docstring content.</p> required <p>Returns:</p> Type Description <p>The updated AST node.</p> Source code in <code>repo_agent/utils/docstring_updater.py</code> <pre><code>def update_doc(node, new_docstring):\n    \"\"\"\n    Replaces the docstring of a given AST node with new content, handling indentation to maintain code structure. If no docstring exists, it adds one; otherwise, it updates the existing one.\n\n    Args:\n        node: The AST node to update.\n        new_docstring: The new docstring content.\n\n    Returns:\n        The updated AST node.\n\n    \"\"\"\n\n    indent = \"    \" if not isinstance(node, ast.Module) else \"\"\n    lines = new_docstring.split(\"\\n\")\n    if len(lines) &gt; 1:\n        lines[1:] = [indent + line for line in lines[1:]]\n    processed_doc = \"\\n\" + indent + \"\\n\".join(lines) + \"\\n\" + indent\n    if ast.get_docstring(node) is None:\n        node.body.insert(0, ast.Expr(value=ast.Str(s=processed_doc)))\n    else:\n        node.body[0] = ast.Expr(value=ast.Str(s=processed_doc))\n    return node\n</code></pre>"},{"location":"repo_agent/utils/gitignore_checker/","title":"Gitignore Checker","text":""},{"location":"repo_agent/utils/gitignore_checker/#repo_agent.utils.gitignore_checker.GitignoreChecker","title":"<code>GitignoreChecker</code>","text":"<p>Checks for files and folders in a directory, excluding those ignored by .gitignore.</p> <p>This class provides functionality to scan a directory and identify Python files and directories that are not excluded by the associated .gitignore file. It parses the .gitignore file, loads patterns, and then checks each file/folder against these patterns.</p> Source code in <code>repo_agent/utils/gitignore_checker.py</code> <pre><code>class GitignoreChecker:\n    \"\"\"\n    Checks for files and folders in a directory, excluding those ignored by .gitignore.\n\n    This class provides functionality to scan a directory and identify Python files and directories\n    that are not excluded by the associated .gitignore file. It parses the .gitignore file,\n    loads patterns, and then checks each file/folder against these patterns.\n    \"\"\"\n\n    def __init__(self, directory: str, gitignore_path: str):\n        \"\"\"\n        Initializes a checker to identify files ignored by Git, given a directory and .gitignore file.\n\n\n\n        Args:\n            directory: The directory to scan for ignored files.\n            gitignore_path: The path to the .gitignore file.\n\n        Returns:\n            None\n\n        \"\"\"\n\n        self.directory = directory\n        self.gitignore_path = gitignore_path\n        self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()\n\n    def _load_gitignore_patterns(self) -&gt; tuple:\n        \"\"\"\n        Reads patterns from a .gitignore file, falling back to a default location if the specified file is not found, then parses and splits them.\n\n        Args:\n            None\n\n        Returns:\n            tuple: A tuple containing the split gitignore patterns.\n\n\n        \"\"\"\n\n        try:\n            with open(self.gitignore_path, \"r\", encoding=\"utf-8\") as file:\n                gitignore_content = file.read()\n        except FileNotFoundError:\n            default_path = os.path.join(\n                os.path.dirname(__file__), \"..\", \"..\", \".gitignore\"\n            )\n            with open(default_path, \"r\", encoding=\"utf-8\") as file:\n                gitignore_content = file.read()\n        patterns = self._parse_gitignore(gitignore_content)\n        return self._split_gitignore_patterns(patterns)\n\n    @staticmethod\n    def _parse_gitignore(gitignore_content: str) -&gt; list:\n        \"\"\"\n        No valid docstring found.\n\n        \"\"\"\n\n        patterns = []\n        for line in gitignore_content.splitlines():\n            line = line.strip()\n            if line and (not line.startswith(\"#\")):\n                patterns.append(line)\n        return patterns\n\n    @staticmethod\n    def _split_gitignore_patterns(gitignore_patterns: list) -&gt; tuple:\n        \"\"\"\n        Categorizes .gitignore patterns, separating those representing directories from those representing files.\n\n        Args:\n            gitignore_patterns: A list of strings representing .gitignore patterns.\n\n        Returns:\n            tuple: A tuple containing two lists:\n                - The first list contains folder patterns.\n                - The second list contains file patterns.\n\n        \"\"\"\n\n        folder_patterns = [\".git\", \".github\", \".idea\", \"venv\", \".venv\"]\n        file_patterns = []\n        for pattern in gitignore_patterns:\n            if pattern.endswith(\"/\"):\n                folder_patterns.append(pattern.rstrip(\"/\"))\n            else:\n                file_patterns.append(pattern)\n        return (folder_patterns, file_patterns)\n\n    @staticmethod\n    def _is_ignored(path: str, patterns: list, is_dir: bool = False) -&gt; bool:\n        \"\"\"\n        No valid docstring found.\n\n        \"\"\"\n\n        for pattern in patterns:\n            if fnmatch.fnmatch(path, pattern):\n                return True\n            if is_dir and pattern.endswith(\"/\") and fnmatch.fnmatch(path, pattern[:-1]):\n                return True\n        return False\n\n    def check_files_and_folders(self) -&gt; list:\n        \"\"\"\n        Identifies and lists Python files and directories within a given directory, respecting configured ignore patterns.\n\n        Args:\n            None\n\n        Returns:\n            list: A list of relative paths to non-ignored Python files and directories.\n\n\n        \"\"\"\n\n        ignored_folders = SettingsManager().get_setting().project.ignore_list\n        not_ignored_files = []\n        for root, dirs, files in os.walk(self.directory):\n            dirs[:] = [\n                d\n                for d in dirs\n                if not self._is_ignored(d, self.folder_patterns, is_dir=True)\n                and (not any([d in i for i in ignored_folders]))\n            ]\n            not_ignored_files += [\n                os.path.relpath(os.path.join(root, d), self.directory) for d in dirs\n            ]\n            for file in files:\n                file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(file_path, self.directory)\n                if not self._is_ignored(\n                    file, self.file_patterns\n                ) and file_path.endswith(\".py\"):\n                    not_ignored_files.append(relative_path)\n        return not_ignored_files\n</code></pre>"},{"location":"repo_agent/utils/gitignore_checker/#repo_agent.utils.gitignore_checker.GitignoreChecker.__init__","title":"<code>__init__(directory, gitignore_path)</code>","text":"<p>Initializes a checker to identify files ignored by Git, given a directory and .gitignore file.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>The directory to scan for ignored files.</p> required <code>gitignore_path</code> <code>str</code> <p>The path to the .gitignore file.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/utils/gitignore_checker.py</code> <pre><code>def __init__(self, directory: str, gitignore_path: str):\n    \"\"\"\n    Initializes a checker to identify files ignored by Git, given a directory and .gitignore file.\n\n\n\n    Args:\n        directory: The directory to scan for ignored files.\n        gitignore_path: The path to the .gitignore file.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    self.directory = directory\n    self.gitignore_path = gitignore_path\n    self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()\n</code></pre>"},{"location":"repo_agent/utils/gitignore_checker/#repo_agent.utils.gitignore_checker.GitignoreChecker.check_files_and_folders","title":"<code>check_files_and_folders()</code>","text":"<p>Identifies and lists Python files and directories within a given directory, respecting configured ignore patterns.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of relative paths to non-ignored Python files and directories.</p> Source code in <code>repo_agent/utils/gitignore_checker.py</code> <pre><code>def check_files_and_folders(self) -&gt; list:\n    \"\"\"\n    Identifies and lists Python files and directories within a given directory, respecting configured ignore patterns.\n\n    Args:\n        None\n\n    Returns:\n        list: A list of relative paths to non-ignored Python files and directories.\n\n\n    \"\"\"\n\n    ignored_folders = SettingsManager().get_setting().project.ignore_list\n    not_ignored_files = []\n    for root, dirs, files in os.walk(self.directory):\n        dirs[:] = [\n            d\n            for d in dirs\n            if not self._is_ignored(d, self.folder_patterns, is_dir=True)\n            and (not any([d in i for i in ignored_folders]))\n        ]\n        not_ignored_files += [\n            os.path.relpath(os.path.join(root, d), self.directory) for d in dirs\n        ]\n        for file in files:\n            file_path = os.path.join(root, file)\n            relative_path = os.path.relpath(file_path, self.directory)\n            if not self._is_ignored(\n                file, self.file_patterns\n            ) and file_path.endswith(\".py\"):\n                not_ignored_files.append(relative_path)\n    return not_ignored_files\n</code></pre>"},{"location":"repo_agent/utils/meta_info_utils/","title":"Meta Info Utils","text":""},{"location":"repo_agent/utils/meta_info_utils/#repo_agent.utils.meta_info_utils.delete_fake_files","title":"<code>delete_fake_files()</code>","text":"<p>Recursively searches for and handles files with a specific version substring within the target repository. Empty files are deleted, while non-empty ones are renamed to their original filenames, effectively restoring previous versions. Informative messages are printed detailing each file action.</p> <p>This method recursively searches for files ending with a specific substring (defined by <code>latest_verison_substring</code>) within the target repository directory. If a corresponding original file exists, it's deleted. Empty fake files are removed, and non-empty ones are renamed to their original names.  Prints messages indicating deleted or recovered files.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repo_agent/utils/meta_info_utils.py</code> <pre><code>def delete_fake_files():\n    \"\"\"\n    Recursively searches for and handles files with a specific version substring within the target repository. Empty files are deleted, while non-empty ones are renamed to their original filenames, effectively restoring previous versions. Informative messages are printed detailing each file action.\n\n    This method recursively searches for files ending with a specific substring\n    (defined by `latest_verison_substring`) within the target repository directory.\n    If a corresponding original file exists, it's deleted. Empty fake files are removed,\n    and non-empty ones are renamed to their original names.  Prints messages indicating\n    deleted or recovered files.\n\n    Args:\n        None\n\n    Returns:\n        None\n\n    \"\"\"\n\n    setting = SettingsManager.get_setting()\n\n    def gci(filepath):\n        files = os.listdir(filepath)\n        for fi in files:\n            fi_d = os.path.join(filepath, fi)\n            if os.path.isdir(fi_d):\n                gci(fi_d)\n            elif fi_d.endswith(latest_verison_substring):\n                origin_name = fi_d.replace(latest_verison_substring, \".py\")\n                if os.path.exists(origin_name):\n                    os.remove(origin_name)\n                if os.path.getsize(fi_d) == 0:\n                    print(\n                        f\"{Fore.LIGHTRED_EX}[Deleting Temp File]: {Style.RESET_ALL}{fi_d[len(str(setting.project.target_repo)):]}, {origin_name[len(str(setting.project.target_repo)):]}\"\n                    )\n                    os.remove(fi_d)\n                else:\n                    print(\n                        f\"{Fore.LIGHTRED_EX}[Recovering Latest Version]: {Style.RESET_ALL}{origin_name[len(str(setting.project.target_repo)):]} &lt;- {fi_d[len(str(setting.project.target_repo)):]}\"\n                    )\n                    os.rename(fi_d, origin_name)\n\n    gci(setting.project.target_repo)\n</code></pre>"},{"location":"repo_agent/utils/meta_info_utils/#repo_agent.utils.meta_info_utils.make_fake_files","title":"<code>make_fake_files()</code>","text":"<p>Generates temporary copies of modified or deleted Python files and identifies untracked Python files, providing a mapping between original and latest version paths.</p> <p>This method first deletes any existing fake files, then identifies untracked .py files and unstaged additions/modifications/deletions. It renames or creates temporary versions of modified/deleted files with a version suffix, effectively creating \"fake\" copies for documentation purposes.  It also skips untracked Python files.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing a dictionary mapping original file paths to    their corresponding latest version paths and a list of skipped    untracked .py files.</p> Source code in <code>repo_agent/utils/meta_info_utils.py</code> <pre><code>def make_fake_files():\n    \"\"\"\n    Generates temporary copies of modified or deleted Python files and identifies untracked Python files, providing a mapping between original and latest version paths.\n\n    This method first deletes any existing fake files, then identifies\n    untracked .py files and unstaged additions/modifications/deletions. It\n    renames or creates temporary versions of modified/deleted files with a\n    version suffix, effectively creating \"fake\" copies for documentation\n    purposes.  It also skips untracked Python files.\n\n    Args:\n        None\n\n    Returns:\n        tuple: A tuple containing a dictionary mapping original file paths to\n               their corresponding latest version paths and a list of skipped\n               untracked .py files.\n\n\n    \"\"\"\n\n    delete_fake_files()\n    setting = SettingsManager.get_setting()\n    repo = git.Repo(setting.project.target_repo)\n    unstaged_changes = repo.index.diff(None)\n    untracked_files = repo.untracked_files\n    jump_files = []\n    for file_name in untracked_files:\n        if file_name.endswith(\".py\"):\n            print(\n                f\"{Fore.LIGHTMAGENTA_EX}[SKIP untracked files]: {Style.RESET_ALL}{file_name}\"\n            )\n            jump_files.append(file_name)\n    for diff_file in unstaged_changes.iter_change_type(\"A\"):\n        if diff_file.a_path.endswith(latest_verison_substring):\n            logger.error(\n                \"FAKE_FILE_IN_GIT_STATUS detected! suggest to use `delete_fake_files` and re-generate document\"\n            )\n            exit()\n        jump_files.append(diff_file.a_path)\n    file_path_reflections = {}\n    for diff_file in itertools.chain(\n        unstaged_changes.iter_change_type(\"M\"), unstaged_changes.iter_change_type(\"D\")\n    ):\n        if diff_file.a_path.endswith(latest_verison_substring):\n            logger.error(\n                \"FAKE_FILE_IN_GIT_STATUS detected! suggest to use `delete_fake_files` and re-generate document\"\n            )\n            exit()\n        now_file_path = diff_file.a_path\n        if now_file_path.endswith(\".py\"):\n            raw_file_content = diff_file.a_blob.data_stream.read().decode(\"utf-8\")\n            latest_file_path = now_file_path[:-3] + latest_verison_substring\n            if os.path.exists(os.path.join(setting.project.target_repo, now_file_path)):\n                os.rename(\n                    os.path.join(setting.project.target_repo, now_file_path),\n                    os.path.join(setting.project.target_repo, latest_file_path),\n                )\n                print(\n                    f\"{Fore.LIGHTMAGENTA_EX}[Save Latest Version of Code]: {Style.RESET_ALL}{now_file_path} -&gt; {latest_file_path}\"\n                )\n            else:\n                print(\n                    f\"{Fore.LIGHTMAGENTA_EX}[Create Temp-File for Deleted(But not Staged) Files]: {Style.RESET_ALL}{now_file_path} -&gt; {latest_file_path}\"\n                )\n                try:\n                    with open(\n                        os.path.join(setting.project.target_repo, latest_file_path),\n                        \"w\",\n                        encoding=\"utf-8\",\n                    ) as writer:\n                        pass\n                except:\n                    pass\n            try:\n                with open(\n                    os.path.join(setting.project.target_repo, now_file_path),\n                    \"w\",\n                    encoding=\"utf-8\",\n                ) as writer:\n                    writer.write(raw_file_content)\n            except:\n                pass\n            file_path_reflections[now_file_path] = latest_file_path\n    return (file_path_reflections, jump_files)\n</code></pre>"},{"location":"tests/test_change_detector/","title":"Test Change Detector","text":""},{"location":"tests/test_change_detector/#tests.test_change_detector.TestChangeDetector","title":"<code>TestChangeDetector</code>","text":"<p>               Bases: <code>TestCase</code></p> <p>Detects changes in a git repository, specifically focusing on Python and Markdown files.</p> <p>This class provides methods to identify staged, unstaged, and modified files within a Git repository. It's designed for use in testing or automation where tracking file changes is necessary.</p> Source code in <code>tests/test_change_detector.py</code> <pre><code>class TestChangeDetector(unittest.TestCase):\n    \"\"\"\n    Detects changes in a git repository, specifically focusing on Python and Markdown files.\n\n    This class provides methods to identify staged, unstaged, and modified files\n    within a Git repository. It's designed for use in testing or automation where\n    tracking file changes is necessary.\n    \"\"\"\n\n    @classmethod\n    def setUpClass(cls):\n        \"\"\"\n        Sets up the test repository for testing.\n\n        \"\"\"\n\n        # \u5b9a\u4e49\u6d4b\u8bd5\u4ed3\u5e93\u7684\u8def\u5f84\n        cls.test_repo_path = os.path.join(os.path.dirname(__file__), \"test_repo\")\n\n        # \u5982\u679c\u6d4b\u8bd5\u4ed3\u5e93\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\uff0c\u5219\u521b\u5efa\u5b83\n        if not os.path.exists(cls.test_repo_path):\n            os.makedirs(cls.test_repo_path)\n\n        # \u521d\u59cb\u5316 Git \u4ed3\u5e93\n        cls.repo = Repo.init(cls.test_repo_path)\n\n        # \u914d\u7f6e Git \u7528\u6237\u4fe1\u606f\n        cls.repo.git.config(\"user.email\", \"ci@example.com\")\n        cls.repo.git.config(\"user.name\", \"CI User\")\n\n        # \u521b\u5efa\u4e00\u4e9b\u6d4b\u8bd5\u6587\u4ef6\n        with open(os.path.join(cls.test_repo_path, \"test_file.py\"), \"w\") as f:\n            f.write('print(\"Hello, Python\")')\n\n        with open(os.path.join(cls.test_repo_path, \"test_file.md\"), \"w\") as f:\n            f.write(\"# Hello, Markdown\")\n\n        # \u6a21\u62df Git \u64cd\u4f5c\uff1a\u6dfb\u52a0\u548c\u63d0\u4ea4\u6587\u4ef6\n        cls.repo.git.add(A=True)\n        cls.repo.git.commit(\"-m\", \"Initial commit\")\n\n    def test_get_staged_pys(self):\n        \"\"\"\n        Tests that a newly created and staged Python file is correctly identified as staged.\n\n        This test creates a new Python file, stages it using git add, and then\n        uses ChangeDetector to verify that the new file is included in the list\n        of staged Python files.\n\n        Args:\n            None\n\n        Returns:\n            None\n\n\n        \"\"\"\n\n        # \u521b\u5efa\u4e00\u4e2a\u65b0\u7684 Python \u6587\u4ef6\u5e76\u6682\u5b58\n        new_py_file = os.path.join(self.test_repo_path, \"new_test_file.py\")\n        with open(new_py_file, \"w\") as f:\n            f.write('print(\"New Python File\")')\n        self.repo.git.add(new_py_file)\n\n        # \u4f7f\u7528 ChangeDetector \u68c0\u67e5\u6682\u5b58\u6587\u4ef6\n        change_detector = ChangeDetector(self.test_repo_path)\n        staged_files = change_detector.get_staged_pys()\n\n        # \u65ad\u8a00\u65b0\u6587\u4ef6\u5728\u6682\u5b58\u6587\u4ef6\u5217\u8868\u4e2d\n        self.assertIn(\n            \"new_test_file.py\", [os.path.basename(path) for path in staged_files]\n        )\n\n        print(f\"\\ntest_get_staged_pys: Staged Python files: {staged_files}\")\n\n    def test_get_unstaged_mds(self):\n        \"\"\"\n        Tests that a modified Markdown file is correctly identified as unstaged and appears in the list of files to be staged.\n\n        This test creates a modified Markdown file, then uses ChangeDetector to\n        identify it as an unstaged file. It asserts that the modified file is\n        present in the list of unstaged files returned by get_to_be_staged_files().\n\n        Args:\n            None\n\n        Returns:\n            None\n\n\n        \"\"\"\n\n        # \u4fee\u6539\u4e00\u4e2a Markdown \u6587\u4ef6\u4f46\u4e0d\u6682\u5b58\n        md_file = os.path.join(self.test_repo_path, \"test_file.md\")\n        with open(md_file, \"a\") as f:\n            f.write(\"\\nAdditional Markdown content\")\n\n        # \u4f7f\u7528 ChangeDetector \u83b7\u53d6\u672a\u6682\u5b58\u7684 Markdown \u6587\u4ef6\n        change_detector = ChangeDetector(self.test_repo_path)\n        unstaged_files = change_detector.get_to_be_staged_files()\n\n        # \u65ad\u8a00\u4fee\u6539\u7684\u6587\u4ef6\u5728\u672a\u6682\u5b58\u6587\u4ef6\u5217\u8868\u4e2d\n        self.assertIn(\n            \"test_file.md\", [os.path.basename(path) for path in unstaged_files]\n        )\n\n        print(f\"\\ntest_get_unstaged_mds: Unstaged Markdown files: {unstaged_files}\")\n\n    def test_add_unstaged_mds(self):\n        \"\"\"\n        Verifies that running `add_unstaged_files` correctly stages previously unstaged Markdown files, leaving no remaining unstaged Markdown files.\n\n        This method adds all unstaged markdown files in the repository and\n        asserts that there are no remaining unstaged markdown files after the add operation.\n\n        Args:\n            None\n\n        Returns:\n            None\n\n\n        \"\"\"\n\n        # \u786e\u4fdd\u6709\u4e00\u4e2a\u672a\u6682\u5b58\u7684 Markdown \u6587\u4ef6\n        self.test_get_unstaged_mds()\n\n        # \u4f7f\u7528 ChangeDetector \u6dfb\u52a0\u672a\u6682\u5b58\u7684 Markdown \u6587\u4ef6\n        change_detector = ChangeDetector(self.test_repo_path)\n        change_detector.add_unstaged_files()\n\n        # \u68c0\u67e5\u6587\u4ef6\u662f\u5426\u88ab\u6682\u5b58\n        unstaged_files_after_add = change_detector.get_to_be_staged_files()\n\n        # \u65ad\u8a00\u6682\u5b58\u64cd\u4f5c\u540e\u6ca1\u6709\u672a\u6682\u5b58\u7684 Markdown \u6587\u4ef6\n        self.assertEqual(len(unstaged_files_after_add), 0)\n\n        remaining_unstaged_files = len(unstaged_files_after_add)\n        print(\n            f\"\\ntest_add_unstaged_mds: Number of remaining unstaged Markdown files after add: {remaining_unstaged_files}\"\n        )\n\n    @classmethod\n    def tearDownClass(cls):\n        \"\"\"\n        Cleans up any resources used by the test suite after all tests have completed.\n\n        Args:\n            cls: The class being torn down.\n\n        Returns:\n            None\n\n        \"\"\"\n\n        # \u6e05\u7406\u6d4b\u8bd5\u4ed3\u5e93\n        cls.repo.close()\n        os.system(\"rm -rf \" + cls.test_repo_path)\n</code></pre>"},{"location":"tests/test_change_detector/#tests.test_change_detector.TestChangeDetector.setUpClass","title":"<code>setUpClass()</code>  <code>classmethod</code>","text":"<p>Sets up the test repository for testing.</p> Source code in <code>tests/test_change_detector.py</code> <pre><code>@classmethod\ndef setUpClass(cls):\n    \"\"\"\n    Sets up the test repository for testing.\n\n    \"\"\"\n\n    # \u5b9a\u4e49\u6d4b\u8bd5\u4ed3\u5e93\u7684\u8def\u5f84\n    cls.test_repo_path = os.path.join(os.path.dirname(__file__), \"test_repo\")\n\n    # \u5982\u679c\u6d4b\u8bd5\u4ed3\u5e93\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\uff0c\u5219\u521b\u5efa\u5b83\n    if not os.path.exists(cls.test_repo_path):\n        os.makedirs(cls.test_repo_path)\n\n    # \u521d\u59cb\u5316 Git \u4ed3\u5e93\n    cls.repo = Repo.init(cls.test_repo_path)\n\n    # \u914d\u7f6e Git \u7528\u6237\u4fe1\u606f\n    cls.repo.git.config(\"user.email\", \"ci@example.com\")\n    cls.repo.git.config(\"user.name\", \"CI User\")\n\n    # \u521b\u5efa\u4e00\u4e9b\u6d4b\u8bd5\u6587\u4ef6\n    with open(os.path.join(cls.test_repo_path, \"test_file.py\"), \"w\") as f:\n        f.write('print(\"Hello, Python\")')\n\n    with open(os.path.join(cls.test_repo_path, \"test_file.md\"), \"w\") as f:\n        f.write(\"# Hello, Markdown\")\n\n    # \u6a21\u62df Git \u64cd\u4f5c\uff1a\u6dfb\u52a0\u548c\u63d0\u4ea4\u6587\u4ef6\n    cls.repo.git.add(A=True)\n    cls.repo.git.commit(\"-m\", \"Initial commit\")\n</code></pre>"},{"location":"tests/test_change_detector/#tests.test_change_detector.TestChangeDetector.tearDownClass","title":"<code>tearDownClass()</code>  <code>classmethod</code>","text":"<p>Cleans up any resources used by the test suite after all tests have completed.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>The class being torn down.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>tests/test_change_detector.py</code> <pre><code>@classmethod\ndef tearDownClass(cls):\n    \"\"\"\n    Cleans up any resources used by the test suite after all tests have completed.\n\n    Args:\n        cls: The class being torn down.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    # \u6e05\u7406\u6d4b\u8bd5\u4ed3\u5e93\n    cls.repo.close()\n    os.system(\"rm -rf \" + cls.test_repo_path)\n</code></pre>"},{"location":"tests/test_change_detector/#tests.test_change_detector.TestChangeDetector.test_add_unstaged_mds","title":"<code>test_add_unstaged_mds()</code>","text":"<p>Verifies that running <code>add_unstaged_files</code> correctly stages previously unstaged Markdown files, leaving no remaining unstaged Markdown files.</p> <p>This method adds all unstaged markdown files in the repository and asserts that there are no remaining unstaged markdown files after the add operation.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>tests/test_change_detector.py</code> <pre><code>def test_add_unstaged_mds(self):\n    \"\"\"\n    Verifies that running `add_unstaged_files` correctly stages previously unstaged Markdown files, leaving no remaining unstaged Markdown files.\n\n    This method adds all unstaged markdown files in the repository and\n    asserts that there are no remaining unstaged markdown files after the add operation.\n\n    Args:\n        None\n\n    Returns:\n        None\n\n\n    \"\"\"\n\n    # \u786e\u4fdd\u6709\u4e00\u4e2a\u672a\u6682\u5b58\u7684 Markdown \u6587\u4ef6\n    self.test_get_unstaged_mds()\n\n    # \u4f7f\u7528 ChangeDetector \u6dfb\u52a0\u672a\u6682\u5b58\u7684 Markdown \u6587\u4ef6\n    change_detector = ChangeDetector(self.test_repo_path)\n    change_detector.add_unstaged_files()\n\n    # \u68c0\u67e5\u6587\u4ef6\u662f\u5426\u88ab\u6682\u5b58\n    unstaged_files_after_add = change_detector.get_to_be_staged_files()\n\n    # \u65ad\u8a00\u6682\u5b58\u64cd\u4f5c\u540e\u6ca1\u6709\u672a\u6682\u5b58\u7684 Markdown \u6587\u4ef6\n    self.assertEqual(len(unstaged_files_after_add), 0)\n\n    remaining_unstaged_files = len(unstaged_files_after_add)\n    print(\n        f\"\\ntest_add_unstaged_mds: Number of remaining unstaged Markdown files after add: {remaining_unstaged_files}\"\n    )\n</code></pre>"},{"location":"tests/test_change_detector/#tests.test_change_detector.TestChangeDetector.test_get_staged_pys","title":"<code>test_get_staged_pys()</code>","text":"<p>Tests that a newly created and staged Python file is correctly identified as staged.</p> <p>This test creates a new Python file, stages it using git add, and then uses ChangeDetector to verify that the new file is included in the list of staged Python files.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>tests/test_change_detector.py</code> <pre><code>def test_get_staged_pys(self):\n    \"\"\"\n    Tests that a newly created and staged Python file is correctly identified as staged.\n\n    This test creates a new Python file, stages it using git add, and then\n    uses ChangeDetector to verify that the new file is included in the list\n    of staged Python files.\n\n    Args:\n        None\n\n    Returns:\n        None\n\n\n    \"\"\"\n\n    # \u521b\u5efa\u4e00\u4e2a\u65b0\u7684 Python \u6587\u4ef6\u5e76\u6682\u5b58\n    new_py_file = os.path.join(self.test_repo_path, \"new_test_file.py\")\n    with open(new_py_file, \"w\") as f:\n        f.write('print(\"New Python File\")')\n    self.repo.git.add(new_py_file)\n\n    # \u4f7f\u7528 ChangeDetector \u68c0\u67e5\u6682\u5b58\u6587\u4ef6\n    change_detector = ChangeDetector(self.test_repo_path)\n    staged_files = change_detector.get_staged_pys()\n\n    # \u65ad\u8a00\u65b0\u6587\u4ef6\u5728\u6682\u5b58\u6587\u4ef6\u5217\u8868\u4e2d\n    self.assertIn(\n        \"new_test_file.py\", [os.path.basename(path) for path in staged_files]\n    )\n\n    print(f\"\\ntest_get_staged_pys: Staged Python files: {staged_files}\")\n</code></pre>"},{"location":"tests/test_change_detector/#tests.test_change_detector.TestChangeDetector.test_get_unstaged_mds","title":"<code>test_get_unstaged_mds()</code>","text":"<p>Tests that a modified Markdown file is correctly identified as unstaged and appears in the list of files to be staged.</p> <p>This test creates a modified Markdown file, then uses ChangeDetector to identify it as an unstaged file. It asserts that the modified file is present in the list of unstaged files returned by get_to_be_staged_files().</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>tests/test_change_detector.py</code> <pre><code>def test_get_unstaged_mds(self):\n    \"\"\"\n    Tests that a modified Markdown file is correctly identified as unstaged and appears in the list of files to be staged.\n\n    This test creates a modified Markdown file, then uses ChangeDetector to\n    identify it as an unstaged file. It asserts that the modified file is\n    present in the list of unstaged files returned by get_to_be_staged_files().\n\n    Args:\n        None\n\n    Returns:\n        None\n\n\n    \"\"\"\n\n    # \u4fee\u6539\u4e00\u4e2a Markdown \u6587\u4ef6\u4f46\u4e0d\u6682\u5b58\n    md_file = os.path.join(self.test_repo_path, \"test_file.md\")\n    with open(md_file, \"a\") as f:\n        f.write(\"\\nAdditional Markdown content\")\n\n    # \u4f7f\u7528 ChangeDetector \u83b7\u53d6\u672a\u6682\u5b58\u7684 Markdown \u6587\u4ef6\n    change_detector = ChangeDetector(self.test_repo_path)\n    unstaged_files = change_detector.get_to_be_staged_files()\n\n    # \u65ad\u8a00\u4fee\u6539\u7684\u6587\u4ef6\u5728\u672a\u6682\u5b58\u6587\u4ef6\u5217\u8868\u4e2d\n    self.assertIn(\n        \"test_file.md\", [os.path.basename(path) for path in unstaged_files]\n    )\n\n    print(f\"\\ntest_get_unstaged_mds: Unstaged Markdown files: {unstaged_files}\")\n</code></pre>"},{"location":"tests/test_structure_tree/","title":"Test Structure Tree","text":""},{"location":"tests/test_structure_tree/#tests.test_structure_tree.build_path_tree","title":"<code>build_path_tree(who_reference_me, reference_who, doc_item_path)</code>","text":"<p>No valid docstring found.</p> Source code in <code>tests/test_structure_tree.py</code> <pre><code>def build_path_tree(who_reference_me, reference_who, doc_item_path):\n    \"\"\"\n    No valid docstring found.\n\n    \"\"\"\n\n    def tree():\n        return defaultdict(tree)\n\n    path_tree = tree()\n\n    for path_list in [who_reference_me, reference_who]:\n        for path in path_list:\n            parts = path.split(os.sep)\n            node = path_tree\n            for part in parts:\n                node = node[part]\n\n    # \u5904\u7406 doc_item_path\n    parts = doc_item_path.split(os.sep)\n    parts[-1] = \"\u2733\ufe0f\" + parts[-1]  # \u5728\u6700\u540e\u4e00\u4e2a\u5bf9\u8c61\u524d\u9762\u52a0\u4e0a\u661f\u53f7\n    node = path_tree\n    for part in parts:\n        node = node[part]\n\n    def tree_to_string(tree, indent=0):\n        s = \"\"\n        for key, value in sorted(tree.items()):\n            s += \"    \" * indent + key + \"\\n\"\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n\n    return tree_to_string(path_tree)\n</code></pre>"}]}